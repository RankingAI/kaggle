{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T11:57:29.450623Z",
     "start_time": "2018-01-08T11:57:29.359224Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import scipy.sparse\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm as tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class DataUtil2:\n",
    "    \"\"\"\"\"\"\n",
    "    @classmethod\n",
    "    def load(cls, file, format, date_cols= None):\n",
    "        \"\"\"\"\"\"\n",
    "        data = ''\n",
    "        if(format== 'csv'):\n",
    "            data = pd.read_csv(file, parse_dates= date_cols)\n",
    "        elif(format== 'json'):\n",
    "            with open(file, 'r') as i_file:\n",
    "                data = json.load(file)\n",
    "            i_file.close()\n",
    "        elif(format== 'pkl'):\n",
    "            with open(file, 'rb') as i_file:\n",
    "                data = pickle.load(i_file)\n",
    "            i_file.close()\n",
    "        elif(format == 'hdf'):\n",
    "            data = pd.read_hdf(path_or_buf= file, key='undefined')\n",
    "        elif(format == 'npz'):\n",
    "            data = scipy.sparse.load_npz(file)\n",
    "\n",
    "        return  data\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls, data, file, format, precision= 8):\n",
    "        \"\"\"\"\"\"\n",
    "        if(format == 'csv'):\n",
    "            data.to_csv(file, float_format= '%%.%df' % precision, index= False)\n",
    "        elif(format == 'json'):\n",
    "            with open(file, 'w') as o_file:\n",
    "                json.dump(data, o_file, ensure_ascii= True, indent= 4)\n",
    "            o_file.close()\n",
    "        elif(format == 'pkl'):\n",
    "            with open(file, 'wb') as o_file:\n",
    "                pickle.dump(data, o_file, -1)\n",
    "            o_file.close()\n",
    "        elif(format== 'hdf'):\n",
    "            data.to_hdf(path_or_buf= file, key='undefined', mode='w', complib='blosc')\n",
    "        elif(format == 'npz'):\n",
    "            scipy.sparse.save_npz(file, data)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T11:57:31.250622Z",
     "start_time": "2018-01-08T11:57:29.452568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data done.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "iformat = 'csv'\n",
    "oformat = 'hdf'\n",
    "DataBase = '../data'\n",
    "DataSet = {}\n",
    "for mod in ['train', 'test']:\n",
    "    DataSet[mod] = DataUtil2.load('%s/raw/%s.%s' % (DataBase, mod, iformat), iformat)\n",
    "    DataSet[mod]['comment_text'] = DataSet[mod]['comment_text'].fillna('nan')\n",
    "print('load data done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T11:58:10.107178Z",
     "start_time": "2018-01-08T11:57:31.252842Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/95851 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 316/95851 [00:00<00:30, 3120.56it/s]\u001b[A\n",
      "  1%|          | 756/95851 [00:00<00:25, 3756.05it/s]\u001b[A\n",
      "  1%|▏         | 1308/95851 [00:00<00:21, 4340.27it/s]\u001b[A\n",
      "  2%|▏         | 1828/95851 [00:00<00:27, 3389.45it/s]\u001b[A\n",
      "  3%|▎         | 2555/95851 [00:00<00:23, 3996.89it/s]\u001b[A\n",
      "  3%|▎         | 3304/95851 [00:00<00:20, 4468.49it/s]\u001b[A\n",
      "  4%|▍         | 4082/95851 [00:00<00:18, 4861.34it/s]\u001b[A\n",
      "  5%|▌         | 4848/95851 [00:00<00:17, 5158.74it/s]\u001b[A\n",
      "  6%|▌         | 5737/95851 [00:01<00:16, 5517.59it/s]\u001b[A\n",
      "  7%|▋         | 6583/95851 [00:01<00:15, 5774.40it/s]\u001b[A\n",
      "  8%|▊         | 7519/95851 [00:01<00:14, 6062.90it/s]\u001b[A\n",
      "  9%|▉         | 8419/95851 [00:01<00:13, 6281.25it/s]\u001b[A\n",
      " 10%|▉         | 9307/95851 [00:01<00:13, 6459.22it/s]\u001b[A\n",
      " 11%|█         | 10209/95851 [00:01<00:12, 6623.38it/s]\u001b[A\n",
      " 12%|█▏        | 11210/95851 [00:01<00:12, 6829.54it/s]\u001b[A\n",
      " 13%|█▎        | 12158/95851 [00:01<00:11, 6982.73it/s]\u001b[A\n",
      " 14%|█▎        | 13138/95851 [00:01<00:11, 7135.41it/s]\u001b[A\n",
      " 15%|█▍        | 14108/95851 [00:01<00:11, 7267.38it/s]\u001b[A\n",
      " 16%|█▌        | 15055/95851 [00:02<00:10, 7363.47it/s]\u001b[A\n",
      " 17%|█▋        | 16069/95851 [00:02<00:10, 7492.76it/s]\u001b[A\n",
      " 18%|█▊        | 17030/95851 [00:02<00:10, 7563.25it/s]\u001b[A\n",
      " 19%|█▊        | 17972/95851 [00:02<00:10, 7598.38it/s]\u001b[A\n",
      " 20%|█▉        | 18881/95851 [00:02<00:10, 7635.62it/s]\u001b[A\n",
      " 21%|██        | 19771/95851 [00:02<00:09, 7657.78it/s]\u001b[A\n",
      " 22%|██▏       | 20639/95851 [00:02<00:09, 7690.64it/s]\u001b[A\n",
      " 23%|██▎       | 21574/95851 [00:02<00:09, 7750.34it/s]\u001b[A\n",
      " 23%|██▎       | 22479/95851 [00:02<00:09, 7794.78it/s]\u001b[A\n",
      " 24%|██▍       | 23370/95851 [00:02<00:09, 7823.59it/s]\u001b[A\n",
      " 25%|██▌       | 24266/95851 [00:03<00:09, 7859.13it/s]\u001b[A\n",
      " 26%|██▌       | 25155/95851 [00:03<00:08, 7891.17it/s]\u001b[A\n",
      " 27%|██▋       | 26042/95851 [00:03<00:08, 7914.32it/s]\u001b[A\n",
      " 28%|██▊       | 26922/95851 [00:03<00:08, 7925.08it/s]\u001b[A\n",
      " 29%|██▉       | 27813/95851 [00:03<00:08, 7953.89it/s]\u001b[A\n",
      " 30%|██▉       | 28707/95851 [00:03<00:08, 7981.15it/s]\u001b[A\n",
      " 31%|███       | 29634/95851 [00:03<00:08, 8015.87it/s]\u001b[A\n",
      " 32%|███▏      | 30527/95851 [00:03<00:08, 8009.28it/s]\u001b[A\n",
      " 33%|███▎      | 31387/95851 [00:03<00:08, 8003.32it/s]\u001b[A\n",
      " 34%|███▎      | 32243/95851 [00:04<00:07, 8017.07it/s]\u001b[A\n",
      " 35%|███▍      | 33085/95851 [00:04<00:07, 8017.42it/s]\u001b[A\n",
      " 35%|███▌      | 33915/95851 [00:04<00:07, 8017.10it/s]\u001b[A\n",
      " 36%|███▋      | 34798/95851 [00:04<00:07, 8035.78it/s]\u001b[A\n",
      " 37%|███▋      | 35682/95851 [00:04<00:07, 8053.09it/s]\u001b[A\n",
      " 38%|███▊      | 36550/95851 [00:04<00:07, 8067.36it/s]\u001b[A\n",
      " 39%|███▉      | 37408/95851 [00:04<00:07, 8072.66it/s]\u001b[A\n",
      " 40%|███▉      | 38257/95851 [00:04<00:07, 8080.17it/s]\u001b[A\n",
      " 41%|████      | 39185/95851 [00:04<00:06, 8105.02it/s]\u001b[A\n",
      " 42%|████▏     | 40057/95851 [00:04<00:06, 8107.45it/s]\u001b[A\n",
      " 43%|████▎     | 40992/95851 [00:05<00:06, 8131.92it/s]\u001b[A\n",
      " 44%|████▍     | 42032/95851 [00:05<00:06, 8175.62it/s]\u001b[A\n",
      " 45%|████▍     | 42960/95851 [00:05<00:06, 8176.78it/s]\u001b[A\n",
      " 46%|████▌     | 43857/95851 [00:05<00:06, 8167.64it/s]\u001b[A\n",
      " 47%|████▋     | 44817/95851 [00:05<00:06, 8193.81it/s]\u001b[A\n",
      " 48%|████▊     | 45707/95851 [00:05<00:06, 8200.92it/s]\u001b[A\n",
      " 49%|████▊     | 46655/95851 [00:05<00:05, 8223.51it/s]\u001b[A\n",
      " 50%|████▉     | 47561/95851 [00:05<00:05, 8237.81it/s]\u001b[A\n",
      " 51%|█████     | 48587/95851 [00:05<00:05, 8272.20it/s]\u001b[A\n",
      " 52%|█████▏    | 49526/95851 [00:05<00:05, 8286.19it/s]\u001b[A\n",
      " 53%|█████▎    | 50510/95851 [00:06<00:05, 8311.72it/s]\u001b[A\n",
      " 54%|█████▎    | 51467/95851 [00:06<00:05, 8331.91it/s]\u001b[A\n",
      " 55%|█████▍    | 52417/95851 [00:06<00:05, 8345.39it/s]\u001b[A\n",
      " 56%|█████▌    | 53356/95851 [00:06<00:05, 8351.25it/s]\u001b[A\n",
      " 57%|█████▋    | 54274/95851 [00:06<00:04, 8360.26it/s]\u001b[A\n",
      " 58%|█████▊    | 55184/95851 [00:06<00:04, 8364.68it/s]\u001b[A\n",
      " 59%|█████▊    | 56115/95851 [00:06<00:04, 8377.78it/s]\u001b[A\n",
      " 59%|█████▉    | 57020/95851 [00:06<00:04, 8384.23it/s]\u001b[A\n",
      " 60%|██████    | 57988/95851 [00:06<00:04, 8403.08it/s]\u001b[A\n",
      " 61%|██████▏   | 58907/95851 [00:07<00:04, 8411.99it/s]\u001b[A\n",
      " 62%|██████▏   | 59821/95851 [00:07<00:04, 8416.83it/s]\u001b[A\n",
      " 63%|██████▎   | 60726/95851 [00:07<00:04, 8425.65it/s]\u001b[A\n",
      " 64%|██████▍   | 61759/95851 [00:07<00:04, 8451.26it/s]\u001b[A\n",
      " 65%|██████▌   | 62734/95851 [00:07<00:03, 8468.73it/s]\u001b[A\n",
      " 66%|██████▋   | 63685/95851 [00:07<00:03, 8479.82it/s]\u001b[A\n",
      " 67%|██████▋   | 64630/95851 [00:07<00:03, 8478.94it/s]\u001b[A\n",
      " 68%|██████▊   | 65544/95851 [00:07<00:03, 8479.25it/s]\u001b[A\n",
      " 69%|██████▉   | 66439/95851 [00:07<00:03, 8473.13it/s]\u001b[A\n",
      " 70%|███████   | 67308/95851 [00:07<00:03, 8467.57it/s]\u001b[A\n",
      " 71%|███████   | 68158/95851 [00:08<00:03, 8459.74it/s]\u001b[A\n",
      " 72%|███████▏  | 69014/95851 [00:08<00:03, 8461.23it/s]\u001b[A\n",
      " 73%|███████▎  | 69885/95851 [00:08<00:03, 8464.35it/s]\u001b[A\n",
      " 74%|███████▍  | 70734/95851 [00:08<00:02, 8461.74it/s]\u001b[A\n",
      " 75%|███████▍  | 71607/95851 [00:08<00:02, 8464.58it/s]\u001b[A\n",
      " 76%|███████▌  | 72458/95851 [00:08<00:02, 8464.87it/s]\u001b[A\n",
      " 76%|███████▋  | 73313/95851 [00:08<00:02, 8465.72it/s]\u001b[A\n",
      " 77%|███████▋  | 74165/95851 [00:08<00:02, 8461.24it/s]\u001b[A\n",
      " 78%|███████▊  | 75004/95851 [00:08<00:02, 8458.10it/s]\u001b[A\n",
      " 79%|███████▉  | 75856/95851 [00:08<00:02, 8458.90it/s]\u001b[A\n",
      " 80%|████████  | 76695/95851 [00:09<00:02, 8457.35it/s]\u001b[A\n",
      " 81%|████████  | 77570/95851 [00:09<00:02, 8460.32it/s]\u001b[A\n",
      " 82%|████████▏ | 78418/95851 [00:09<00:02, 8459.51it/s]\u001b[A\n",
      " 83%|████████▎ | 79317/95851 [00:09<00:01, 8464.39it/s]\u001b[A\n",
      " 84%|████████▎ | 80247/95851 [00:09<00:01, 8473.22it/s]\u001b[A\n",
      " 85%|████████▍ | 81244/95851 [00:09<00:01, 8488.86it/s]\u001b[A\n",
      " 86%|████████▌ | 82160/95851 [00:09<00:01, 8493.31it/s]\u001b[A\n",
      " 87%|████████▋ | 83132/95851 [00:09<00:01, 8505.59it/s]\u001b[A\n",
      " 88%|████████▊ | 84059/95851 [00:09<00:01, 8502.65it/s]\u001b[A\n",
      " 89%|████████▊ | 84955/95851 [00:09<00:01, 8504.26it/s]Exception in thread Thread-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 95851/95851 [00:11<00:00, 8537.33it/s]\n",
      "100%|██████████| 226998/226998 [00:27<00:00, 8231.60it/s]\n"
     ]
    }
   ],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def stem_word(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def lemmatize_word(text):\n",
    "    return lemmatizer.lemmatize(text)\n",
    "\n",
    "def reduce_text(conversion, text):\n",
    "    return \" \".join(map(conversion, wordpunct_tokenize(text.lower())))\n",
    "\n",
    "def reduce_texts(conversion, texts):\n",
    "    return [reduce_text(conversion, str(text))\n",
    "            for text in tqdm(texts)]\n",
    "\n",
    "for mod in ['train', 'test']:\n",
    "    DataSet[mod]['comment_text_stemmed'] = reduce_texts(stem_word, DataSet[mod]['comment_text'])\n",
    "    #DataSet[mod]['comment_text_lemmatized'] = reduce_texts(lemmatize_word, DataSet[mod]['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T11:58:10.389945Z",
     "start_time": "2018-01-08T11:58:10.108749Z"
    }
   },
   "outputs": [],
   "source": [
    "label2binary = np.array([\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 1, 1, 0],\n",
    "    [0, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1],\n",
    "    [0, 0, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 1, 1],\n",
    "    [0, 0, 1, 1, 0, 0],\n",
    "    [0, 0, 1, 1, 0, 1],\n",
    "    [0, 0, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 1, 1, 1],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 1, 1],\n",
    "    [0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 1],\n",
    "    [0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 1, 1],\n",
    "    [0, 1, 1, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 1],\n",
    "    [0, 1, 1, 0, 1, 0],\n",
    "    [0, 1, 1, 0, 1, 1],\n",
    "    [0, 1, 1, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 0, 1],\n",
    "    [0, 1, 1, 1, 1, 0],\n",
    "    [0, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 1, 1],\n",
    "    [1, 0, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 1, 1, 0],\n",
    "    [1, 0, 0, 1, 1, 1],\n",
    "    [1, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 1],\n",
    "    [1, 0, 1, 0, 1, 0],\n",
    "    [1, 0, 1, 0, 1, 1],\n",
    "    [1, 0, 1, 1, 0, 0],\n",
    "    [1, 0, 1, 1, 0, 1],\n",
    "    [1, 0, 1, 1, 1, 0],\n",
    "    [1, 0, 1, 1, 1, 1],\n",
    "    [1, 1, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0, 1],\n",
    "    [1, 1, 0, 0, 1, 0],\n",
    "    [1, 1, 0, 0, 1, 1],\n",
    "    [1, 1, 0, 1, 0, 0],\n",
    "    [1, 1, 0, 1, 0, 1],\n",
    "    [1, 1, 0, 1, 1, 0],\n",
    "    [1, 1, 0, 1, 1, 1],\n",
    "    [1, 1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 0],\n",
    "    [1, 1, 1, 0, 1, 1],\n",
    "    [1, 1, 1, 1, 0, 0],\n",
    "    [1, 1, 1, 1, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 0],\n",
    "    [1, 1, 1, 1, 1, 1],\n",
    "])\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    columns = y_true.shape[1]\n",
    "    column_losses = []\n",
    "    for i in range(0, columns):\n",
    "        column_losses.append(log_loss(y_true[:, i], y_pred[:, i]))\n",
    "    return np.array(column_losses).mean()\n",
    "\n",
    "def cv(model, X, y, label2binary, n_splits=3):\n",
    "    def split(X, y):\n",
    "        return StratifiedKFold(n_splits=n_splits).split(X, y)\n",
    "    \n",
    "    def convert_y(y):\n",
    "        new_y = np.zeros([len(y)])\n",
    "        for i, val in enumerate(label2binary):\n",
    "            idx = (y == val).max(axis=1)\n",
    "            new_y[idx] = i\n",
    "        return new_y\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    scores = []\n",
    "    for train, test in tqdm(split(X, convert_y(y)), total=n_splits):\n",
    "        fitted_model = model(X[train], y[train])\n",
    "        scores.append(metric(y[test], fitted_model(X[test])))\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T11:58:10.395729Z",
     "start_time": "2018-01-08T11:58:10.392124Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## check\n",
    "# def dummy_model(X, y):\n",
    "#     def _predict(X):\n",
    "#         return np.ones([X.shape[0], 6]) * 0.5\n",
    "    \n",
    "#     return _predict\n",
    "\n",
    "# ret = cv(dummy_model,\n",
    "#    DataSet['train']['comment_text'],\n",
    "#    DataSet['train'][['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],\n",
    "#    label2binary)\n",
    "\n",
    "# print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T11:58:10.404925Z",
     "start_time": "2018-01-08T11:58:10.397712Z"
    }
   },
   "outputs": [],
   "source": [
    "# def regression_baseline(X, y):\n",
    "#     tfidf = TfidfVectorizer()\n",
    "#     X_tfidf = tfidf.fit_transform(X)\n",
    "#     columns = y.shape[1]\n",
    "#     regressions = [\n",
    "#         LogisticRegression().fit(X_tfidf, y[:, i])\n",
    "#         for i in range(columns)\n",
    "#     ]\n",
    "    \n",
    "#     def _predict(X):\n",
    "#         X_tfidf = tfidf.transform(X)\n",
    "#         predictions = np.zeros([len(X), columns])\n",
    "#         for i, regression in enumerate(regressions):\n",
    "#             regression_prediction = regression.predict_proba(X_tfidf)\n",
    "#             predictions[:, i] = regression_prediction[:, regression.classes_ == 1][:, 0]\n",
    "#         return predictions\n",
    "    \n",
    "#     return _predict\n",
    "\n",
    "# ret = cv(regression_baseline,\n",
    "#    DataSet['train']['comment_text'],\n",
    "#    DataSet['train'][['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],\n",
    "#    label2binary)\n",
    "# print(ret)\n",
    "\n",
    "# ret = cv(regression_baseline,\n",
    "#    DataSet['train']['comment_text_stemmed'],\n",
    "#    DataSet['train'][['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],\n",
    "#    label2binary)\n",
    "# print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T12:01:29.220287Z",
     "start_time": "2018-01-08T11:58:10.406454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:18<00:00, 66.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05256475  0.05343051  0.0537346 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def regression_wordchars(X, y):\n",
    "    tfidf_word = TfidfVectorizer()\n",
    "    X_tfidf_word = tfidf_word.fit_transform(X[:, 1])\n",
    "    tfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(1, 2), lowercase=False)\n",
    "    X_tfidf_char = tfidf_char.fit_transform(X[:, 0])\n",
    "    X_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])\n",
    "    \n",
    "    columns = y.shape[1]\n",
    "    regressions = [\n",
    "        LogisticRegression().fit(X_tfidf, y[:, i])\n",
    "        for i in range(columns)\n",
    "    ]\n",
    "    \n",
    "    def _predict(X):\n",
    "        X_tfidf_word = tfidf_word.transform(X[:, 1])\n",
    "        X_tfidf_char = tfidf_char.transform(X[:, 0])\n",
    "        X_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])\n",
    "        predictions = np.zeros([len(X), columns])\n",
    "        for i, regression in enumerate(regressions):\n",
    "            regression_prediction = regression.predict_proba(X_tfidf)\n",
    "            predictions[:, i] = regression_prediction[:, regression.classes_ == 1][:, 0]\n",
    "        return predictions\n",
    "    \n",
    "    return _predict\n",
    "\n",
    "ret = cv(regression_wordchars,\n",
    "   DataSet['train'][['comment_text', 'comment_text_stemmed']],\n",
    "   DataSet['train'][['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],\n",
    "   label2binary)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T12:04:58.775882Z",
     "start_time": "2018-01-08T12:03:37.292351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 2.41 s, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = regression_wordchars(np.array(DataSet['train'][['comment_text', 'comment_text_stemmed']]),\n",
    "                             np.array(DataSet['train'][['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T12:09:12.810058Z",
     "start_time": "2018-01-08T12:07:14.053840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 4.44 s, total: 1min 58s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction = model(np.array(DataSet['test'][['comment_text', 'comment_text_stemmed']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T12:12:46.748341Z",
     "start_time": "2018-01-08T12:12:46.687831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "226898  999544330298  0.032443      0.001476  0.009973  0.001276  0.020829   \n",
      "226899  999551526659  0.051080      0.006610  0.015649  0.001585  0.006678   \n",
      "226900  999557088193  0.023915      0.003601  0.008847  0.000857  0.008722   \n",
      "226901  999560902532  0.010306      0.003172  0.003391  0.001992  0.001695   \n",
      "226902  999567329075  0.019577      0.000853  0.009575  0.000454  0.004032   \n",
      "226903  999568036865  0.008346      0.001488  0.006877  0.002715  0.005354   \n",
      "226904  999569805515  0.035748      0.007173  0.028661  0.001075  0.023376   \n",
      "226905  999570016962  0.046290      0.003724  0.010511  0.006591  0.009025   \n",
      "226906  999574161238  0.016588      0.001744  0.009710  0.000879  0.011044   \n",
      "226907  999574286534  0.149694      0.038070  0.042902  0.002105  0.025391   \n",
      "226908  999576983525  0.559071      0.038655  0.151538  0.023705  0.202261   \n",
      "226909  999580611238  0.019359      0.001929  0.008800  0.001120  0.006475   \n",
      "226910  999583242114  0.082032      0.003366  0.014247  0.004256  0.036579   \n",
      "226911  999591416325  0.049303      0.000999  0.049473  0.000565  0.005823   \n",
      "226912  999594218896  0.027960      0.005417  0.009102  0.002271  0.014624   \n",
      "226913  999596225267  0.097621      0.006391  0.029065  0.000844  0.017077   \n",
      "226914  999599022128  0.028827      0.002335  0.011124  0.000791  0.019710   \n",
      "226915  999609257941  0.002214      0.000875  0.002423  0.000364  0.001322   \n",
      "226916  999613784133  0.005840      0.001801  0.002922  0.000389  0.001552   \n",
      "226917  999617476108  0.003904      0.000496  0.003905  0.000206  0.001983   \n",
      "226918  999627264419  0.989540      0.152287  0.998269  0.003466  0.985144   \n",
      "226919  999628397531  0.007526      0.002899  0.004065  0.000895  0.005761   \n",
      "226920  999635018058  0.036280      0.008643  0.022917  0.001883  0.008826   \n",
      "226921  999638272403  0.009946      0.001001  0.005218  0.000297  0.007175   \n",
      "226922  999644207085  0.058315      0.011944  0.032903  0.003938  0.057744   \n",
      "226923  999647296880  0.037998      0.001172  0.013121  0.001096  0.011146   \n",
      "226924  999655100014  0.004239      0.001282  0.004370  0.000541  0.002561   \n",
      "226925  999655405375  0.006452      0.002476  0.006197  0.001331  0.003351   \n",
      "226926  999656664980  0.023866      0.012455  0.015311  0.001914  0.005476   \n",
      "226927  999663559784  0.027751      0.007720  0.024626  0.001644  0.008442   \n",
      "...              ...       ...           ...       ...       ...       ...   \n",
      "226968  999890906666  0.020338      0.000789  0.008655  0.000482  0.005915   \n",
      "226969  999891110310  0.049200      0.013075  0.029324  0.004079  0.011731   \n",
      "226970  999901299717  0.068018      0.003706  0.034080  0.001475  0.016035   \n",
      "226971  999902107116  0.033716      0.001117  0.007093  0.000663  0.007398   \n",
      "226972  999902754612  0.004473      0.000917  0.002959  0.000486  0.001653   \n",
      "226973  999904175486  0.007202      0.000410  0.004330  0.000360  0.004933   \n",
      "226974  999905240338  0.076015      0.001932  0.005301  0.000325  0.010455   \n",
      "226975  999906741497  0.013129      0.003505  0.002948  0.002006  0.003305   \n",
      "226976  999911717743  0.012315      0.001392  0.007330  0.001379  0.003904   \n",
      "226977  999918907071  0.047728      0.018415  0.026547  0.002280  0.012685   \n",
      "226978  999920895395  0.001382      0.000382  0.000995  0.000166  0.000590   \n",
      "226979  999921181305  0.013642      0.001490  0.016525  0.000701  0.020117   \n",
      "226980  999921616383  0.145846      0.011081  0.029665  0.002344  0.092668   \n",
      "226981  999924773922  0.006660      0.002955  0.010765  0.001142  0.005414   \n",
      "226982  999927268950  0.013140      0.001410  0.007656  0.001229  0.005390   \n",
      "226983  999937028166  0.001497      0.000496  0.001963  0.000409  0.001131   \n",
      "226984  999939655437  0.000876      0.000204  0.001127  0.000127  0.000578   \n",
      "226985  999941555776  0.026617      0.005537  0.006403  0.002270  0.004352   \n",
      "226986  999942678927  0.004866      0.001614  0.001455  0.000910  0.000883   \n",
      "226987  999943838330  0.009589      0.004415  0.017715  0.001098  0.007350   \n",
      "226988  999956516304  0.040849      0.012111  0.017449  0.001661  0.012668   \n",
      "226989  999960677672  0.003148      0.000506  0.002514  0.000434  0.001939   \n",
      "226990  999963214716  0.003202      0.000693  0.004030  0.000386  0.002198   \n",
      "226991  999964222196  0.028176      0.001616  0.011749  0.000401  0.016240   \n",
      "226992  999964949747  0.002247      0.000813  0.003523  0.000488  0.001604   \n",
      "226993  999966872214  0.049452      0.006881  0.023807  0.001888  0.018369   \n",
      "226994  999968525410  0.015858      0.004251  0.009359  0.002248  0.007537   \n",
      "226995  999980053494  0.006965      0.003647  0.009785  0.001797  0.002764   \n",
      "226996  999980680364  0.017230      0.007192  0.018792  0.001095  0.007491   \n",
      "226997  999997819802  0.002353      0.000997  0.002056  0.000372  0.000948   \n",
      "\n",
      "        identity_hate  \n",
      "226898       0.006632  \n",
      "226899       0.003046  \n",
      "226900       0.004731  \n",
      "226901       0.000683  \n",
      "226902       0.001852  \n",
      "226903       0.003133  \n",
      "226904       0.015556  \n",
      "226905       0.002675  \n",
      "226906       0.002574  \n",
      "226907       0.008501  \n",
      "226908       0.027486  \n",
      "226909       0.005450  \n",
      "226910       0.003765  \n",
      "226911       0.001755  \n",
      "226912       0.002309  \n",
      "226913       0.004034  \n",
      "226914       0.003183  \n",
      "226915       0.000303  \n",
      "226916       0.000899  \n",
      "226917       0.000504  \n",
      "226918       0.023771  \n",
      "226919       0.002128  \n",
      "226920       0.003734  \n",
      "226921       0.001766  \n",
      "226922       0.008963  \n",
      "226923       0.005093  \n",
      "226924       0.000722  \n",
      "226925       0.001068  \n",
      "226926       0.004633  \n",
      "226927       0.005875  \n",
      "...               ...  \n",
      "226968       0.002296  \n",
      "226969       0.005450  \n",
      "226970       0.003315  \n",
      "226971       0.001805  \n",
      "226972       0.000817  \n",
      "226973       0.001654  \n",
      "226974       0.003703  \n",
      "226975       0.001405  \n",
      "226976       0.001160  \n",
      "226977       0.005860  \n",
      "226978       0.000687  \n",
      "226979       0.005011  \n",
      "226980       0.010117  \n",
      "226981       0.002778  \n",
      "226982       0.001300  \n",
      "226983       0.000523  \n",
      "226984       0.000631  \n",
      "226985       0.001400  \n",
      "226986       0.000513  \n",
      "226987       0.004270  \n",
      "226988       0.004567  \n",
      "226989       0.002419  \n",
      "226990       0.001019  \n",
      "226991       0.003505  \n",
      "226992       0.000271  \n",
      "226993       0.002342  \n",
      "226994       0.003302  \n",
      "226995       0.001863  \n",
      "226996       0.004122  \n",
      "226997       0.000303  \n",
      "\n",
      "[100 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = DataSet['test']['id']\n",
    "for i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
    "    submission[label] = prediction[:, i]\n",
    "# print(submission.tail(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T12:17:22.178186Z",
     "start_time": "2018-01-08T12:17:16.791307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip ../data/l0/lr_tfidf_word_char_2018-01-08.zip ../data/l0/lr_tfidf_word_char_2018-01-08.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys,os,datetime\n",
    "\n",
    "strategy = 'lr_tfidf_word_char'\n",
    "SubmitOutputDir = '../data/l0'\n",
    "if(os.path.exists(SubmitOutputDir) == False):\n",
    "    os.makedirs(SubmitOutputDir)\n",
    "SubmitFileName = '%s_%s' % (strategy, datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "submission.to_csv('%s.csv' % SubmitFileName, index= None)\n",
    "print('zip %s/%s.zip %s/%s.csv' % (SubmitOutputDir, SubmitFileName, SubmitOutputDir, SubmitFileName))\n",
    "os.system('zip %s/%s.zip %s/%s.csv' % (SubmitOutputDir, SubmitFileName, SubmitOutputDir, SubmitFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
