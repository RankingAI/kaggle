{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T15:26:23.862275Z",
     "start_time": "2017-12-31T15:26:23.777218Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "import scipy.sparse\n",
    "\n",
    "class DataUtil2:\n",
    "    \"\"\"\"\"\"\n",
    "    @classmethod\n",
    "    def load(cls, file, format, date_cols= None):\n",
    "        \"\"\"\"\"\"\n",
    "        data = ''\n",
    "        if(format== 'csv'):\n",
    "            data = pd.read_csv(file, parse_dates= date_cols)\n",
    "        elif(format== 'json'):\n",
    "            with open(file, 'r') as i_file:\n",
    "                data = json.load(file)\n",
    "            i_file.close()\n",
    "        elif(format== 'pkl'):\n",
    "            with open(file, 'rb') as i_file:\n",
    "                data = pickle.load(i_file)\n",
    "            i_file.close()\n",
    "        elif(format == 'hdf'):\n",
    "            data = pd.read_hdf(path_or_buf= file, key='undefined')\n",
    "#         elif(format == 'csr'):\n",
    "#             loader = np.load(file)\n",
    "#             data = csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])\n",
    "        elif(format == 'npz'):\n",
    "            data = scipy.sparse.load_npz(file)\n",
    "\n",
    "        return  data\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls, data, file, format, precision= 8):\n",
    "        \"\"\"\"\"\"\n",
    "        if(format == 'csv'):\n",
    "            data.to_csv(file, float_format= '%%.%df' % precision, index= False)\n",
    "        elif(format == 'json'):\n",
    "            with open(file, 'w') as o_file:\n",
    "                json.dump(data, o_file, ensure_ascii= True, indent= 4)\n",
    "            o_file.close()\n",
    "        elif(format == 'pkl'):\n",
    "            with open(file, 'wb') as o_file:\n",
    "                pickle.dump(data, o_file, -1)\n",
    "            o_file.close()\n",
    "        elif(format== 'hdf'):\n",
    "            data.to_hdf(path_or_buf= file, key='undefined', mode='w', complib='blosc')\n",
    "#         elif(format == 'csr'):\n",
    "#             np.savez(file, data= data['data'], indices= data['indices'], indptr= data['indptr'], shape= data['shape'])\n",
    "        elif(format == 'npz'):\n",
    "            scipy.sparse.save_npz(file, data)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T15:26:23.910016Z",
     "start_time": "2017-12-31T15:26:23.864187Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T15:26:23.981747Z",
     "start_time": "2017-12-31T15:26:23.912302Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_mdl(x, y):\n",
    "    y = y.values\n",
    "    p_1 = x[y == 1].sum(0)\n",
    "    pr_1 = (p_1 + 1) / ((y == 1).sum() + 1)\n",
    "    p_0 = x[y == 0].sum(0)\n",
    "    pr_0 = (p_0 + 1) / ((y == 0).sum() + 1)\n",
    "    r = np.log(pr_1 / pr_0)\n",
    "    m = LogisticRegression(C= 40, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n",
    "\n",
    "def ComputeAUC(truth, predict):\n",
    "    ''''''\n",
    "    n = len(truth)\n",
    "    #\n",
    "    pos_num = np.sum(truth)\n",
    "    neg_num = len(truth) - pos_num\n",
    "    #\n",
    "    pairs = zip(truth, predict)\n",
    "    sorted_pairs = sorted(pairs, key= lambda x: x[1])\n",
    "    sorted_truth = [s[0] for s in sorted_pairs]\n",
    "    #\n",
    "    auc = 0.0\n",
    "    x = np.zeros((n), dtype= 'float')\n",
    "    y = np.zeros((n), dtype= 'float')\n",
    "    x[0] = 1.0\n",
    "    y[0] = 1.0\n",
    "    for i in range(1, n):\n",
    "        a = (n - i - np.sum(sorted_truth[i:n]))/neg_num\n",
    "        b = np.sum(sorted_truth[i:n])/pos_num\n",
    "        x[i] = a\n",
    "        y[i] = b\n",
    "        #print(auc)\n",
    "        auc += ((y[i] + y[i - 1]) * (x[i - 1] - x[i]))/2.0\n",
    "    auc += (y[n - 1] * x[n - 1])/2.0\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T15:33:31.165066Z",
     "start_time": "2017-12-31T15:33:09.420108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading features for fold 0 done. time elapsed 1.38s\n",
      "loading xy for fold 0 done. time elapsed 1.54s\n",
      "loading features for fold 1 done. time elapsed 2.89s\n",
      "loading xy for fold 1 done. time elapsed 3.04s\n",
      "loading features for fold 2 done. time elapsed 4.39s\n",
      "loading xy for fold 2 done. time elapsed 4.53s\n",
      "loading features for fold 3 done. time elapsed 5.85s\n",
      "loading xy for fold 3 done. time elapsed 5.99s\n",
      "loading features for fold 4 done. time elapsed 7.30s\n",
      "loading xy for fold 4 done. time elapsed 7.45s\n",
      "load data done.\n",
      "train/valid = 62040/15511\n",
      "train/valid = 62040/15511\n",
      "fitting for toxic done.\n",
      "prediction for toxic done.\n",
      "0.65556077957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting for severe_toxic done.\n",
      "prediction for severe_toxic done.\n",
      "0.734764768639\n",
      "fitting for obscene done.\n",
      "prediction for obscene done.\n",
      "0.650730622618\n",
      "fitting for threat done.\n",
      "prediction for threat done.\n",
      "0.636340268863\n",
      "fitting for insult done.\n",
      "prediction for insult done.\n",
      "0.584748065867\n",
      "fitting for identity_hate done.\n",
      "prediction for identity_hate done.\n",
      "0.598625050456\n",
      "training for fold 0 done.\n",
      "save for valid done.\n",
      "saving for fold 0 done. cv_score 0.7959\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time, os, sys\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "DataBase = '../data'\n",
    "R = 3\n",
    "K = 5\n",
    "iformat = 'npz'\n",
    "oformat = 'csv'\n",
    "algo = 'nbsvm'\n",
    "strategy = 'tfidf'\n",
    "level = 0\n",
    "ilevel = 'l%s' % level\n",
    "olevel = 'l%s' % (level + 1)\n",
    "start = time.time()\n",
    "for r in range(R):\n",
    "    BootstrapInputDir = '%s/bootstrap/%s' % (DataBase, r)\n",
    "    # load data\n",
    "    valid_feats = []\n",
    "    holdout_feats = []\n",
    "    test_feats = []\n",
    "    valid_xy = []\n",
    "    holdout_xy = []\n",
    "    test_xy = []\n",
    "    for fold in range(K):\n",
    "        FoldInputDir = '%s/%s/kfold/%s' % (BootstrapInputDir, ilevel, fold)\n",
    "        TFIDFInputDir = '%s/%s' % (FoldInputDir, strategy)\n",
    "        # for features\n",
    "        valid = DataUtil2.load('%s/valid_feats.%s' % (TFIDFInputDir, iformat), iformat)\n",
    "        valid_feats.append(valid)\n",
    "        holdout = DataUtil2.load('%s/holdout_feats.%s' % (TFIDFInputDir, iformat), iformat)\n",
    "        holdout_feats.append(holdout)\n",
    "        test = DataUtil2.load('%s/test_feats.%s' % (TFIDFInputDir, iformat), iformat)\n",
    "        test_feats.append(test)\n",
    "        end = time.time()\n",
    "        print('loading features for fold %s done. time elapsed %.2fs' % (fold, (end - start)))\n",
    "        # for xy\n",
    "        valid = DataUtil2.load('%s/valid_xy.%s' % (TFIDFInputDir, 'csv'), 'csv')\n",
    "        valid['fold'] = fold\n",
    "        valid_xy.append(valid)\n",
    "        holdout = DataUtil2.load('%s/holdout_xy.%s' % (TFIDFInputDir, 'csv'), 'csv')\n",
    "        holdout_xy.append(holdout)\n",
    "        test = DataUtil2.load('%s/test_xy.%s' % (TFIDFInputDir, 'csv'), 'csv')\n",
    "        test_xy.append(test)\n",
    "        end = time.time()\n",
    "        print('loading xy for fold %s done. time elapsed %.2fs' % (fold, (end - start)))\n",
    "    valid_xy_df = pd.concat(valid_xy, axis= 0, ignore_index= True)\n",
    "    del valid_xy\n",
    "    gc.collect()\n",
    "    print('load data done.')\n",
    "    # scores for evaluation\n",
    "    cv_score = .0\n",
    "    holdout_score = .0\n",
    "    y_test_pred = 0\n",
    "    ## training\n",
    "    for fold in range(K):\n",
    "        #\n",
    "        FoldXData = {\n",
    "            'valid': valid_feats[fold],\n",
    "            'holdout': holdout_feats[fold],\n",
    "            'test': test_feats[fold]\n",
    "        }\n",
    "        FoldXData['train'] = sparse.vstack([valid_feats[i] for i in range(K) if(i != fold)], format= 'csr')\n",
    "        print('train/valid = %s/%s' % (FoldXData['train'].shape[0], FoldXData['valid'].shape[0]))\n",
    "        #\n",
    "        FoldYData = {\n",
    "            'train': valid_xy_df[valid_xy_df['fold'] != fold],\n",
    "            'valid': valid_xy_df[valid_xy_df['fold'] == fold],\n",
    "            'holdout': holdout_xy[fold],\n",
    "            'test': test_xy[fold]\n",
    "        }\n",
    "        print('train/valid = %s/%s' % (len(FoldYData['train']), len(FoldYData['valid'])))\n",
    "#         drop_cols = ['id', 'fold']\n",
    "#         drop_cols.extend(label_cols)\n",
    "#         feat_cols = [c for c in FoldData['train'].columns if(c not in drop_cols)]\n",
    "        targets = []\n",
    "        cv_logerror = 0.0\n",
    "        holdout_logerror = 0.0\n",
    "        for i in range(len(label_cols)):\n",
    "            # train\n",
    "#             model = NbSvmClassifier(C=4, dual=True, n_jobs=-1)\n",
    "#             model.fit(FoldXData['train'], FoldYData['train'][label_cols[i]])\n",
    "            m, r = get_mdl(FoldXData['train'], FoldYData['train'][label_cols[i]])\n",
    "            print('fitting for %s done.' % label_cols[i])\n",
    "            # for valid\n",
    "            target = 'pred_%s_%s_%s' % (algo, strategy, label_cols[i])\n",
    "            targets.append(target)\n",
    "#             FoldYData['valid'][target] = model.predict_proba(FoldXData['valid'])\n",
    "            FoldYData['valid'][target] = m.predict_proba(FoldXData['valid'].multiply(r))[:,1] #FoldXData['valid']\n",
    "            cv_logerror += -np.sum(np.log(FoldYData['valid'][target]) * FoldYData['valid'][label_cols[i]])\n",
    "            print('prediction for %s done.' % label_cols[i])\n",
    "            #print(FoldYData['valid'][[label_cols[i], target]].head(50))\n",
    "            #print('auc for %s is %.4f' % (label_cols[i], ComputeAUC(FoldYData['valid'][label_cols[i]], FoldYData['valid'][target])))\n",
    "            print(roc_auc_score(FoldYData['valid'][label_cols[i]], FoldYData['valid'][target]))\n",
    "        print('training for fold %s done.' % fold)\n",
    "        # evaluation\n",
    "        cv_logerror /= len(FoldYData['valid'])\n",
    "        cv_score += cv_logerror\n",
    "        # save\n",
    "        FoldOutputDir = '%s/%s/kfold/%s' % (BootstrapInputDir, olevel, fold)\n",
    "        if(os.path.exists(FoldOutputDir) == False):\n",
    "            os.makedirs(FoldOutputDir)\n",
    "        for mod in FoldYData.keys():\n",
    "            if(mod == 'train'):\n",
    "                continue\n",
    "            if(mod == 'valid'):\n",
    "                OutputFile = '%s/%s_%s_%s.%s' % (FoldOutputDir, mod, algo, strategy, oformat)\n",
    "                DataUtil2.save(FoldYData[mod][targets], OutputFile, oformat)\n",
    "                print('save for %s done.' % mod)\n",
    "        print('saving for fold %s done. cv_score %.4f' % (fold, cv_logerror))\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
