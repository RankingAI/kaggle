{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:23:55.283453Z",
     "start_time": "2018-01-11T09:23:55.194740Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import scipy.sparse\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm as tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy import sparse\n",
    "import time, os, sys\n",
    "\n",
    "class DataUtil2:\n",
    "    \"\"\"\"\"\"\n",
    "    @classmethod\n",
    "    def load(cls, file, format, date_cols= None):\n",
    "        \"\"\"\"\"\"\n",
    "        data = ''\n",
    "        if(format== 'csv'):\n",
    "            data = pd.read_csv(file, parse_dates= date_cols)\n",
    "        elif(format== 'json'):\n",
    "            with open(file, 'r') as i_file:\n",
    "                data = json.load(file)\n",
    "            i_file.close()\n",
    "        elif(format== 'pkl'):\n",
    "            with open(file, 'rb') as i_file:\n",
    "                data = pickle.load(i_file)\n",
    "            i_file.close()\n",
    "        elif(format == 'hdf'):\n",
    "            data = pd.read_hdf(path_or_buf= file, key='undefined')\n",
    "        elif(format == 'npz'):\n",
    "            data = scipy.sparse.load_npz(file)\n",
    "\n",
    "        return  data\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls, data, file, format, precision= 8):\n",
    "        \"\"\"\"\"\"\n",
    "        if(format == 'csv'):\n",
    "            data.to_csv(file, float_format= '%%.%df' % precision, index= False)\n",
    "        elif(format == 'json'):\n",
    "            with open(file, 'w') as o_file:\n",
    "                json.dump(data, o_file, ensure_ascii= True, indent= 4)\n",
    "            o_file.close()\n",
    "        elif(format == 'pkl'):\n",
    "            with open(file, 'wb') as o_file:\n",
    "                pickle.dump(data, o_file, -1)\n",
    "            o_file.close()\n",
    "        elif(format== 'hdf'):\n",
    "            data.to_hdf(path_or_buf= file, key='undefined', mode='w', complib='blosc')\n",
    "        elif(format == 'npz'):\n",
    "            scipy.sparse.save_npz(file, data)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:23:55.303325Z",
     "start_time": "2018-01-11T09:23:55.285442Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for i in range(len(new_list) - ngram_range + 1):\n",
    "            for ngram_value in range(2, ngram_range + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:23:57.001068Z",
     "start_time": "2018-01-11T09:23:55.304893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data done, time elapsed 1.69s\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "iformat = 'csv'\n",
    "oformat = 'hdf'\n",
    "DataBase = '../data'\n",
    "DataSet = {}\n",
    "start = time.time()\n",
    "for mod in ['train', 'test']:\n",
    "    DataSet[mod] = DataUtil2.load('%s/raw/%s.%s' % (DataBase, mod, iformat), iformat)\n",
    "    DataSet[mod]['comment_text'] = DataSet[mod]['comment_text'].fillna('nan')\n",
    "end = time.time()\n",
    "print('load data done, time elapsed %.2fs' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:24:34.125594Z",
     "start_time": "2018-01-11T09:23:57.002797Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95851/95851 [00:10<00:00, 8877.76it/s]\n",
      "100%|██████████| 226998/226998 [00:26<00:00, 8672.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming done, time elapsed 37.09s\n"
     ]
    }
   ],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def stem_word(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def lemmatize_word(text):\n",
    "    return lemmatizer.lemmatize(text)\n",
    "\n",
    "def reduce_text(conversion, text):\n",
    "    return \" \".join(map(conversion, wordpunct_tokenize(text.lower())))\n",
    "\n",
    "def reduce_texts(conversion, texts):\n",
    "    return [reduce_text(conversion, str(text))\n",
    "            for text in tqdm(texts)]\n",
    "\n",
    "start = time.time()\n",
    "for mod in ['train', 'test']:\n",
    "    DataSet[mod]['comment_text_stemmed'] = reduce_texts(stem_word, DataSet[mod]['comment_text'])\n",
    "    DataSet[mod]['comment_text_stemmed'] = DataSet[mod]['comment_text_stemmed'].fillna('nan')\n",
    "    DataSet[mod].drop('comment_text', axis= 1, inplace= True)\n",
    "end = time.time()\n",
    "print('stemming done, time elapsed %.2fs' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-11T14:27:56.928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW done, time elapsed 13.65s\n",
      "add n-gram done, time elapsed 13.65s\n",
      "Pad sequences (samples x time)\n",
      "train shape:  (95851, 65)\n",
      "test shape:  (226998, 65)\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 312418560 elements. This may consume a large amount of memory.\n",
      "  \"This may consume a large amount of memory.\" % num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95851 samples, validate on 10 samples\n",
      "Epoch 1/4\n"
     ]
    }
   ],
   "source": [
    "def MyTokenizer(data, number_words= 20000):\n",
    "    corpus = {}\n",
    "    for i in range(len(data)):\n",
    "        word_vec = data[i]\n",
    "        for j in range(len(word_vec)):\n",
    "            if(word_vec[j] not in corpus):\n",
    "                corpus[word_vec[j]] = 1\n",
    "            else:\n",
    "                corpus[word_vec[j]] += 1\n",
    "    top_words = sorted([(value, key) for (key,value) in corpus.items()])[-number_words:]\n",
    "    word_dict = dict([(word, idx) for (idx,word) in enumerate([w[1] for w in top_words])])\n",
    "    \n",
    "    def _transform(X):\n",
    "        for i in range(len(X)):\n",
    "            word_vec = X[i]\n",
    "            new_vec = []\n",
    "            for j in range(len(word_vec)):\n",
    "                if(word_vec[j] in word_dict):\n",
    "                    new_vec.append(word_dict[word_vec[j]])\n",
    "            X[i] = new_vec\n",
    "        return X\n",
    "    return _transform\n",
    "\n",
    "x_train = DataSet['train']['comment_text_stemmed']\n",
    "y_train = DataSet['train'][targets].values\n",
    "x_test = DataSet['test']['comment_text_stemmed']\n",
    "for target in targets:\n",
    "    DataSet['test'][target] = .0\n",
    "y_test = DataSet['test'][targets].values\n",
    "\n",
    "start = time.time()\n",
    "## bag-of-words\n",
    "x_train = x_train.str.split(' ').values\n",
    "x_test = x_test.str.split(' ').values\n",
    "tk = MyTokenizer(x_train)\n",
    "x_train = tk(x_train)\n",
    "x_test = tk(x_test)\n",
    "end = time.time()\n",
    "# print(x_train[:5])\n",
    "# print(x_test[:5])\n",
    "print('BOW done, time elapsed %.2fs' % (end - start))\n",
    "\n",
    "## add n-gram into feature space\n",
    "ngram_range = 1\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    print('current feture space %s' % max_features)\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    print('Average %s sequence length: %s' % (mod, np.mean(list(map(len, x_train)), dtype=int)))\n",
    "end = time.time()\n",
    "print('add n-gram done, time elapsed %.2fs' % (end - start))\n",
    "\n",
    "## building model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, SpatialDropout1D, Dropout\n",
    "from keras.layers import Embedding, GlobalMaxPool1D, BatchNormalization\n",
    "\n",
    "targets = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "maxlen = 65\n",
    "embedding_dims = 64\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "# padding into a smaller length\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('train shape: ' , x_train.shape)\n",
    "print('test shape: ' , x_test.shape)\n",
    "\n",
    "# print(type(y_train))\n",
    "# sys.exit(1)\n",
    "\n",
    "print('Build model...')\n",
    "comment_input = Input((maxlen,))\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "comment_emb = Embedding(max_features, embedding_dims, input_length=maxlen)(comment_input)\n",
    "\n",
    "# we add a GlobalMaxPool1D, which will extract information from the embeddings\n",
    "# of all words in the document\n",
    "comment_emb = SpatialDropout1D(0.25)(comment_emb)\n",
    "max_emb = GlobalMaxPool1D()(comment_emb)\n",
    "\n",
    "# normalized dense layer followed by dropout\n",
    "main = BatchNormalization()(max_emb)\n",
    "main = Dense(64)(main)\n",
    "main = Dropout(0.5)(main)\n",
    "\n",
    "# We project onto a six-unit output layer, and squash it with sigmoids:\n",
    "output = Dense(6, activation='sigmoid')(main)\n",
    "\n",
    "model = Model(inputs=comment_input, outputs=output)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "x_valid = x_train[-10:,]\n",
    "y_valid = y_train[-10:,]\n",
    "# print(x_valid)\n",
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data= (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:25:19.375404Z",
     "start_time": "2018-01-11T09:23:55.199Z"
    }
   },
   "outputs": [],
   "source": [
    "# label2binary = np.array([\n",
    "#     [0, 0, 0, 0, 0, 0],\n",
    "#     [0, 0, 0, 0, 0, 1],\n",
    "#     [0, 0, 0, 0, 1, 0],\n",
    "#     [0, 0, 0, 0, 1, 1],\n",
    "#     [0, 0, 0, 1, 0, 0],\n",
    "#     [0, 0, 0, 1, 0, 1],\n",
    "#     [0, 0, 0, 1, 1, 0],\n",
    "#     [0, 0, 0, 1, 1, 1],\n",
    "#     [0, 0, 1, 0, 0, 0],\n",
    "#     [0, 0, 1, 0, 0, 1],\n",
    "#     [0, 0, 1, 0, 1, 0],\n",
    "#     [0, 0, 1, 0, 1, 1],\n",
    "#     [0, 0, 1, 1, 0, 0],\n",
    "#     [0, 0, 1, 1, 0, 1],\n",
    "#     [0, 0, 1, 1, 1, 0],\n",
    "#     [0, 0, 1, 1, 1, 1],\n",
    "#     [0, 1, 0, 0, 0, 0],\n",
    "#     [0, 1, 0, 0, 0, 1],\n",
    "#     [0, 1, 0, 0, 1, 0],\n",
    "#     [0, 1, 0, 0, 1, 1],\n",
    "#     [0, 1, 0, 1, 0, 0],\n",
    "#     [0, 1, 0, 1, 0, 1],\n",
    "#     [0, 1, 0, 1, 1, 0],\n",
    "#     [0, 1, 0, 1, 1, 1],\n",
    "#     [0, 1, 1, 0, 0, 0],\n",
    "#     [0, 1, 1, 0, 0, 1],\n",
    "#     [0, 1, 1, 0, 1, 0],\n",
    "#     [0, 1, 1, 0, 1, 1],\n",
    "#     [0, 1, 1, 1, 0, 0],\n",
    "#     [0, 1, 1, 1, 0, 1],\n",
    "#     [0, 1, 1, 1, 1, 0],\n",
    "#     [0, 1, 1, 1, 1, 1],\n",
    "#     [1, 0, 0, 0, 0, 0],\n",
    "#     [1, 0, 0, 0, 0, 1],\n",
    "#     [1, 0, 0, 0, 1, 0],\n",
    "#     [1, 0, 0, 0, 1, 1],\n",
    "#     [1, 0, 0, 1, 0, 0],\n",
    "#     [1, 0, 0, 1, 0, 1],\n",
    "#     [1, 0, 0, 1, 1, 0],\n",
    "#     [1, 0, 0, 1, 1, 1],\n",
    "#     [1, 0, 1, 0, 0, 0],\n",
    "#     [1, 0, 1, 0, 0, 1],\n",
    "#     [1, 0, 1, 0, 1, 0],\n",
    "#     [1, 0, 1, 0, 1, 1],\n",
    "#     [1, 0, 1, 1, 0, 0],\n",
    "#     [1, 0, 1, 1, 0, 1],\n",
    "#     [1, 0, 1, 1, 1, 0],\n",
    "#     [1, 0, 1, 1, 1, 1],\n",
    "#     [1, 1, 0, 0, 0, 0],\n",
    "#     [1, 1, 0, 0, 0, 1],\n",
    "#     [1, 1, 0, 0, 1, 0],\n",
    "#     [1, 1, 0, 0, 1, 1],\n",
    "#     [1, 1, 0, 1, 0, 0],\n",
    "#     [1, 1, 0, 1, 0, 1],\n",
    "#     [1, 1, 0, 1, 1, 0],\n",
    "#     [1, 1, 0, 1, 1, 1],\n",
    "#     [1, 1, 1, 0, 0, 0],\n",
    "#     [1, 1, 1, 0, 0, 1],\n",
    "#     [1, 1, 1, 0, 1, 0],\n",
    "#     [1, 1, 1, 0, 1, 1],\n",
    "#     [1, 1, 1, 1, 0, 0],\n",
    "#     [1, 1, 1, 1, 0, 1],\n",
    "#     [1, 1, 1, 1, 1, 0],\n",
    "#     [1, 1, 1, 1, 1, 1],\n",
    "# ])\n",
    "\n",
    "# def metric(y_true, y_pred):\n",
    "#     assert y_true.shape == y_pred.shape\n",
    "#     columns = y_true.shape[1]\n",
    "#     column_losses = []\n",
    "#     for i in range(0, columns):\n",
    "#         column_losses.append(log_loss(y_true[:, i], y_pred[:, i]))\n",
    "#     return np.array(column_losses).mean()\n",
    "\n",
    "# def cv(model, X, y, label2binary, n_splits=3):\n",
    "#     def split(X, y):\n",
    "#         return StratifiedKFold(n_splits=n_splits).split(X, y)\n",
    "    \n",
    "#     def convert_y(y):\n",
    "#         new_y = np.zeros([len(y)])\n",
    "#         for i, val in enumerate(label2binary):\n",
    "#             idx = (y == val).max(axis=1)\n",
    "#             new_y[idx] = i\n",
    "#         return new_y\n",
    "    \n",
    "#     X = np.array(X)\n",
    "#     y = np.array(y)\n",
    "#     scores = []\n",
    "#     for train, test in tqdm(split(X, convert_y(y)), total=n_splits):\n",
    "#         fitted_model = model(X[train], y[train])\n",
    "#         scores.append(metric(y[test], fitted_model(X[test])))\n",
    "#     return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:25:19.376250Z",
     "start_time": "2018-01-11T09:23:55.200Z"
    }
   },
   "outputs": [],
   "source": [
    "# def regression_wordchars(X, y):\n",
    "#     tfidf_word = TfidfVectorizer(\n",
    "#         sublinear_tf=True,\n",
    "#         strip_accents='unicode',\n",
    "#         analyzer='word',\n",
    "#         min_df=3, \n",
    "#         max_df=0.9,\n",
    "#         use_idf= 1,\n",
    "#         smooth_idf= 1,\n",
    "#         ngram_range=(1,1),\n",
    "#         max_features=20000\n",
    "#     )\n",
    "#     X_tfidf_word = tfidf_word.fit_transform(X[:, 1])\n",
    "#     tfidf_char = TfidfVectorizer(\n",
    "#         sublinear_tf=True,\n",
    "#         strip_accents='unicode',\n",
    "#         analyzer='char', \n",
    "#         ngram_range=(1, 4),\n",
    "#         max_features=20000,\n",
    "#         lowercase=False)\n",
    "#     X_tfidf_char = tfidf_char.fit_transform(X[:, 0])\n",
    "#     X_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])\n",
    "    \n",
    "#     columns = y.shape[1]\n",
    "#     regressions = [\n",
    "#         LogisticRegression(C= 4).fit(X_tfidf, y[:, i])\n",
    "#         for i in range(columns)\n",
    "#     ]\n",
    "    \n",
    "#     def _predict(X):\n",
    "#         X_tfidf_word = tfidf_word.transform(X[:, 1])\n",
    "#         X_tfidf_char = tfidf_char.transform(X[:, 0])\n",
    "#         X_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])\n",
    "#         predictions = np.zeros([len(X), columns])\n",
    "#         for i, regression in enumerate(regressions):\n",
    "#             regression_prediction = regression.predict_proba(X_tfidf)\n",
    "#             predictions[:, i] = regression_prediction[:, regression.classes_ == 1][:, 0]\n",
    "#         return predictions\n",
    "    \n",
    "#     return _predict\n",
    "\n",
    "# ret = cv(regression_wordchars,\n",
    "#    DataSet['train'][['comment_text', 'comment_text_stemmed']],\n",
    "#    DataSet['train'][['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],\n",
    "#    label2binary)\n",
    "# print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:25:19.377302Z",
     "start_time": "2018-01-11T09:23:55.201Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = regression_wordchars(np.array(DataSet['train'][['comment_text', 'comment_text_stemmed']]),\n",
    "#                              np.array(DataSet['train'][['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:25:19.378294Z",
     "start_time": "2018-01-11T09:23:55.202Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# prediction = model(np.array(DataSet['test'][['comment_text', 'comment_text_stemmed']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:25:19.379252Z",
     "start_time": "2018-01-11T09:23:55.203Z"
    }
   },
   "outputs": [],
   "source": [
    "# submission = pd.DataFrame()\n",
    "# submission['id'] = DataSet['test']['id']\n",
    "# for i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
    "#     submission[label] = prediction[:, i]\n",
    "# # print(submission.tail(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:25:19.380227Z",
     "start_time": "2018-01-11T09:23:55.203Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sys,os,datetime\n",
    "\n",
    "# strategy = 'lr_tfidf_word_char'\n",
    "# SubmitOutputDir = '../data/l0'\n",
    "# if(os.path.exists(SubmitOutputDir) == False):\n",
    "#     os.makedirs(SubmitOutputDir)\n",
    "# SubmitFileName = '%s_%s' % (strategy, datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "# submission.to_csv('%s/%s.csv' % (SubmitOutputDir, SubmitFileName), index= None)\n",
    "# print('zip %s/%s.zip %s/%s.csv' % (SubmitOutputDir, SubmitFileName, SubmitOutputDir, SubmitFileName))\n",
    "# os.system('zip %s/%s.zip %s/%s.csv' % (SubmitOutputDir, SubmitFileName, SubmitOutputDir, SubmitFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
