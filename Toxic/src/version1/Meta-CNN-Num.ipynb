{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-11T09:57:15.429Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data for fold 0 done.\n",
      "load data for fold 1 done.\n",
      "load data for fold 2 done.\n",
      "load data for fold 3 done.\n",
      "load data done, train 159571, time elapsed 1.0719916820526123\n",
      "load embedding features done, corpus size 2000000, time elapsed 76.90698409080505\n",
      "====== fold 0 ======\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119662 samples, validate on 39909 samples\n",
      "Epoch 1/3\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.985881 \n",
      "\n",
      "31s - loss: 0.0632 - acc: 0.9781 - val_loss: 0.0465 - val_acc: 0.9822\n",
      "Epoch 2/3\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.988516 \n",
      "\n",
      "30s - loss: 0.0421 - acc: 0.9840 - val_loss: 0.0482 - val_acc: 0.9815\n",
      "Epoch 3/3\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.988644 \n",
      "\n",
      "30s - loss: 0.0369 - acc: 0.9856 - val_loss: 0.0432 - val_acc: 0.9834\n",
      "fitting done, time elapsed 104.09131240844727.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:340: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/joe/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0, score 0.98864, time elapsed 111.42s\n",
      "====== fold 1 ======\n",
      "\n",
      "Train on 119671 samples, validate on 39900 samples\n",
      "Epoch 1/3\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.985084 \n",
      "\n",
      "31s - loss: 0.0615 - acc: 0.9789 - val_loss: 0.0478 - val_acc: 0.9821\n",
      "Epoch 2/3\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.987385 \n",
      "\n",
      "30s - loss: 0.0420 - acc: 0.9839 - val_loss: 0.0427 - val_acc: 0.9836\n",
      "Epoch 3/3\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.987659 \n",
      "\n",
      "30s - loss: 0.0366 - acc: 0.9858 - val_loss: 0.0453 - val_acc: 0.9826\n",
      "fitting done, time elapsed 215.92129468917847.\n",
      "fold 1, score 0.98766, time elapsed 223.33s\n",
      "====== fold 2 ======\n",
      "\n",
      "Train on 119684 samples, validate on 39887 samples\n",
      "Epoch 1/3\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.982749 \n",
      "\n",
      "31s - loss: 0.0616 - acc: 0.9789 - val_loss: 0.0480 - val_acc: 0.9822\n",
      "Epoch 2/3\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.985942 \n",
      "\n",
      "30s - loss: 0.0421 - acc: 0.9839 - val_loss: 0.0434 - val_acc: 0.9834\n",
      "Epoch 3/3\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.985557 \n",
      "\n",
      "30s - loss: 0.0368 - acc: 0.9857 - val_loss: 0.0451 - val_acc: 0.9829\n",
      "fitting done, time elapsed 327.9664921760559.\n",
      "fold 2, score 0.98556, time elapsed 335.37s\n",
      "====== fold 3 ======\n",
      "\n",
      "Train on 119696 samples, validate on 39875 samples\n",
      "Epoch 1/3\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.986271 \n",
      "\n",
      "31s - loss: 0.0606 - acc: 0.9793 - val_loss: 0.0455 - val_acc: 0.9828\n",
      "Epoch 2/3\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.988126 \n",
      "\n",
      "30s - loss: 0.0425 - acc: 0.9837 - val_loss: 0.0427 - val_acc: 0.9834\n",
      "Epoch 3/3\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.987923 \n",
      "\n",
      "30s - loss: 0.0372 - acc: 0.9854 - val_loss: 0.0458 - val_acc: 0.9821\n",
      "fitting done, time elapsed 439.95237255096436.\n",
      "fold 3, score 0.98792, time elapsed 447.43s\n",
      "\n",
      "================\n",
      "cv score 0.98745,  time elapsed 447.43546772003174\n",
      "================\n",
      "zip ../../data/version2/l0/submit/cnn-num_submit_2018-03-16.zip ../../data/version2/l0/submit/cnn-num_submit_2018-03-16.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import os,sys,time,datetime\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.special import erfinv\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "DataBaseDir = '../../data/version2'\n",
    "InputDir = '%s/l0/kfold' % DataBaseDir\n",
    "OutputDir = '%s/l1' % DataBaseDir\n",
    "kfold = 4\n",
    "strategy = 'cnn-num'\n",
    "# load data\n",
    "start = time.time()\n",
    "valid_dfs = []\n",
    "for fold in range(kfold):\n",
    "    FoldInputDir = '%s/%s' % (InputDir, fold)\n",
    "    valid = pd.read_csv('%s/valid.csv' % FoldInputDir).reset_index(drop= True)#.sample(frac= 0.1)\n",
    "    ## for valid/holdout data set\n",
    "    if(fold == 0):\n",
    "        TestData = pd.read_csv('%s/test.csv' % FoldInputDir).reset_index(drop= True)#.sample(frac= 0.1)\n",
    "    valid['fold'] = fold\n",
    "    valid_dfs.append(valid)\n",
    "    print('load data for fold %s done.' % fold)\n",
    "TrainData = pd.concat(valid_dfs, axis= 0, ignore_index= True)\n",
    "end = time.time()\n",
    "print('load data done, train %s, time elapsed %s' % (len(TrainData), (end - start)))\n",
    "##\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def LoadEmbeddingVectors(f):\n",
    "    ## debug\n",
    "    k = 1000\n",
    "    EmbeddingDict = {}\n",
    "    with open(f, 'r') as i_file:\n",
    "        for line in i_file:\n",
    "            #if(k == 0):\n",
    "            #    break\n",
    "            w, coe_vec= get_coefs(*line.rstrip().rsplit(' '))\n",
    "            EmbeddingDict[w] = coe_vec\n",
    "            k -= 1\n",
    "    i_file.close()\n",
    "    return EmbeddingDict\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "EmbeddingFile = '../../data/raw/crawl-300d-2M.vec'\n",
    "max_features = 80000\n",
    "maxlen = 256\n",
    "#max_features = 3000\n",
    "#maxlen = 10\n",
    "embed_size = 300\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "start = time.time()\n",
    "EmbeddingIndex = LoadEmbeddingVectors(EmbeddingFile)\n",
    "end = time.time()\n",
    "print('load embedding features done, corpus size %s, time elapsed %s' % (len(EmbeddingIndex), (end - start)))\n",
    "\n",
    "pred_cols = ['%s_%s' % (strategy, c) for c in targets]\n",
    "for c in pred_cols:\n",
    "    TestData[c] = .0\n",
    "\n",
    "def get_num_feats(df):\n",
    "    # Get length in words and characters\n",
    "    df[\"raw_word_len\"] = df[\"comment_text\"].apply(lambda x: len(x.split())) + 1\n",
    "    df[\"raw_char_len\"] = df[\"comment_text\"].apply(lambda x: len(x)) + 1\n",
    "    # Count number of \\n\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    df['ant_slash_n_ratio'] = df[\"ant_slash_n\"]/df[\"raw_char_len\"]\n",
    "    # Check number of upper case, if you're angry you may write in upper case\n",
    "    df[\"nb_upper\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n",
    "    df['nb_upper_ratio'] = df[\"nb_upper\"]/df[\"raw_char_len\"]\n",
    "    # Number of F words - f..k contains folk, fork,\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    df['nb_fk_ratio'] = df[\"nb_fk\"]/df['raw_word_len']\n",
    "    # Number of S word\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    df['nb_sk_ratio'] = df[\"nb_sk\"]/df['raw_word_len']\n",
    "    # Number of D words\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    df['nb_dk_ratio'] = df['nb_dk']/df['raw_word_len']\n",
    "    # Number of occurence of You, insulting someone usually needs someone called : you\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    df['nb_you_ratio'] = df[\"nb_you\"]/df['raw_word_len']\n",
    "    \n",
    "    # Just to check you really refered to my mother ;-)\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    df['nb_mother_ratio'] = df[\"nb_mother\"]/df['raw_word_len']\n",
    "    # Just checking for toxic 19th century vocabulary\n",
    "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n",
    "    df['nb_ng_ratio'] = df[\"nb_ng\"]/df['raw_word_len']\n",
    "    # Some Sentences start with a <:> so it may help\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    df['start_with_columns_ratio'] = df[\"start_with_columns\"]/(1 + df[\"ant_slash_n\"])\n",
    "    \n",
    "    df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['question_mask_ratio'] = df['num_question_marks']/df[\"raw_char_len\"]\n",
    "    \n",
    "    df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['exclamation_mark_ratio'] = df['num_exclamation_marks']/df[\"raw_char_len\"]\n",
    "    \n",
    "    df['num_punctuation'] = df['comment_text'].apply( lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "    df['punctuation_ratio'] = df['num_punctuation']/df[\"raw_char_len\"]\n",
    "    \n",
    "    df['imcomplete_punctuation'] = df['comment_text'].apply( lambda comment: sum(comment.count(w) for w in '*,#,$'))\n",
    "    df['imcomplete_punctuation_ratio'] = df['imcomplete_punctuation']/df[\"raw_char_len\"]\n",
    "    \n",
    "    \n",
    "    ##\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['raw_word_len']\n",
    "    df['num_smilies'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "    df['similes_ratio'] = df['num_smilies'] / df['raw_word_len']\n",
    "    \n",
    "    df[\"count_standard_punctuations\"] = df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    df[\"standard_punctuations_ratio\"] = df[\"count_standard_punctuations\"]/df['raw_char_len']\n",
    "    df[\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    df[\"words_title_ratio\"] = df[\"count_words_title\"]/df['raw_word_len']\n",
    "    \n",
    "    df['unique_words_greater_200'] = (df['num_unique_words'] > 200).astype(int)\n",
    "    \n",
    "    # Check for time stamp\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    # Check for dates 18:44, 8 December 2010\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for date short 8 December 2010\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for http links\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    # check for mail\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(\n",
    "        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n",
    "    )\n",
    "#     # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "TrainData = get_num_feats(TrainData)\n",
    "TestData = get_num_feats(TestData)\n",
    "\n",
    "## numeric features\n",
    "tmp_cols = ['id', 'fold', 'comment_text']\n",
    "tmp_cols.extend(targets)\n",
    "num_feats = [c for c in TrainData.columns if(c not in tmp_cols)]\n",
    "\n",
    "# print(num_feats)\n",
    "# sys.exit(1)\n",
    "\n",
    "## MinMax Normalization\n",
    "# entire_num_df = pd.concat([TrainData[num_feats], TestData[num_feats]], axis= 0, ignore_index= True)\n",
    "# scaler = MinMaxScaler().fit(entire_num_df)\n",
    "# TrainData[num_feats] = scaler.transform(TrainData[num_feats].values)\n",
    "# TestData[num_feats] = scaler.transform(TestData[num_feats].values)\n",
    "\n",
    "## Guass Rank Normalization\n",
    "TrainData['source'] = 'train'\n",
    "TestData['source'] = 'test'\n",
    "# tmp_cols = num_feats.copy()\n",
    "# tmp_cols.append('source')\n",
    "all_data = pd.concat([TrainData, TestData], ignore_index= True)\n",
    "for c in num_feats:\n",
    "    rank = np.argsort(all_data[c], axis= 0)\n",
    "    upper = np.max(rank)\n",
    "    lower = np.min(rank)\n",
    "    # linear normalization to 0-1\n",
    "    all_data[c] = (all_data[c] - lower)/(upper - lower)\n",
    "    # gauss normalization\n",
    "    all_data[c] = erfinv(all_data[c])\n",
    "    all_data[c] -= np.mean(all_data[c])\n",
    "\n",
    "TrainData = all_data[all_data['source'] == 'train'].drop(['source'], axis= 1)\n",
    "TestData = all_data[all_data['source'] == 'test'].drop(['source'], axis= 1)\n",
    "\n",
    "## tokenized features\n",
    "tokenizer = text.Tokenizer(num_words= max_features)\n",
    "EntireCorpus = list(TrainData['comment_text'].values) + list(TestData['comment_text'].values)\n",
    "tokenizer.fit_on_texts(EntireCorpus)\n",
    "\n",
    "## embedding with pre-trained embedding library\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = EmbeddingIndex.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "def get_model(X_input):\n",
    "    # input for token words\n",
    "    inp_token = Input(shape=(maxlen, ), name= 'token_words')\n",
    "    # input for num feats\n",
    "    inp_num = Input(shape=[X_input['num_feats'].shape[1]], name= \"num_feats\")\n",
    "    # embedding for token words\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp_token)\n",
    "    x = SpatialDropout1D(0.5)(x)\n",
    "    \n",
    "    x1 = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    \n",
    "    x2 = Conv1D(128, kernel_size = 2, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "\n",
    "    x3 = Conv1D(32, kernel_size = 4, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    \n",
    "    x4 = Conv1D(16, kernel_size = 5, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    \n",
    "    avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    \n",
    "    avg_pool3 = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3 = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    avg_pool4 = GlobalAveragePooling1D()(x4)\n",
    "    max_pool4 = GlobalMaxPooling1D()(x4)\n",
    "    \n",
    "    # concate results of bi-gru with num features\n",
    "    conc = concatenate([avg_pool1, max_pool1, \n",
    "                        avg_pool2, max_pool2,\n",
    "                        avg_pool3, max_pool3,\n",
    "                        avg_pool4, max_pool4,\n",
    "                        inp_num])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    # \n",
    "    model = Model(inputs= [inp_token, inp_num], outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer= Adam(lr=1e-3),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "##\n",
    "cv_score = .0\n",
    "start = time.time()\n",
    "for fold in range(kfold):\n",
    "    print('====== fold %s ======\\n' % fold)\n",
    "    FoldData = {\n",
    "        'train': TrainData[TrainData['fold'] != fold],\n",
    "        'valid': TrainData[TrainData['fold'] == fold],\n",
    "        'test': TestData\n",
    "    }\n",
    "    for c in pred_cols:\n",
    "        FoldData['valid'][c] = .0\n",
    "        FoldData['test'][c] = .0\n",
    "    ## input for X\n",
    "    X_train_input = {}\n",
    "    X_valid_input = {}\n",
    "    X_test_input = {}\n",
    "    ## tokenize with entire corpus composed by train/valid/test\n",
    "    X_train_input['token_words'] = tokenizer.texts_to_sequences(FoldData['train']['comment_text'].values)\n",
    "    X_valid_input['token_words'] = tokenizer.texts_to_sequences(FoldData['valid']['comment_text'].values)\n",
    "    X_test_input['token_words'] = tokenizer.texts_to_sequences(FoldData['test']['comment_text'].values)\n",
    "    \n",
    "    X_train_input['token_words'] = sequence.pad_sequences(X_train_input['token_words'], maxlen= maxlen)\n",
    "    X_valid_input['token_words'] = sequence.pad_sequences(X_valid_input['token_words'], maxlen= maxlen)\n",
    "    X_test_input['token_words'] = sequence.pad_sequences(X_test_input['token_words'], maxlen= maxlen)\n",
    "\n",
    "    ## num data\n",
    "    X_train_input['num_feats'] = FoldData['train'][num_feats].values\n",
    "    X_valid_input['num_feats'] = FoldData['valid'][num_feats].values\n",
    "    X_test_input['num_feats'] = FoldData['test'][num_feats].values\n",
    "    \n",
    "    ## input for Y\n",
    "    Y_train = FoldData['train'][targets].values\n",
    "    Y_valid = FoldData['valid'][targets].values\n",
    "    \n",
    "    ## construct bi-gru model\n",
    "    model = get_model(X_train_input)\n",
    "    RocAuc = RocAucEvaluation(validation_data= (X_valid_input, Y_valid), interval=1)\n",
    "    hist = model.fit(X_train_input, Y_train, \n",
    "                     batch_size= batch_size, \n",
    "                     epochs= epochs, \n",
    "                     validation_data= (X_valid_input, Y_valid),\n",
    "                     callbacks=[RocAuc],\n",
    "                     verbose=2)\n",
    "    end = time.time()\n",
    "    print('fitting done, time elapsed %s.' % (end - start))\n",
    "    ## predict for valid\n",
    "    pred_valid = model.predict(X_valid_input, batch_size=1024)\n",
    "    FoldData['valid'][pred_cols] = pred_valid\n",
    "    ## predict for test\n",
    "    pred_test = model.predict(X_test_input, batch_size=1024)\n",
    "    FoldData['test'][pred_cols] = pred_test\n",
    "    TestData[pred_cols] += pred_test\n",
    "    ## evaluate\n",
    "    score = roc_auc_score(FoldData['valid'][targets], FoldData['valid'][pred_cols])\n",
    "    cv_score += score\n",
    "    ## output\n",
    "    FoldOutputDir = '%s/kfold/%s' % (OutputDir, fold)\n",
    "    if(os.path.exists(FoldOutputDir) == False):\n",
    "        os.makedirs(FoldOutputDir)\n",
    "    for mod in ['valid', 'test']:\n",
    "        if(mod == 'test'):\n",
    "            out_cols = ['id']\n",
    "            out_cols.extend(pred_cols)\n",
    "        else:\n",
    "            out_cols = pred_cols.copy()\n",
    "            out_cols.extend(targets)\n",
    "        FoldData[mod][out_cols].to_csv('%s/%s_%s.csv' % (FoldOutputDir, mod, strategy),float_format='%.8f', index= False) \n",
    "    end = time.time()\n",
    "    print('fold %s, score %.5f, time elapsed %.2fs' % (fold, score, (end - start)))\n",
    "cv_score /= kfold\n",
    "TestData[pred_cols] /= kfold\n",
    "end = time.time()\n",
    "\n",
    "print('\\n================')\n",
    "print('cv score %.5f,  time elapsed %s' % (cv_score, (end - start)))\n",
    "print('================')\n",
    "\n",
    "## submit\n",
    "sub = TestData[['id']].copy()\n",
    "sub[targets] = TestData[pred_cols]\n",
    "OutputFileName = '%s_submit_%s' % (strategy, datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "SubmitDir = '%s/l0/submit' % DataBaseDir\n",
    "if(os.path.exists(SubmitDir) == False):\n",
    "    os.makedirs(SubmitDir)\n",
    "sub.to_csv('%s/%s.csv' % (SubmitDir, OutputFileName), float_format='%.8f', index=False)\n",
    "print('zip %s/%s.zip %s/%s.csv' % (SubmitDir, OutputFileName, SubmitDir, OutputFileName))\n",
    "os.system('zip %s/%s.zip %s/%s.csv' % (SubmitDir, OutputFileName, SubmitDir, OutputFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
