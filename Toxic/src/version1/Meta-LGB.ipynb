{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T11:48:38.543918Z",
     "start_time": "2018-03-10T11:46:01.842605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data for fold 0 done.\n",
      "load data for fold 1 done.\n",
      "load data for fold 2 done.\n",
      "load data for fold 3 done.\n",
      "load data done, train 159571, test 153164\n",
      "[load data] done in 1 s\n",
      "[pre-processing] done in 108 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import psutil\n",
    "import lightgbm\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    \"\"\"\n",
    "    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n",
    "    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n",
    "    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "def prepare_for_char_n_gram(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "def get_indicators_and_clean_comments(df):\n",
    "    \"\"\"\n",
    "    Check all sorts of content as it may help find toxic comment\n",
    "    Though I'm not sure all of them improve scores\n",
    "    \"\"\"\n",
    "    # Count number of \\n\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    # Get length in words and characters\n",
    "    df[\"raw_word_len\"] = df[\"comment_text\"].apply(lambda x: len(x.split()))\n",
    "    df[\"raw_char_len\"] = df[\"comment_text\"].apply(lambda x: len(x))\n",
    "    # Check number of upper case, if you're angry you may write in upper case\n",
    "    df[\"nb_upper\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n",
    "    # Number of F words - f..k contains folk, fork,\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    # Number of S word\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    # Number of D words\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    # Number of occurence of You, insulting someone usually needs someone called : you\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    # Just to check you really refered to my mother ;-)\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    # Just checking for toxic 19th century vocabulary\n",
    "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n",
    "    # Some Sentences start with a <:> so it may help\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    # Check for time stamp\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    # Check for dates 18:44, 8 December 2010\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for date short 8 December 2010\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for http links\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    # check for mail\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(\n",
    "        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n",
    "    )\n",
    "    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "\n",
    "    # Now clean comments\n",
    "    df[\"clean_comment\"] = df[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))\n",
    "\n",
    "    # Get the new length in words and characters\n",
    "    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n",
    "    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n",
    "    # Number of different characters used in a comment\n",
    "    # Using the f word only will reduce the number of letters required in the comment\n",
    "    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n",
    "    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) / df[\"clean_comment\"].apply(\n",
    "        lambda x: 1 + min(99, len(x)))\n",
    "\n",
    "def char_analyzer(text):\n",
    "    \"\"\"\n",
    "    This is used to split strings in small lots\n",
    "    I saw this in an article (I can't find the link anymore)\n",
    "    so <talk> and <talking> would have <Tal> <alk> in common\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return [token[i: i + 3] for token in tokens for i in range(len(token) - 2)]\n",
    "    \n",
    "DataBaseDir = '../../data/version2'\n",
    "InputDir = '%s/l0/kfold' % DataBaseDir\n",
    "OutputDir = '%s/l1' % DataBaseDir\n",
    "kfold = 4\n",
    "strategy = 'lgb'\n",
    "## load data\n",
    "valid_dfs = []\n",
    "with timer('load data'):\n",
    "    for fold in range(kfold):\n",
    "        FoldInputDir = '%s/%s' % (InputDir, fold)\n",
    "        valid = pd.read_csv('%s/valid.csv' % FoldInputDir).reset_index(drop= True)#.sample(frac= 0.1)\n",
    "        ## for valid/holdout data set\n",
    "        if(fold == 0):\n",
    "            TestData = pd.read_csv('%s/test.csv' % FoldInputDir).reset_index(drop= True)#.sample(frac= 0.1)\n",
    "        valid['fold'] = fold\n",
    "        valid_dfs.append(valid)\n",
    "        print('load data for fold %s done.' % fold)\n",
    "    TrainData = pd.concat(valid_dfs, axis= 0, ignore_index= True)\n",
    "    print('load data done, train %s, test %s' % (len(TrainData), len(TestData)))\n",
    "\n",
    "## pre-preprocessing\n",
    "with timer(\"pre-processing\"):\n",
    "    get_indicators_and_clean_comments(TrainData)\n",
    "    get_indicators_and_clean_comments(TestData)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:27:23.114506Z",
     "start_time": "2018-03-10T12:18:12.535363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== fold 0 ======\n",
      "\n",
      "[Creating numerical features] done in 1 s\n",
      "[Tfidf on word] done in 66 s\n",
      "[Tfidf on char n_gram] done in 70 s\n",
      "1336\n",
      "[Staking matrices] done in 6 s\n",
      "[train lgb] done in 255 s\n",
      "fold 0, score 0.98147\n",
      "[evaluation] done in 1 s\n",
      "====== fold 1 ======\n",
      "\n",
      "[Creating numerical features] done in 0 s\n",
      "[Tfidf on word] done in 70 s\n",
      "[Tfidf on char n_gram] done in 73 s\n",
      "1336\n",
      "[Staking matrices] done in 7 s\n",
      "[train lgb] done in 261 s\n",
      "fold 1, score 0.98254\n",
      "[evaluation] done in 1 s\n",
      "====== fold 2 ======\n",
      "\n",
      "[Creating numerical features] done in 0 s\n",
      "[Tfidf on word] done in 61 s\n",
      "[Tfidf on char n_gram] done in 71 s\n",
      "1336\n",
      "[Staking matrices] done in 5 s\n",
      "[train lgb] done in 258 s\n",
      "fold 2, score 0.98084\n",
      "[evaluation] done in 1 s\n",
      "====== fold 3 ======\n",
      "\n",
      "[Creating numerical features] done in 0 s\n",
      "[Tfidf on word] done in 57 s\n",
      "[Tfidf on char n_gram] done in 68 s\n",
      "1296\n",
      "[Staking matrices] done in 4 s\n",
      "[train lgb] done in 236 s\n",
      "fold 3, score 0.98279\n",
      "[evaluation] done in 1 s\n",
      "\n",
      "================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a2c9ef990e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n================'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cv score %.5f,  time elapsed %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcv_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'================'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start' is not defined"
     ]
    }
   ],
   "source": [
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "# parameters\n",
    "params = {\n",
    "        \"objective\": \"binary\",\n",
    "        'metric': {'auc'},\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"verbosity\": -1,\n",
    "        \"num_threads\": 4,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"num_leaves\": 31,\n",
    "        \"verbose\": -1,\n",
    "        \"min_split_gain\": .1,\n",
    "        \"reg_alpha\": .1\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "cv_score = .0\n",
    "pred_cols = ['%s_%s' % (strategy, c) for c in targets]\n",
    "for c in pred_cols:\n",
    "    TestData[c] = .0\n",
    "for fold in range(kfold):\n",
    "    print('====== fold %s ======\\n' % fold)\n",
    "    FoldData = {\n",
    "        'train': TrainData[TrainData['fold'] != fold].copy(),\n",
    "        'valid': TrainData[TrainData['fold'] == fold].copy(),\n",
    "        'test': TestData.copy()\n",
    "    }\n",
    "    for c in pred_cols:\n",
    "        FoldData['valid'][c] = .0\n",
    "        FoldData['test'][c] = .0\n",
    "    # Scaling numerical features with MinMaxScaler though tree boosters don't need that\n",
    "    with timer(\"Creating numerical features\"):\n",
    "        num_features = [f_ for f_ in FoldData['train'].columns\n",
    "                        if f_ not in ['fold', \"comment_text\", \"clean_comment\", \"id\", \n",
    "                                      \"remaining_chars\", 'has_ip_address'] + targets]\n",
    "        skl = MinMaxScaler()\n",
    "        entire_num_features = pd.concat([FoldData['train'][num_features], FoldData['valid'][num_features], FoldData['test'][num_features]])\n",
    "        skl = skl.fit(entire_num_features)\n",
    "        train_num_features = csr_matrix(skl.transform(FoldData['train'][num_features]))\n",
    "        valid_num_features = csr_matrix(skl.transform(FoldData['valid'][num_features]))\n",
    "        test_num_features = csr_matrix(skl.transform(FoldData['test'][num_features]))\n",
    "\n",
    "    # Get TF-IDF features\n",
    "    EntireCorpus = pd.concat([FoldData['train']['clean_comment'], FoldData['valid']['clean_comment'], FoldData['test']['clean_comment']])\n",
    "\n",
    "    # First on real words\n",
    "    with timer(\"Tfidf on word\"):\n",
    "        word_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            token_pattern=r'\\w{1,}',\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=20000)\n",
    "        word_vectorizer.fit(EntireCorpus)\n",
    "        train_word_features = word_vectorizer.transform(FoldData['train']['clean_comment'])\n",
    "        valid_word_features = word_vectorizer.transform(FoldData['valid']['clean_comment'])\n",
    "        test_word_features = word_vectorizer.transform(FoldData['test']['clean_comment'])\n",
    "\n",
    "    del word_vectorizer\n",
    "    gc.collect()\n",
    "\n",
    "    # Now use the char_analyzer to get another TFIDF\n",
    "    # Char level TFIDF would go through words when char analyzer only considers\n",
    "    # characters inside a word\n",
    "    with timer(\"Tfidf on char n_gram\"):\n",
    "        char_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            tokenizer=char_analyzer,\n",
    "            analyzer='word',\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=50000)\n",
    "        char_vectorizer.fit(EntireCorpus)\n",
    "        train_char_features = char_vectorizer.transform(FoldData['train']['clean_comment'])\n",
    "        valid_char_features = char_vectorizer.transform(FoldData['valid']['clean_comment'])\n",
    "        test_char_features = char_vectorizer.transform(FoldData['test']['clean_comment'])\n",
    "\n",
    "    del char_vectorizer, EntireCorpus\n",
    "    gc.collect()\n",
    "\n",
    "    print((train_char_features > 0).sum(axis=1).max())\n",
    "\n",
    "    # Now stack TF IDF matrices\n",
    "    with timer(\"Staking matrices\"):\n",
    "        csr_trn = hstack(\n",
    "            [\n",
    "                train_char_features,\n",
    "                train_word_features,\n",
    "                train_num_features\n",
    "            ]\n",
    "        ).tocsr()\n",
    "        del train_word_features\n",
    "        del train_num_features\n",
    "        del train_char_features\n",
    "        gc.collect()\n",
    "\n",
    "        csr_valid = hstack(\n",
    "            [\n",
    "                valid_char_features,\n",
    "                valid_word_features,\n",
    "                valid_num_features\n",
    "            ]\n",
    "        ).tocsr()\n",
    "        del valid_word_features\n",
    "        del valid_num_features\n",
    "        del valid_char_features\n",
    "        gc.collect()\n",
    "        \n",
    "        csr_test = hstack(\n",
    "            [\n",
    "                test_char_features,\n",
    "                test_word_features,\n",
    "                test_num_features\n",
    "            ]\n",
    "        ).tocsr()\n",
    "        del test_word_features\n",
    "        del test_num_features\n",
    "        del test_char_features\n",
    "        gc.collect()\n",
    "    \n",
    "#     # Drop now useless columns in train and test\n",
    "#     drop_f = [f_ for f_ in FoldData['train'] if f_ not in [\"id\"] + targets]\n",
    "#     train.drop(drop_f, axis=1, inplace=True)\n",
    "#     gc.collect()\n",
    "    \n",
    "    with timer(\"train lgb\"):\n",
    "        for target in targets:\n",
    "            lgb_train = lightgbm.Dataset(csr_trn, \n",
    "                                    label= FoldData['train'][target].values, \n",
    "                                    silent= True, \n",
    "                                    free_raw_data= True)\n",
    "            lgb_valid = lightgbm.Dataset(csr_valid, \n",
    "                                    label= FoldData['valid'][target].values, \n",
    "                                    silent= True, \n",
    "                                    free_raw_data= True)\n",
    "            model = lightgbm.train(params= params, \n",
    "                                   train_set= lgb_train, \n",
    "                                   valid_sets= [lgb_train, lgb_valid],\n",
    "                                   early_stopping_rounds= 50,\n",
    "                                   num_boost_round= 100, \n",
    "                                   verbose_eval=0)\n",
    "            ## predict for valid\n",
    "            pred_col = '%s_%s' % (strategy, target)\n",
    "            pred_valid = model.predict(csr_valid)\n",
    "            FoldData['valid'][pred_col] = pred_valid\n",
    "            ## predict for test\n",
    "            pred_test = model.predict(csr_test)\n",
    "            FoldData['test'][pred_col] = pred_test\n",
    "            TestData[pred_col] += pred_test\n",
    "    ## evaluate\n",
    "    with timer('evaluation'):\n",
    "        score = roc_auc_score(FoldData['valid'][targets], FoldData['valid'][pred_cols])\n",
    "        cv_score += score\n",
    "        ## output\n",
    "        FoldOutputDir = '%s/kfold/%s' % (OutputDir, fold)\n",
    "        if(os.path.exists(FoldOutputDir) == False):\n",
    "            os.makedirs(FoldOutputDir)\n",
    "        for mod in ['valid', 'test']:\n",
    "            if(mod == 'test'):\n",
    "                out_cols = ['id']\n",
    "                out_cols.extend(pred_cols)\n",
    "            else:\n",
    "                out_cols = pred_cols.copy()\n",
    "                out_cols.extend(targets)\n",
    "            FoldData[mod][out_cols].to_csv('%s/%s_%s.csv' % (FoldOutputDir, mod, strategy),float_format='%.8f', index= False) \n",
    "        print('fold %s, score %.5f' % (fold, score))\n",
    "    \n",
    "cv_score /= kfold\n",
    "TestData[pred_cols] /= kfold\n",
    "end = time.time()\n",
    "print('\\n================')\n",
    "print('cv score %.5f,  time elapsed %s' % (cv_score, (end - start)))\n",
    "print('================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip ../../data/version2/l0/submit/lgb_submit_2018-03-10.zip ../../data/version2/l0/submit/lgb_submit_2018-03-10.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "## submit\n",
    "sub = TestData[['id']].copy()\n",
    "sub[targets] = TestData[pred_cols]\n",
    "OutputFileName = '%s_submit_%s' % (strategy, datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "SubmitDir = '%s/l0/submit' % DataBaseDir\n",
    "if(os.path.exists(SubmitDir) == False):\n",
    "    os.makedirs(SubmitDir)\n",
    "sub.to_csv('%s/%s.csv' % (SubmitDir, OutputFileName), float_format='%.8f', index=False)\n",
    "print('zip %s/%s.zip %s/%s.csv' % (SubmitDir, OutputFileName, SubmitDir, OutputFileName))\n",
    "os.system('zip %s/%s.zip %s/%s.csv' % (SubmitDir, OutputFileName, SubmitDir, OutputFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
