{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T11:48:38.543918Z",
     "start_time": "2018-03-10T11:46:01.842605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data for fold 0 done.\n",
      "load data for fold 1 done.\n",
      "load data for fold 2 done.\n",
      "load data for fold 3 done.\n",
      "load data for fold 4 done.\n",
      "load data for fold 5 done.\n",
      "load data for fold 6 done.\n",
      "load data for fold 7 done.\n",
      "load data for fold 8 done.\n",
      "load data for fold 9 done.\n",
      "load data done, train 159571, test 153164\n",
      "[load data] done in 1 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9257aa4c05aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;31m## pre-preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pre-processing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mget_indicators_and_clean_comments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0mget_indicators_and_clean_comments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTestData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9257aa4c05aa>\u001b[0m in \u001b[0;36mget_indicators_and_clean_comments\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_date_long\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount_regexp_occ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Check for date short 8 December 2010\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_date_short\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount_regexp_occ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\D\\d{1,2} \\w+ \\d{4}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;31m# Check for http links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_http\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount_regexp_occ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"http[s]{0,1}://\\S+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9257aa4c05aa>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_date_long\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount_regexp_occ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Check for date short 8 December 2010\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_date_short\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount_regexp_occ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\D\\d{1,2} \\w+ \\d{4}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;31m# Check for http links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_http\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount_regexp_occ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"http[s]{0,1}://\\S+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9257aa4c05aa>\u001b[0m in \u001b[0;36mcount_regexp_occ\u001b[0;34m(regexp, text)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_regexp_occ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;34m\"\"\" Simple way to get the number of occurence of a regex\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_indicators_and_clean_comments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import psutil\n",
    "import lightgbm\n",
    "from scipy.special import erfinv\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    \"\"\"\n",
    "    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n",
    "    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n",
    "    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "def prepare_for_char_n_gram(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "def get_indicators_and_clean_comments(df):\n",
    "    \"\"\"\n",
    "    Check all sorts of content as it may help find toxic comment\n",
    "    Though I'm not sure all of them improve scores\n",
    "    \"\"\"\n",
    "    # Get length in words and characters\n",
    "    df[\"raw_word_len\"] = df[\"comment_text\"].apply(lambda x: len(x.split())) + 1\n",
    "    df[\"raw_char_len\"] = df[\"comment_text\"].apply(lambda x: len(x)) + 1\n",
    "    # Count number of \\n\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    df['ant_slash_n_ratio'] = df[\"ant_slash_n\"]/df[\"raw_char_len\"]\n",
    "    # Check number of upper case, if you're angry you may write in upper case\n",
    "    df[\"nb_upper\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n",
    "    df['nb_upper_ratio'] = df[\"nb_upper\"]/df[\"raw_char_len\"]\n",
    "    # Number of F words - f..k contains folk, fork,\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    df['nb_fk_ratio'] = df[\"nb_fk\"]/df['raw_word_len']\n",
    "    # Number of S word\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    df['nb_sk_ratio'] = df[\"nb_sk\"]/df['raw_word_len']\n",
    "    # Number of D words\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    df['nb_dk_ratio'] = df['nb_dk']/df['raw_word_len']\n",
    "    # Number of occurence of You, insulting someone usually needs someone called : you\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    df['nb_you_ratio'] = df[\"nb_you\"]/df['raw_word_len']\n",
    "    # Just to check you really refered to my mother ;-)\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    df['nb_mother_ratio'] = df[\"nb_mother\"]/df['raw_word_len']\n",
    "    # Just checking for toxic 19th century vocabulary\n",
    "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n",
    "    df['nb_ng_ratio'] = df[\"nb_ng\"]/df['raw_word_len']\n",
    "    # Some Sentences start with a <:> so it may help\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    df['start_with_columns_ratio'] = df[\"start_with_columns\"]/(1 + df[\"ant_slash_n\"])\n",
    "    \n",
    "    ## new features\n",
    "    df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['num_punctuation'] = df['comment_text'].apply( lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "    df['imcomplete_punctuation'] = df['comment_text'].apply( lambda comment: sum(comment.count(w) for w in '*,#,$'))    \n",
    "    df['question_mask_ratio'] = df['num_question_marks']/df[\"raw_char_len\"]\n",
    "    df['exclamation_mark_ratio'] = df['num_exclamation_marks']/df[\"raw_char_len\"]\n",
    "    df['punctuation_ratio'] = df['num_punctuation']/df[\"raw_char_len\"]\n",
    "    df['imcomplete_punctuation_ratio'] = df['imcomplete_punctuation']/df[\"raw_char_len\"]\n",
    "    ##\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['raw_word_len']\n",
    "    df['num_smilies'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "    df['similes_ratio'] = df['num_smilies'] / df['raw_word_len']\n",
    "    \n",
    "    df[\"count_standard_punctuations\"] = df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    df[\"standard_punctuations_ratio\"] = df[\"count_standard_punctuations\"]/df['raw_char_len']\n",
    "    df[\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    df[\"words_title_ratio\"] = df[\"count_words_title\"]/df['raw_word_len']\n",
    "    \n",
    "    df['unique_words_greater_200'] = (df['num_unique_words'] > 200).astype(int)\n",
    "    \n",
    "    # Check for time stamp\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    # Check for dates 18:44, 8 December 2010\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for date short 8 December 2010\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for http links\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    # check for mail\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(\n",
    "        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n",
    "    )\n",
    "    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "\n",
    "    # Now clean comments\n",
    "    df[\"clean_comment\"] = df[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))\n",
    "\n",
    "    # Get the new length in words and characters\n",
    "    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n",
    "    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n",
    "    # Number of different characters used in a comment\n",
    "    # Using the f word only will reduce the number of letters required in the comment\n",
    "    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n",
    "    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) / df[\"clean_comment\"].apply(\n",
    "        lambda x: 1 + min(99, len(x)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def char_analyzer(text):\n",
    "    \"\"\"\n",
    "    This is used to split strings in small lots\n",
    "    I saw this in an article (I can't find the link anymore)\n",
    "    so <talk> and <talking> would have <Tal> <alk> in common\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return [token[i: i + 3] for token in tokens for i in range(len(token) - 2)]\n",
    "    \n",
    "DataBaseDir = '../../data/version4'\n",
    "InputDir = '%s/l0/kfold' % DataBaseDir\n",
    "OutputDir = '%s/l1' % DataBaseDir\n",
    "kfold = 10\n",
    "strategy = 'lgb'\n",
    "## load data\n",
    "valid_dfs = []\n",
    "with timer('load data'):\n",
    "    for fold in range(kfold):\n",
    "        FoldInputDir = '%s/%s' % (InputDir, fold)\n",
    "        valid = pd.read_csv('%s/valid.csv' % FoldInputDir).reset_index(drop= True)#.sample(frac= 0.1)\n",
    "        ## for valid/holdout data set\n",
    "        if(fold == 0):\n",
    "            TestData = pd.read_csv('%s/test.csv' % FoldInputDir).reset_index(drop= True)#.sample(frac= 0.1)\n",
    "        valid['fold'] = fold\n",
    "        valid_dfs.append(valid)\n",
    "        print('load data for fold %s done.' % fold)\n",
    "    TrainData = pd.concat(valid_dfs, axis= 0, ignore_index= True)\n",
    "    print('load data done, train %s, test %s' % (len(TrainData), len(TestData)))\n",
    "\n",
    "## pre-preprocessing\n",
    "with timer(\"pre-processing\"):\n",
    "    get_indicators_and_clean_comments(TrainData)\n",
    "    get_indicators_and_clean_comments(TestData)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T12:27:23.114506Z",
     "start_time": "2018-03-10T12:18:12.535363Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "# parameters\n",
    "params = {\n",
    "        \"objective\": \"binary\",\n",
    "        'metric': {'auc'},\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"verbosity\": -1,\n",
    "        \"num_threads\": 4,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"feature_fraction\": 0.6,\n",
    "        \"learning_rate\": 0.06,\n",
    "        \"num_leaves\": 63,\n",
    "        \"verbose\": -1,\n",
    "        \"min_split_gain\": .1,\n",
    "        \"reg_alpha\": .1\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "cv_score = .0\n",
    "pred_cols = ['%s_%s' % (strategy, c) for c in targets]\n",
    "for c in pred_cols:\n",
    "    TestData[c] = .0\n",
    "for fold in range(kfold):\n",
    "    print('====== fold %s ======\\n' % fold)\n",
    "    FoldData = {\n",
    "        'train': TrainData[TrainData['fold'] != fold].copy(),\n",
    "        'valid': TrainData[TrainData['fold'] == fold].copy(),\n",
    "        'test': TestData.copy()\n",
    "    }\n",
    "    for c in pred_cols:\n",
    "        FoldData['valid'][c] = .0\n",
    "        FoldData['test'][c] = .0\n",
    "    # Scaling numerical features with MinMaxScaler though tree boosters don't need that\n",
    "    with timer(\"Creating numerical features\"):\n",
    "#         FoldData['train'][num_features].fillna(0, inplace= True)\n",
    "#         FoldData['valid'][num_features].fillna(0, inplace= True)\n",
    "#         FoldData['test'][num_features].fillna(0, inplace= True)\n",
    "        num_features = [f_ for f_ in FoldData['train'].columns\n",
    "                        if f_ not in ['fold', \"comment_text\", \"clean_comment\", \"id\", \n",
    "                                      \"remaining_chars\", 'has_ip_address'] + targets]\n",
    "    \n",
    "        FoldData['train']['source'] = 'train'\n",
    "        FoldData['valid']['source'] = 'valid'\n",
    "        FoldData['test']['source'] = 'test'\n",
    "        all_data = pd.concat([FoldData['train'], FoldData['valid'], FoldData['test']], ignore_index= True)\n",
    "        for c in num_features:\n",
    "            rank = np.argsort(all_data[c], axis= 0)\n",
    "            upper = np.max(rank)\n",
    "            lower = np.min(rank)\n",
    "            # linear normalization to 0-1\n",
    "            all_data[c] = (all_data[c] - lower)/(upper - lower)\n",
    "            # gauss normalization\n",
    "            all_data[c] = erfinv(all_data[c])\n",
    "            all_data[c] -= np.mean(all_data[c])\n",
    "        train_num_features = csr_matrix(all_data[all_data['source'] == 'train'].drop(['source'], axis= 1)[num_features].values)\n",
    "        valid_num_features = csr_matrix(all_data[all_data['source'] == 'valid'].drop(['source'], axis= 1)[num_features].values)\n",
    "        test_num_features = csr_matrix(all_data[all_data['source'] == 'test'].drop(['source'], axis= 1)[num_features].values)\n",
    "    \n",
    "#         skl = MinMaxScaler()\n",
    "#         entire_num_features = pd.concat([FoldData['train'][num_features], FoldData['valid'][num_features], FoldData['test'][num_features]])\n",
    "#         skl = skl.fit(entire_num_features)\n",
    "#         train_num_features = csr_matrix(skl.transform(FoldData['train'][num_features]))\n",
    "#         valid_num_features = csr_matrix(skl.transform(FoldData['valid'][num_features]))\n",
    "#         test_num_features = csr_matrix(skl.transform(FoldData['test'][num_features]))\n",
    "\n",
    "    # Get TF-IDF features\n",
    "    EntireCorpus = pd.concat([FoldData['train']['clean_comment'], FoldData['valid']['clean_comment'], FoldData['test']['clean_comment']])\n",
    "\n",
    "    # First on real words\n",
    "    with timer(\"Tfidf on word\"):\n",
    "        word_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            token_pattern=r'\\w{1,}',\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=20000)\n",
    "        word_vectorizer.fit(EntireCorpus)\n",
    "        train_word_features = word_vectorizer.transform(FoldData['train']['clean_comment'])\n",
    "        valid_word_features = word_vectorizer.transform(FoldData['valid']['clean_comment'])\n",
    "        test_word_features = word_vectorizer.transform(FoldData['test']['clean_comment'])\n",
    "\n",
    "    del word_vectorizer\n",
    "    gc.collect()\n",
    "\n",
    "    # Now use the char_analyzer to get another TFIDF\n",
    "    # Char level TFIDF would go through words when char analyzer only considers\n",
    "    # characters inside a word\n",
    "    with timer(\"Tfidf on char n_gram\"):\n",
    "        char_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            tokenizer=char_analyzer,\n",
    "            analyzer='word',\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=50000)\n",
    "        char_vectorizer.fit(EntireCorpus)\n",
    "        train_char_features = char_vectorizer.transform(FoldData['train']['clean_comment'])\n",
    "        valid_char_features = char_vectorizer.transform(FoldData['valid']['clean_comment'])\n",
    "        test_char_features = char_vectorizer.transform(FoldData['test']['clean_comment'])\n",
    "\n",
    "    del char_vectorizer, EntireCorpus\n",
    "    gc.collect()\n",
    "\n",
    "    print((train_char_features > 0).sum(axis=1).max())\n",
    "\n",
    "    # Now stack TF IDF matrices\n",
    "    with timer(\"Staking matrices\"):\n",
    "        csr_trn = hstack(\n",
    "            [\n",
    "                train_char_features,\n",
    "                train_word_features,\n",
    "                train_num_features\n",
    "            ]\n",
    "        ).tocsr()\n",
    "        del train_word_features\n",
    "        del train_num_features\n",
    "        del train_char_features\n",
    "        gc.collect()\n",
    "\n",
    "        csr_valid = hstack(\n",
    "            [\n",
    "                valid_char_features,\n",
    "                valid_word_features,\n",
    "                valid_num_features\n",
    "            ]\n",
    "        ).tocsr()\n",
    "        del valid_word_features\n",
    "        del valid_num_features\n",
    "        del valid_char_features\n",
    "        gc.collect()\n",
    "        \n",
    "        csr_test = hstack(\n",
    "            [\n",
    "                test_char_features,\n",
    "                test_word_features,\n",
    "                test_num_features\n",
    "            ]\n",
    "        ).tocsr()\n",
    "        del test_word_features\n",
    "        del test_num_features\n",
    "        del test_char_features\n",
    "        gc.collect()\n",
    "    \n",
    "#     # Drop now useless columns in train and test\n",
    "#     drop_f = [f_ for f_ in FoldData['train'] if f_ not in [\"id\"] + targets]\n",
    "#     train.drop(drop_f, axis=1, inplace=True)\n",
    "#     gc.collect()\n",
    "    \n",
    "    with timer(\"train lgb\"):\n",
    "        for target in targets:\n",
    "            lgb_train = lightgbm.Dataset(csr_trn, \n",
    "                                    label= FoldData['train'][target].values, \n",
    "                                    silent= True, \n",
    "                                    free_raw_data= True)\n",
    "            lgb_valid = lightgbm.Dataset(csr_valid, \n",
    "                                    label= FoldData['valid'][target].values, \n",
    "                                    silent= True, \n",
    "                                    free_raw_data= True)\n",
    "            model = lightgbm.train(params= params, \n",
    "                                   train_set= lgb_train, \n",
    "                                   valid_sets= [lgb_train, lgb_valid],\n",
    "                                   early_stopping_rounds= 100,\n",
    "                                   num_boost_round= 200, \n",
    "                                   verbose_eval=0)\n",
    "            ## predict for valid\n",
    "            pred_col = '%s_%s' % (strategy, target)\n",
    "            pred_valid = model.predict(csr_valid)\n",
    "            FoldData['valid'][pred_col] = pred_valid\n",
    "            ## predict for test\n",
    "            pred_test = model.predict(csr_test)\n",
    "            FoldData['test'][pred_col] = pred_test\n",
    "            TestData[pred_col] += pred_test\n",
    "    ## evaluate\n",
    "    with timer('evaluation'):\n",
    "        score = roc_auc_score(FoldData['valid'][targets], FoldData['valid'][pred_cols])\n",
    "        cv_score += score\n",
    "        ## output\n",
    "        FoldOutputDir = '%s/kfold/%s' % (OutputDir, fold)\n",
    "        if(os.path.exists(FoldOutputDir) == False):\n",
    "            os.makedirs(FoldOutputDir)\n",
    "        for mod in ['valid', 'test']:\n",
    "            if(mod == 'test'):\n",
    "                out_cols = ['id']\n",
    "                out_cols.extend(pred_cols)\n",
    "            else:\n",
    "                out_cols = pred_cols.copy()\n",
    "                out_cols.extend(targets)\n",
    "            FoldData[mod][out_cols].to_csv('%s/%s_%s.csv' % (FoldOutputDir, mod, strategy),float_format='%.8f', index= False) \n",
    "        print('fold %s, score %.5f' % (fold, score))\n",
    "    \n",
    "cv_score /= kfold\n",
    "TestData[pred_cols] /= kfold\n",
    "end = time.time()\n",
    "print('\\n================')\n",
    "print('cv score %.5f,  time elapsed %s' % (cv_score, (end - start)))\n",
    "print('================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "## submit\n",
    "sub = TestData[['id']].copy()\n",
    "sub[targets] = TestData[pred_cols]\n",
    "OutputFileName = '%s_submit_%s' % (strategy, datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "SubmitDir = '%s/l0/submit' % DataBaseDir\n",
    "if(os.path.exists(SubmitDir) == False):\n",
    "    os.makedirs(SubmitDir)\n",
    "sub.to_csv('%s/%s.csv' % (SubmitDir, OutputFileName), float_format='%.8f', index=False)\n",
    "print('zip %s/%s.zip %s/%s.csv' % (SubmitDir, OutputFileName, SubmitDir, OutputFileName))\n",
    "os.system('zip %s/%s.zip %s/%s.csv' % (SubmitDir, OutputFileName, SubmitDir, OutputFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
