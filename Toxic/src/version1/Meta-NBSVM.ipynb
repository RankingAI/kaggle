{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T05:35:03.938511Z",
     "start_time": "2018-03-09T23:51:03.050747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data for fold 0 done.\n",
      "load data for fold 1 done.\n",
      "load data for fold 2 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 375/159571 [00:00<00:42, 3743.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data for fold 3 done.\n",
      "load data done, train 159571, time elapsed 1.6437060832977295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:16<00:00, 9834.77it/s]\n",
      "100%|██████████| 153164/153164 [00:16<00:00, 9253.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== fold 0 ======\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0, score 0.98679, time elapsed 17780.82\n",
      "====== fold 1 ======\n",
      "\n",
      "fold 1, score 0.98650, time elapsed 18658.47\n",
      "====== fold 2 ======\n",
      "\n",
      "fold 2, score 0.98731, time elapsed 19617.18\n",
      "====== fold 3 ======\n",
      "\n",
      "fold 3, score 0.98759, time elapsed 20602.23\n",
      "\n",
      "================\n",
      "cv score 0.98705,  time elapsed 20602.246532201767\n",
      "================\n",
      "zip ../../data/version2/l0/submit/nbsvm_submit_2018-03-10.zip ../../data/version2/l0/submit/nbsvm_submit_2018-03-10.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import scipy.sparse\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm as tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os,sys,time, datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "DataBaseDir = '../../data/version2'\n",
    "InputDir = '%s/l0/kfold' % DataBaseDir\n",
    "OutputDir = '%s/l1' % DataBaseDir\n",
    "kfold = 4\n",
    "strategy = 'nbsvm'\n",
    "# load data\n",
    "start = time.time()\n",
    "valid_dfs = []\n",
    "for fold in range(kfold):\n",
    "    FoldInputDir = '%s/%s' % (InputDir, fold)\n",
    "    valid = pd.read_csv('%s/valid.csv' % FoldInputDir).reset_index(drop= True)#.sample(frac= 0.1)\n",
    "    ## for valid/holdout data set\n",
    "    if(fold == 0):\n",
    "        TestData = pd.read_csv('%s/test.csv' % FoldInputDir).reset_index(drop= True)#.sample(frac= 0.1)\n",
    "    valid['fold'] = fold\n",
    "    valid_dfs.append(valid)\n",
    "    print('load data for fold %s done.' % fold)\n",
    "TrainData = pd.concat(valid_dfs, axis= 0, ignore_index= True)\n",
    "end = time.time()\n",
    "print('load data done, train %s, time elapsed %s' % (len(TrainData), (end - start)))\n",
    "\n",
    "# pre-process\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def stem_word(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def lemmatize_word(text):\n",
    "    return lemmatizer.lemmatize(text)\n",
    "\n",
    "def reduce_text(conversion, text):\n",
    "    return \" \".join(map(conversion, wordpunct_tokenize(text.lower())))\n",
    "\n",
    "def reduce_texts(conversion, texts):\n",
    "    return [reduce_text(conversion, str(text))\n",
    "            for text in tqdm(texts)]\n",
    "\n",
    "TrainData['comment_text_stemmed'] = reduce_texts(stem_word, TrainData['comment_text'])\n",
    "TestData['comment_text_stemmed'] = reduce_texts(stem_word, TestData['comment_text'])\n",
    "\n",
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "##\n",
    "def get_model(X, y):\n",
    "    tfidf_word = TfidfVectorizer(\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        sublinear_tf= 1,\n",
    "        ngram_range=(1,1),\n",
    "        max_features=20000\n",
    "    )\n",
    "    X_tfidf_word = tfidf_word.fit_transform(X[:, 1])\n",
    "    tfidf_char = TfidfVectorizer(\n",
    "        strip_accents='unicode',\n",
    "        analyzer='char', \n",
    "        sublinear_tf= 1,\n",
    "        ngram_range=(1, 4),\n",
    "        max_features=20000,\n",
    "        lowercase=False)\n",
    "    X_tfidf_char = tfidf_char.fit_transform(X[:, 0])\n",
    "    X_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])\n",
    "    \n",
    "    def fit(x, y):\n",
    "        x = x.tocsr()\n",
    "        p_1 = x[y == 1].sum(0)\n",
    "        pr_1 = (p_1 + 1) / ((y == 1).sum() + 1)\n",
    "        p_0 = x[y == 0].sum(0)\n",
    "        pr_0 = (p_0 + 1) / ((y == 0).sum() + 1)\n",
    "        r = np.log(pr_1 / pr_0)\n",
    "        m = LogisticRegression(C= 0.4)\n",
    "        x_nb = x.multiply(r)\n",
    "        return m.fit(x_nb, y), r\n",
    "    \n",
    "    columns = y.shape[1]\n",
    "    regressions = [fit(X_tfidf, y[:, i]) for i in range(columns)]\n",
    "    \n",
    "    def _predict(X):\n",
    "        X_tfidf_word = tfidf_word.transform(X[:, 1])\n",
    "        X_tfidf_char = tfidf_char.transform(X[:, 0])\n",
    "        X_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])\n",
    "        predictions = np.zeros([len(X), columns])\n",
    "        for i, (regression, r) in enumerate(regressions):\n",
    "            predictions[:, i] = regression.predict_proba(X_tfidf.multiply(r))[:, regression.classes_ == 1][:, 0]\n",
    "        return predictions\n",
    "    \n",
    "    return _predict\n",
    "\n",
    "##\n",
    "cv_score = .0\n",
    "start = time.time()\n",
    "pred_cols = ['%s_%s' % (strategy, c) for c in targets]\n",
    "for c in pred_cols:\n",
    "    TestData[c] = .0\n",
    "for fold in range(kfold):\n",
    "    print('====== fold %s ======\\n' % fold)\n",
    "    FoldData = {\n",
    "        'train': TrainData[TrainData['fold'] != fold],\n",
    "        'valid': TrainData[TrainData['fold'] == fold],\n",
    "        'test': TestData\n",
    "    }\n",
    "    for c in pred_cols:\n",
    "        FoldData['valid'][c] = .0\n",
    "        FoldData['test'][c] = .0\n",
    "    ## construct bi-gru model\n",
    "    model = get_model(FoldData['train'][['comment_text', 'comment_text_stemmed']].values, \n",
    "                      FoldData['train'][targets].values)\n",
    "    ## predict for valid\n",
    "    pred_valid = model(FoldData['valid'][['comment_text', 'comment_text_stemmed']].values)\n",
    "    FoldData['valid'][pred_cols] = pred_valid\n",
    "    ## predict for test\n",
    "    pred_test = model(FoldData['test'][['comment_text', 'comment_text_stemmed']].values)\n",
    "    FoldData['test'][pred_cols] = pred_test\n",
    "    TestData[pred_cols] += pred_test\n",
    "    ## evaluate\n",
    "    score = roc_auc_score(FoldData['valid'][targets], FoldData['valid'][pred_cols])\n",
    "    cv_score += score\n",
    "    ## output\n",
    "    FoldOutputDir = '%s/kfold/%s' % (OutputDir, fold)\n",
    "    if(os.path.exists(FoldOutputDir) == False):\n",
    "        os.makedirs(FoldOutputDir)\n",
    "    for mod in ['valid', 'test']:\n",
    "        if(mod == 'test'):\n",
    "            out_cols = ['id']\n",
    "            out_cols.extend(pred_cols)\n",
    "        else:\n",
    "            out_cols = pred_cols.copy()\n",
    "            out_cols.extend(targets)\n",
    "        FoldData[mod][out_cols].to_csv('%s/%s_%s.csv' % (FoldOutputDir, mod, strategy),float_format='%.8f', index= False) \n",
    "    end = time.time()\n",
    "    print('fold %s, score %.5f, time elapsed %.2f' % (fold, score, (end - start)))\n",
    "\n",
    "cv_score /= kfold\n",
    "TestData[pred_cols] /= kfold\n",
    "end = time.time()\n",
    "print('\\n================')\n",
    "print('cv score %.5f,  time elapsed %s' % (cv_score, (end - start)))\n",
    "print('================')\n",
    "\n",
    "## submit\n",
    "sub = TestData[['id']].copy()\n",
    "sub[targets] = TestData[pred_cols]\n",
    "OutputFileName = '%s_submit_%s' % (strategy, datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "SubmitDir = '%s/l0/submit' % DataBaseDir\n",
    "if(os.path.exists(SubmitDir) == False):\n",
    "    os.makedirs(SubmitDir) \n",
    "sub.to_csv('%s/%s.csv' % (SubmitDir, OutputFileName), float_format='%.8f', index=False)\n",
    "print('zip %s/%s.zip %s/%s.csv' % (SubmitDir, OutputFileName, SubmitDir, OutputFileName))\n",
    "os.system('zip %s/%s.zip %s/%s.csv' % (SubmitDir, OutputFileName, SubmitDir, OutputFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
