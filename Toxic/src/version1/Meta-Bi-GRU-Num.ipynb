{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-11T09:57:15.429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data for fold 0 done.\n",
      "load data for fold 1 done.\n",
      "load data for fold 2 done.\n",
      "load data for fold 3 done.\n",
      "load data done, train 15958, time elapsed 1.6504640579223633\n",
      "load embedding features done, corpus size 1000, time elapsed 0.08364319801330566\n",
      "====== fold 0 ======\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-85ed7cc13208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m                      \u001b[0;31m#validation_data= (X_valid_input, Y_valid),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                      \u001b[0;31m#callbacks=[RocAuc],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                      verbose=2)\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fitting done, time elapsed %s.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import os,sys,time,datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "DataBaseDir = '../../data/version2'\n",
    "InputDir = '%s/l0/kfold' % DataBaseDir\n",
    "OutputDir = '%s/l1' % DataBaseDir\n",
    "kfold = 4\n",
    "strategy = 'bi-gru-num'\n",
    "# load data\n",
    "start = time.time()\n",
    "valid_dfs = []\n",
    "for fold in range(kfold):\n",
    "    FoldInputDir = '%s/%s' % (InputDir, fold)\n",
    "    valid = pd.read_csv('%s/valid.csv' % FoldInputDir).reset_index(drop= True).sample(frac= 0.1)\n",
    "    ## for valid/holdout data set\n",
    "    if(fold == 0):\n",
    "        TestData = pd.read_csv('%s/test.csv' % FoldInputDir).reset_index(drop= True).sample(frac= 0.1)\n",
    "    valid['fold'] = fold\n",
    "    valid_dfs.append(valid)\n",
    "    print('load data for fold %s done.' % fold)\n",
    "TrainData = pd.concat(valid_dfs, axis= 0, ignore_index= True)\n",
    "end = time.time()\n",
    "print('load data done, train %s, time elapsed %s' % (len(TrainData), (end - start)))\n",
    "##\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def LoadEmbeddingVectors(f):\n",
    "    ## debug\n",
    "    k = 1000\n",
    "    EmbeddingDict = {}\n",
    "    with open(f, 'r') as i_file:\n",
    "        for line in i_file:\n",
    "            if(k == 0):\n",
    "                break\n",
    "            w, coe_vec= get_coefs(*line.rstrip().rsplit(' '))\n",
    "            EmbeddingDict[w] = coe_vec\n",
    "            k -= 1\n",
    "    i_file.close()\n",
    "    return EmbeddingDict\n",
    "\n",
    "# class RocAucEvaluation(Callback):\n",
    "#     def __init__(self, validation_data=(), interval=1):\n",
    "#         super(Callback, self).__init__()\n",
    "\n",
    "#         self.interval = interval\n",
    "#         self.X_val, self.y_val = validation_data\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         if epoch % self.interval == 0:\n",
    "#             y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "#             score = roc_auc_score(self.y_val, y_pred)\n",
    "#             print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "EmbeddingFile = '../../data/raw/crawl-300d-2M.vec'\n",
    "max_features = 30000\n",
    "maxlen = 150\n",
    "#max_features = 3000\n",
    "#maxlen = 10\n",
    "embed_size = 300\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "start = time.time()\n",
    "EmbeddingIndex = LoadEmbeddingVectors(EmbeddingFile)\n",
    "end = time.time()\n",
    "print('load embedding features done, corpus size %s, time elapsed %s' % (len(EmbeddingIndex), (end - start)))\n",
    "\n",
    "pred_cols = ['%s_%s' % (strategy, c) for c in targets]\n",
    "for c in pred_cols:\n",
    "    TestData[c] = .0\n",
    "\n",
    "def get_num_feats(text_df):\n",
    "    text_df['num_words'] = text_df['comment_text'].str.count('\\S+')\n",
    "    text_df['num_comas'] = text_df['comment_text'].str.count('\\.')\n",
    "    text_df['num_bangs'] = text_df['comment_text'].str.count('\\!')\n",
    "    text_df['num_quotas'] = text_df['comment_text'].str.count('\\\"')\n",
    "    text_df['avg_word'] = text_df['comment_text'].str.len() / (1 + text_df['num_words'])\n",
    "    \n",
    "    return text_df\n",
    "\n",
    "## numeric features\n",
    "num_feats = ['num_words','num_comas','num_bangs','num_quotas','avg_word']\n",
    "TrainData = get_num_feats(TrainData)\n",
    "TestData = get_num_feats(TestData)\n",
    "entire_num_df = pd.concat([TrainData[num_feats], TestData[num_feats]], axis= 0, ignore_index= True)\n",
    "scaler = MinMaxScaler().fit(entire_num_df)\n",
    "TrainData[num_feats] = scaler.transform(TrainData[num_feats].values)\n",
    "TestData[num_feats] = scaler.transform(TestData[num_feats].values)\n",
    "\n",
    "## tokenized features\n",
    "tokenizer = text.Tokenizer(num_words= max_features)\n",
    "EntireCorpus = list(TrainData['comment_text'].values) + list(TestData['comment_text'].values)\n",
    "tokenizer.fit_on_texts(EntireCorpus)\n",
    "\n",
    "## embedding with pre-trained embedding library\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = EmbeddingIndex.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "def get_model(X_input):\n",
    "    # input for token words\n",
    "    inp_token = Input(shape=(maxlen, ), name= 'token_words')\n",
    "    # input for num feats\n",
    "    inp_num = Input(shape=[X_input['num_feats'].shape[1]], name= \"num_feats\")\n",
    "    # embedding for token words\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp_token)\n",
    "    x = SpatialDropout1D(0.25)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    # concate results of bi-gru with num features\n",
    "    conc = concatenate([avg_pool, max_pool, inp_num])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    # \n",
    "    model = Model(inputs= [inp_token, inp_num], outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "##\n",
    "cv_score = .0\n",
    "start = time.time()\n",
    "for fold in range(kfold):\n",
    "    print('====== fold %s ======\\n' % fold)\n",
    "    FoldData = {\n",
    "        'train': TrainData[TrainData['fold'] != fold],\n",
    "        'valid': TrainData[TrainData['fold'] == fold],\n",
    "        'test': TestData\n",
    "    }\n",
    "    for c in pred_cols:\n",
    "        FoldData['valid'][c] = .0\n",
    "        FoldData['test'][c] = .0\n",
    "    ## input for X\n",
    "    X_train_input = {}\n",
    "    X_valid_input = {}\n",
    "    X_test_input = {}\n",
    "    ## tokenize with entire corpus composed by train/valid/test\n",
    "    X_train_input['token_words'] = tokenizer.texts_to_sequences(FoldData['train']['comment_text'].values)\n",
    "    X_valid_input['token_words'] = tokenizer.texts_to_sequences(FoldData['valid']['comment_text'].values)\n",
    "    X_test_input['token_words'] = tokenizer.texts_to_sequences(FoldData['test']['comment_text'].values)\n",
    "    \n",
    "    X_train_input['token_words'] = sequence.pad_sequences(X_train_input['token_words'], maxlen= maxlen)\n",
    "    X_valid_input['token_words'] = sequence.pad_sequences(X_valid_input['token_words'], maxlen= maxlen)\n",
    "    X_test_input['token_words'] = sequence.pad_sequences(X_test_input['token_words'], maxlen= maxlen)\n",
    "\n",
    "    ## num data\n",
    "    X_train_input['num_feats'] = FoldData['train'][num_feats].values\n",
    "    X_valid_input['num_feats'] = FoldData['valid'][num_feats].values\n",
    "    X_test_input['num_feats'] = FoldData['test'][num_feats].values\n",
    "    \n",
    "    ## input for Y\n",
    "    Y_train = FoldData['train'][targets].values\n",
    "    Y_valid = FoldData['valid'][targets].values\n",
    "    \n",
    "    ## construct bi-gru model\n",
    "    model = get_model(X_train_input)\n",
    "    RocAuc = RocAucEvaluation(validation_data= (X_valid_input, Y_valid), interval=1)\n",
    "    hist = model.fit(X_train_input, Y_train, \n",
    "                     batch_size= batch_size, \n",
    "                     epochs= epochs, \n",
    "                     validation_data= (X_valid_input, Y_valid),\n",
    "                     callbacks=[RocAuc],\n",
    "                     verbose=2)\n",
    "    end = time.time()\n",
    "    print('fitting done, time elapsed %s.' % (end - start))\n",
    "    ## predict for valid\n",
    "    pred_valid = model.predict(X_valid_input, batch_size=1024)\n",
    "    FoldData['valid'][pred_cols] = pred_valid\n",
    "    ## predict for test\n",
    "    pred_test = model.predict(X_test_input, batch_size=1024)\n",
    "    FoldData['test'][pred_cols] = pred_test\n",
    "    TestData[pred_cols] += pred_test\n",
    "    ## evaluate\n",
    "    score = roc_auc_score(FoldData['valid'][targets], FoldData['valid'][pred_cols])\n",
    "    cv_score += score\n",
    "    ## output\n",
    "    FoldOutputDir = '%s/kfold/%s' % (OutputDir, fold)\n",
    "    if(os.path.exists(FoldOutputDir) == False):\n",
    "        os.makedirs(FoldOutputDir)\n",
    "    for mod in ['valid', 'test']:\n",
    "        if(mod == 'test'):\n",
    "            out_cols = ['id']\n",
    "            out_cols.extend(pred_cols)\n",
    "        else:\n",
    "            out_cols = pred_cols.copy()\n",
    "            out_cols.extend(targets)\n",
    "        FoldData[mod][out_cols].to_csv('%s/%s_%s.csv' % (FoldOutputDir, mod, strategy),float_format='%.8f', index= False) \n",
    "    end = time.time()\n",
    "    print('fold %s, score %.5f, time elapsed %.2fs' % (fold, score, (end - start)))\n",
    "cv_score /= kfold\n",
    "TestData[pred_cols] /= kfold\n",
    "end = time.time()\n",
    "print('\\n================')\n",
    "print('cv score %.5f,  time elapsed %s' % (cv_score, (end - start)))\n",
    "print('================')\n",
    "\n",
    "## submit\n",
    "sub = TestData[['id']].copy()\n",
    "sub[targets] = TestData[pred_cols]\n",
    "OutputFileName = '%s_submit_%s' % (strategy, datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "SubmitDir = '%s/l0/submit' % DataBaseDir\n",
    "if(os.path.exists(SubmitDir) == False):\n",
    "    os.makedirs(SubmitDir)\n",
    "sub.to_csv('%s/%s.csv' % (SubmitDir, OutputFileName), float_format='%.8f', index=False)\n",
    "print('zip %s/%s.zip %s/%s.csv' % (SubmitDir, OutputFileName, SubmitDir, OutputFileName))\n",
    "os.system('zip %s/%s.zip %s/%s.csv' % (SubmitDir, OutputFileName, SubmitDir, OutputFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
