{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:23:55.283453Z",
     "start_time": "2018-01-11T09:23:55.194740Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm as tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy import sparse\n",
    "import time, os, sys\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, SpatialDropout1D, Dropout\n",
    "from keras.layers import Embedding, GlobalMaxPool1D, BatchNormalization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "class DataUtil2:\n",
    "    \"\"\"\"\"\"\n",
    "    @classmethod\n",
    "    def load(cls, file, format, date_cols= None):\n",
    "        \"\"\"\"\"\"\n",
    "        data = ''\n",
    "        if(format== 'csv'):\n",
    "            data = pd.read_csv(file, parse_dates= date_cols)\n",
    "        elif(format== 'json'):\n",
    "            with open(file, 'r') as i_file:\n",
    "                data = json.load(file)\n",
    "            i_file.close()\n",
    "        elif(format== 'pkl'):\n",
    "            with open(file, 'rb') as i_file:\n",
    "                data = pickle.load(i_file)\n",
    "            i_file.close()\n",
    "        elif(format == 'hdf'):\n",
    "            data = pd.read_hdf(path_or_buf= file, key='undefined')\n",
    "        elif(format == 'npz'):\n",
    "            data = scipy.sparse.load_npz(file)\n",
    "\n",
    "        return  data\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls, data, file, format, precision= 8):\n",
    "        \"\"\"\"\"\"\n",
    "        if(format == 'csv'):\n",
    "            data.to_csv(file, float_format= '%%.%df' % precision, index= False)\n",
    "        elif(format == 'json'):\n",
    "            with open(file, 'w') as o_file:\n",
    "                json.dump(data, o_file, ensure_ascii= True, indent= 4)\n",
    "            o_file.close()\n",
    "        elif(format == 'pkl'):\n",
    "            with open(file, 'wb') as o_file:\n",
    "                pickle.dump(data, o_file, -1)\n",
    "            o_file.close()\n",
    "        elif(format== 'hdf'):\n",
    "            data.to_hdf(path_or_buf= file, key='undefined', mode='w', complib='blosc')\n",
    "        elif(format == 'npz'):\n",
    "            scipy.sparse.save_npz(file, data)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:23:57.001068Z",
     "start_time": "2018-01-11T09:23:55.304893Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/95851 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data done, time elapsed 1.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95851/95851 [00:04<00:00, 20083.17it/s]\n",
      "100%|██████████| 226998/226998 [00:09<00:00, 23068.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming done, time elapsed 14.69s\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "iformat = 'csv'\n",
    "oformat = 'hdf'\n",
    "DataBase = '../data'\n",
    "DataSet = {}\n",
    "start = time.time()\n",
    "for mod in ['train', 'test']:\n",
    "    DataSet[mod] = DataUtil2.load('%s/raw/%s.%s' % (DataBase, mod, iformat), iformat)\n",
    "    DataSet[mod]['comment_text'] = DataSet[mod]['comment_text'].fillna('nan')\n",
    "end = time.time()\n",
    "print('load data done, time elapsed %.2fs' % (end - start))\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def stem_word(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def lemmatize_word(text):\n",
    "    return lemmatizer.lemmatize(text)\n",
    "\n",
    "def reduce_text(conversion, text):\n",
    "    return \" \".join(map(conversion, wordpunct_tokenize(text.lower())))\n",
    "\n",
    "def reduce_texts(conversion, texts):\n",
    "    return [reduce_text(conversion, str(text))\n",
    "            for text in tqdm(texts)]\n",
    "\n",
    "start = time.time()\n",
    "for mod in ['train', 'test']:\n",
    "    #DataSet[mod]['comment_text_stemmed'] = reduce_texts(stem_word, DataSet[mod]['comment_text'])\n",
    "    #DataSet[mod]['comment_text_stemmed'] = DataSet[mod]['comment_text_stemmed'].fillna('nan')\n",
    "    DataSet[mod]['comment_text_lemma'] = reduce_texts(lemmatize_word, DataSet[mod]['comment_text'])\n",
    "    DataSet[mod]['comment_text_lemma'] = DataSet[mod]['comment_text_lemma'].fillna('nan') \n",
    "end = time.time()\n",
    "print('stemming done, time elapsed %.2fs' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n",
      "BOW done, time elapsed 23.05s\n",
      "Pad sequences (samples x time)\n",
      "train shape:  (95851, 100)\n",
      "test shape:  (226998, 100)\n"
     ]
    }
   ],
   "source": [
    "## standarize BOW\n",
    "max_features= 50000\n",
    "maxlen= 100\n",
    "targets = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "text_col = 'comment_text_lemma'\n",
    "\n",
    "x_train = DataSet['train'][text_col]\n",
    "y_train = DataSet['train'][targets].values\n",
    "x_test = DataSet['test'][text_col]\n",
    "for target in targets:\n",
    "    DataSet['test'][target] = .0\n",
    "y_test = DataSet['test'][targets].values\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print('Tokenizing data...')\n",
    "tok = Tokenizer(num_words=max_features)\n",
    "tok.fit_on_texts(list(x_train) + list(x_test))\n",
    "x_train = tok.texts_to_sequences(x_train)\n",
    "x_test = tok.texts_to_sequences(x_test)\n",
    "end = time.time()\n",
    "print('BOW done, time elapsed %.2fs' % (end - start))\n",
    "\n",
    "# padding into a smaller length\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('train shape: ' , x_train.shape)\n",
    "print('test shape: ' , x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:25:19.375404Z",
     "start_time": "2018-01-11T09:23:55.199Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label2binary = np.array([\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 1, 1, 0],\n",
    "    [0, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1],\n",
    "    [0, 0, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 1, 1],\n",
    "    [0, 0, 1, 1, 0, 0],\n",
    "    [0, 0, 1, 1, 0, 1],\n",
    "    [0, 0, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 1, 1, 1],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 1, 1],\n",
    "    [0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 1],\n",
    "    [0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 1, 1],\n",
    "    [0, 1, 1, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 1],\n",
    "    [0, 1, 1, 0, 1, 0],\n",
    "    [0, 1, 1, 0, 1, 1],\n",
    "    [0, 1, 1, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 0, 1],\n",
    "    [0, 1, 1, 1, 1, 0],\n",
    "    [0, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 1, 1],\n",
    "    [1, 0, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 1, 1, 0],\n",
    "    [1, 0, 0, 1, 1, 1],\n",
    "    [1, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 1],\n",
    "    [1, 0, 1, 0, 1, 0],\n",
    "    [1, 0, 1, 0, 1, 1],\n",
    "    [1, 0, 1, 1, 0, 0],\n",
    "    [1, 0, 1, 1, 0, 1],\n",
    "    [1, 0, 1, 1, 1, 0],\n",
    "    [1, 0, 1, 1, 1, 1],\n",
    "    [1, 1, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0, 1],\n",
    "    [1, 1, 0, 0, 1, 0],\n",
    "    [1, 1, 0, 0, 1, 1],\n",
    "    [1, 1, 0, 1, 0, 0],\n",
    "    [1, 1, 0, 1, 0, 1],\n",
    "    [1, 1, 0, 1, 1, 0],\n",
    "    [1, 1, 0, 1, 1, 1],\n",
    "    [1, 1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 0],\n",
    "    [1, 1, 1, 0, 1, 1],\n",
    "    [1, 1, 1, 1, 0, 0],\n",
    "    [1, 1, 1, 1, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 0],\n",
    "    [1, 1, 1, 1, 1, 1],\n",
    "])\n",
    "\n",
    "OutputDir = '../data/meta/kfold/'\n",
    "strategy = 'ft_word_char_tfidf'\n",
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    columns = y_true.shape[1]\n",
    "    column_losses = []\n",
    "    for i in range(0, columns):\n",
    "        column_losses.append(log_loss(y_true[:, i], y_pred[:, i]))\n",
    "    return np.array(column_losses).mean()\n",
    "\n",
    "def cv(model, X, y, label2binary, n_splits=3):\n",
    "    def split(X, y):\n",
    "        return StratifiedKFold(n_splits=n_splits).split(X, y)\n",
    "    \n",
    "    def convert_y(y):\n",
    "        new_y = np.zeros([len(y)])\n",
    "        for i, val in enumerate(label2binary):\n",
    "            idx = (y == val).max(axis=1)\n",
    "            new_y[idx] = i\n",
    "        return new_y\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    scores = []\n",
    "    fold = 0\n",
    "    for train, test in tqdm(split(X, convert_y(y)), total=n_splits):\n",
    "        FoldOutput = '%s/%s' % (OutputDir, fold)\n",
    "        if(os.path.exists(FoldOutput) == False):\n",
    "            os.makedirs(FoldOutput)\n",
    "        fitted_model = model(X[train], y[train])\n",
    "        predict = fitted_model(X[test])\n",
    "        score = metric(y[test], predict)\n",
    "        scores.append(score)\n",
    "        FoldOutputFile = '%s/valid_%s.csv' % (FoldOutput, strategy)\n",
    "        df = pd.DataFrame(index= range(len(predict)))\n",
    "        #df.columns = ['%s_%s' % (strategy,t) for t in targets]\n",
    "        for idx in range(len(targets)):\n",
    "            df['%s_%s' % (strategy, targets[idx])] = predict[:,idx]\n",
    "        DataUtil2.save(df, FoldOutputFile, 'csv', 6)\n",
    "        print('fold %s, cv score %.4f' % (fold, score))\n",
    "        fold += 1\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-11T14:27:56.928Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Epoch 1/4\n",
      "63900/63900 [==============================] - 16s - loss: 0.1031 - acc: 0.9633    \n",
      "Epoch 2/4\n",
      "63900/63900 [==============================] - 14s - loss: 0.0603 - acc: 0.9793    \n",
      "Epoch 3/4\n",
      "63900/63900 [==============================] - 14s - loss: 0.0529 - acc: 0.9813    \n",
      "Epoch 4/4\n",
      "63900/63900 [==============================] - 14s - loss: 0.0482 - acc: 0.9826    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 1/3 [01:00<02:01, 60.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0, cv score 0.0509\n",
      "Build model...\n",
      "Epoch 1/4\n",
      "63901/63901 [==============================] - 14s - loss: 0.1040 - acc: 0.9642    \n",
      "Epoch 2/4\n",
      "63901/63901 [==============================] - 14s - loss: 0.0605 - acc: 0.9793    \n",
      "Epoch 3/4\n",
      "63901/63901 [==============================] - 14s - loss: 0.0536 - acc: 0.9811    \n",
      "Epoch 4/4\n",
      "63901/63901 [==============================] - 14s - loss: 0.0485 - acc: 0.9826    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [01:59<00:59, 59.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1, cv score 0.0512\n",
      "Build model...\n",
      "Epoch 1/4\n",
      "63901/63901 [==============================] - 14s - loss: 0.1032 - acc: 0.9637    \n",
      "Epoch 2/4\n",
      "63901/63901 [==============================] - 14s - loss: 0.0594 - acc: 0.9795    \n",
      "Epoch 3/4\n",
      "63901/63901 [==============================] - 14s - loss: 0.0530 - acc: 0.9812    \n",
      "Epoch 4/4\n",
      "63901/63901 [==============================] - 14s - loss: 0.0481 - acc: 0.9828    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 3/3 [02:59<00:00, 59.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2, cv score 0.0522\n",
      "[ 0.05086079  0.05117356  0.05216046]\n",
      "0.0513982719331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "embedding_dims = 80\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "def embedding_words(X, y):\n",
    "    print('Build model...')\n",
    "    comment_input = Input((maxlen,))\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    comment_emb = Embedding(max_features, embedding_dims, input_length=maxlen)(comment_input)\n",
    "\n",
    "    # we add a GlobalMaxPool1D, which will extract information from the embeddings\n",
    "    # of all words in the document\n",
    "    comment_emb = SpatialDropout1D(0.25)(comment_emb)\n",
    "    max_emb = GlobalMaxPool1D()(comment_emb)\n",
    "\n",
    "    # normalized dense layer followed by dropout\n",
    "    main = BatchNormalization()(max_emb)\n",
    "    main = Dense(embedding_dims)(main)\n",
    "    main = Dropout(0.05)(main)\n",
    "\n",
    "    # We project onto a six-unit output layer, and squash it with sigmoids:\n",
    "    output = Dense(6, activation='sigmoid')(main)\n",
    "\n",
    "    model = Model(inputs=comment_input, outputs=output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X, y, batch_size= batch_size, epochs= epochs)\n",
    "    \n",
    "    def _predict(X):\n",
    "        return model.predict(X)\n",
    "    \n",
    "    return _predict\n",
    "\n",
    "scores = cv(embedding_words, x_train, y_train, label2binary)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:25:19.377302Z",
     "start_time": "2018-01-11T09:23:55.201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Epoch 1/4\n",
      "95851/95851 [==============================] - 22s - loss: 0.0916 - acc: 0.9683    \n",
      "Epoch 2/4\n",
      "95851/95851 [==============================] - 21s - loss: 0.0576 - acc: 0.9800    \n",
      "Epoch 3/4\n",
      "95851/95851 [==============================] - 21s - loss: 0.0520 - acc: 0.9815    \n",
      "Epoch 4/4\n",
      "95851/95851 [==============================] - 21s - loss: 0.0485 - acc: 0.9825    \n",
      "CPU times: user 3min 28s, sys: 1min 50s, total: 5min 18s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = embedding_words(np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:25:19.378294Z",
     "start_time": "2018-01-11T09:23:55.202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.5 s, sys: 9.71 s, total: 27.2 s\n",
      "Wall time: 6.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction = model(np.array(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:25:19.379252Z",
     "start_time": "2018-01-11T09:23:55.203Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = DataSet['test']['id']\n",
    "for i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
    "    submission[label] = prediction[:, i]\n",
    "# print(submission.tail(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:25:19.380227Z",
     "start_time": "2018-01-11T09:23:55.203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip ../data/meta/submit/ft_word_char_tfidf_2018-01-18.zip ../data/meta/submit/ft_word_char_tfidf_2018-01-18.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys,os,datetime\n",
    "\n",
    "SubmitOutputDir = '../data/meta/submit'\n",
    "if(os.path.exists(SubmitOutputDir) == False):\n",
    "    os.makedirs(SubmitOutputDir)\n",
    "SubmitFileName = '%s_%s' % (strategy, datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "submission.to_csv('%s/%s.csv' % (SubmitOutputDir, SubmitFileName), index= None)\n",
    "print('zip %s/%s.zip %s/%s.csv' % (SubmitOutputDir, SubmitFileName, SubmitOutputDir, SubmitFileName))\n",
    "os.system('zip -r %s/%s.zip %s/%s.csv' % (SubmitOutputDir, SubmitFileName, SubmitOutputDir, SubmitFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
