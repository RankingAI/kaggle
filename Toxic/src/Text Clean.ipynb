{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "\n",
    "class DataUtil2:\n",
    "    \"\"\"\"\"\"\n",
    "    @classmethod\n",
    "    def load(cls, file, format, date_cols= None):\n",
    "        \"\"\"\"\"\"\n",
    "        data = ''\n",
    "        if(format== 'csv'):\n",
    "            data = pd.read_csv(file, parse_dates= date_cols)\n",
    "        elif(format== 'json'):\n",
    "            with open(file, 'r') as i_file:\n",
    "                data = json.load(file)\n",
    "            i_file.close()\n",
    "        elif(format== 'pkl'):\n",
    "            with open(file, 'rb') as i_file:\n",
    "                data = pickle.load(i_file)\n",
    "            i_file.close()\n",
    "        elif(format == 'hdf'):\n",
    "            data = pd.read_hdf(path_or_buf= file, key='undefined')\n",
    "\n",
    "        return  data\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls, data, file, format, precision= 8):\n",
    "        \"\"\"\"\"\"\n",
    "        if(format == 'csv'):\n",
    "            data.to_csv(file, float_format= '%%.%df' % precision, index= False)\n",
    "        elif(format == 'json'):\n",
    "            with open(file, 'w') as o_file:\n",
    "                json.dump(data, o_file, ensure_ascii= True, indent= 4)\n",
    "            o_file.close()\n",
    "        elif(format == 'pkl'):\n",
    "            with open(file, 'wb') as o_file:\n",
    "                pickle.dump(data, o_file, -1)\n",
    "            o_file.close()\n",
    "        elif(format== 'hdf'):\n",
    "            data.to_hdf(path_or_buf= file, key='undefined', mode='w', complib='blosc')\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T12:04:34.105688Z",
     "start_time": "2017-12-28T12:04:14.350022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length 95851.\n",
      "0 done, time elapsed 0.00s.\n",
      "5000 done, time elapsed 7.88s.\n",
      "10000 done, time elapsed 15.62s.\n",
      "15000 done, time elapsed 23.45s.\n",
      "20000 done, time elapsed 31.56s.\n",
      "25000 done, time elapsed 38.80s.\n",
      "30000 done, time elapsed 46.34s.\n",
      "35000 done, time elapsed 54.20s.\n",
      "40000 done, time elapsed 62.00s.\n",
      "45000 done, time elapsed 69.43s.\n",
      "50000 done, time elapsed 77.00s.\n",
      "55000 done, time elapsed 85.24s.\n",
      "60000 done, time elapsed 92.66s.\n",
      "65000 done, time elapsed 100.07s.\n",
      "70000 done, time elapsed 107.34s.\n",
      "75000 done, time elapsed 115.29s.\n",
      "80000 done, time elapsed 123.35s.\n",
      "85000 done, time elapsed 131.17s.\n",
      "90000 done, time elapsed 139.19s.\n",
      "95000 done, time elapsed 147.67s.\n",
      "time 149.18s\n",
      "total length 226998.\n",
      "0 done, time elapsed 0.00s.\n",
      "5000 done, time elapsed 9.38s.\n",
      "10000 done, time elapsed 30.80s.\n",
      "15000 done, time elapsed 40.95s.\n",
      "20000 done, time elapsed 49.40s.\n",
      "25000 done, time elapsed 58.33s.\n",
      "30000 done, time elapsed 67.23s.\n",
      "35000 done, time elapsed 76.75s.\n",
      "40000 done, time elapsed 85.39s.\n",
      "45000 done, time elapsed 276.49s.\n",
      "50000 done, time elapsed 285.36s.\n",
      "55000 done, time elapsed 293.91s.\n",
      "60000 done, time elapsed 302.45s.\n",
      "65000 done, time elapsed 310.98s.\n",
      "70000 done, time elapsed 319.98s.\n",
      "75000 done, time elapsed 328.73s.\n",
      "80000 done, time elapsed 337.84s.\n",
      "85000 done, time elapsed 346.79s.\n",
      "90000 done, time elapsed 355.69s.\n",
      "95000 done, time elapsed 365.23s.\n",
      "100000 done, time elapsed 373.84s.\n",
      "105000 done, time elapsed 382.66s.\n",
      "110000 done, time elapsed 392.30s.\n",
      "115000 done, time elapsed 401.18s.\n",
      "120000 done, time elapsed 409.82s.\n",
      "125000 done, time elapsed 418.69s.\n",
      "130000 done, time elapsed 427.57s.\n",
      "135000 done, time elapsed 436.29s.\n",
      "140000 done, time elapsed 445.13s.\n",
      "145000 done, time elapsed 454.13s.\n",
      "150000 done, time elapsed 462.66s.\n",
      "155000 done, time elapsed 471.44s.\n",
      "160000 done, time elapsed 480.56s.\n",
      "165000 done, time elapsed 491.36s.\n",
      "170000 done, time elapsed 500.36s.\n",
      "175000 done, time elapsed 509.08s.\n",
      "180000 done, time elapsed 518.99s.\n",
      "185000 done, time elapsed 528.73s.\n",
      "190000 done, time elapsed 537.43s.\n",
      "195000 done, time elapsed 546.06s.\n",
      "200000 done, time elapsed 555.55s.\n",
      "205000 done, time elapsed 564.96s.\n",
      "210000 done, time elapsed 574.22s.\n",
      "215000 done, time elapsed 583.90s.\n",
      "220000 done, time elapsed 592.94s.\n",
      "225000 done, time elapsed 603.04s.\n",
      "time 607.12s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # for aristhmetic computing\n",
    "import numpy as np \n",
    "import math\n",
    "import datetime # for timing\n",
    "import os,sys,gc # for debuging\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "# replacements\n",
    "replacement_patterns = [\n",
    "     (r'won\\'t', 'will not'),\n",
    "     (r'can\\'t', 'cannot'),\n",
    "     (r'i\\'m', 'i am'),\n",
    "     (r'ain\\'t', 'is not'),\n",
    "     (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "     (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "     (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "     (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "     (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "     (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s\n",
    "# special characters\n",
    "special_character_removal= re.compile(r'[^a-z\\d]', re.IGNORECASE)\n",
    "# numeric characters\n",
    "replace_numbers= re.compile(r'\\d+', re.IGNORECASE)\n",
    "# stop words\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "# lemmatization\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(sentence):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "# process rows\n",
    "def ApplyBOW(ColValues):\n",
    "    ''''''\n",
    "    n = len(ColValues)\n",
    "    result = ['' for i in range(n)]\n",
    "    i = 0\n",
    "    print('total length %s.' % n)\n",
    "    start = time.time()\n",
    "    for i in range(n):\n",
    "        end = time.time()\n",
    "        if(i % 5000 == 0):\n",
    "            print('%s done, time elapsed %.2fs.' % (i, (end - start)))\n",
    "        row = str(ColValues[i])\n",
    "        # replacement\n",
    "        replacer = RegexpReplacer()\n",
    "        row = replacer.replace(row)\n",
    "        #Remove Special Characters\n",
    "        row = special_character_removal.sub(' ', row)\n",
    "        #Replace Numbers\n",
    "        row = replace_numbers.sub('n', row)\n",
    "        # remove punctations\n",
    "        tokenizer = RegexpTokenizer(r\"[\\w']+\")\n",
    "        if(row == np.nan):\n",
    "            word_vec = []\n",
    "        try:\n",
    "            word_vec = tokenizer.tokenize(row)\n",
    "        except Exception as e:\n",
    "            print(type(row))\n",
    "            sys.exit(1)\n",
    "        # to lower\n",
    "        word_vec = [word.lower() for word in word_vec]\n",
    "\n",
    "        # remove stopwords\n",
    "        word_vec = [word for word in word_vec if(word not in stopwords_set)]\n",
    "        # lemmatization\n",
    "        word_vec = lemmatize_all(word_vec)\n",
    "        # length > 1\n",
    "        word_vec = [w for w in word_vec if(len(w) > 1)]\n",
    "        # join\n",
    "        text = ' '.join(word_vec)\n",
    "        result[i] = text\n",
    "        i += 1\n",
    "    return result\n",
    "# data source   \n",
    "DataBase = '../data'\n",
    "DataSet = {\n",
    "    'train': pd.read_csv('%s/raw/train.csv' % DataBase),\n",
    "    'test': pd.read_csv('%s/raw/test.csv' % DataBase),\n",
    "    #'sub': pd.read_csv('%s/raw/sample_submission.csv' % DataBase)\n",
    "}\n",
    "# save\n",
    "OutputDir = '%s/clean' % DataBase\n",
    "if(os.path.exists(OutputDir) == False):\n",
    "    os.makedirs(OutputDir)\n",
    "COMMENT = 'comment_text'\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for col in label_cols:\n",
    "    DataSet['test'][col] = 0\n",
    "# entry\n",
    "# #TODO\n",
    "# np.random.seed(2017)\n",
    "# DataSet['train'] = DataSet['train'].sample(frac= 0.1)\n",
    "# DataSet['test'] = DataSet['test'].sample(frac= 0.1)\n",
    "for mod in ['train', 'test']:\n",
    "    s = time.time()\n",
    "    # fill null comment text\n",
    "    DataSet[mod][COMMENT].fillna(\"\", inplace= True)\n",
    "    # process rows\n",
    "    DataSet[mod][COMMENT] = ApplyBOW(DataSet[mod][COMMENT].values)\n",
    "    # log1p length\n",
    "    #DataSet[mod]['total_log1p_len'] = np.log1p(DataSet[mod][COMMENT].map(lambda x: len(x.split(' '))))\n",
    "    # none label\n",
    "    DataSet[mod]['none'] = 1 - DataSet[mod][label_cols].max(axis=1)\n",
    "    # save\n",
    "    DataSet[mod][:200].to_csv('%s/%s.csv' % (OutputDir, mod), index= False)\n",
    "    DataUtil2.save(DataSet[mod], '%s/%s.%s' % (OutputDir, mod, 'hdf'), 'hdf')\n",
    "    e = time.time()\n",
    "    print('time %.2fs' % ((e - s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
