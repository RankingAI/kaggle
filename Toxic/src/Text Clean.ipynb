{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "\n",
    "class DataUtil2:\n",
    "    \"\"\"\"\"\"\n",
    "    @classmethod\n",
    "    def load(cls, file, format, date_cols= None):\n",
    "        \"\"\"\"\"\"\n",
    "        data = ''\n",
    "        if(format== 'csv'):\n",
    "            data = pd.read_csv(file, parse_dates= date_cols)\n",
    "        elif(format== 'json'):\n",
    "            with open(file, 'r') as i_file:\n",
    "                data = json.load(file)\n",
    "            i_file.close()\n",
    "        elif(format== 'pkl'):\n",
    "            with open(file, 'rb') as i_file:\n",
    "                data = pickle.load(i_file)\n",
    "            i_file.close()\n",
    "        elif(format == 'hdf'):\n",
    "            data = pd.read_hdf(path_or_buf= file, key='undefined')\n",
    "\n",
    "        return  data\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls, data, file, format, precision= 8):\n",
    "        \"\"\"\"\"\"\n",
    "        if(format == 'csv'):\n",
    "            data.to_csv(file, float_format= '%%.%df' % precision, index= False)\n",
    "        elif(format == 'json'):\n",
    "            with open(file, 'w') as o_file:\n",
    "                json.dump(data, o_file, ensure_ascii= True, indent= 4)\n",
    "            o_file.close()\n",
    "        elif(format == 'pkl'):\n",
    "            with open(file, 'wb') as o_file:\n",
    "                pickle.dump(data, o_file, -1)\n",
    "            o_file.close()\n",
    "        elif(format== 'hdf'):\n",
    "            data.to_hdf(path_or_buf= file, key='undefined', mode='w', complib='blosc')\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T12:04:34.105688Z",
     "start_time": "2017-12-28T12:04:14.350022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length 95851.\n",
      "0 done, time elapsed 0.00s.\n",
      "5000 done, time elapsed 0.22s.\n",
      "10000 done, time elapsed 0.45s.\n",
      "15000 done, time elapsed 0.66s.\n",
      "20000 done, time elapsed 0.86s.\n",
      "25000 done, time elapsed 1.07s.\n",
      "30000 done, time elapsed 1.28s.\n",
      "35000 done, time elapsed 1.52s.\n",
      "40000 done, time elapsed 1.74s.\n",
      "45000 done, time elapsed 1.96s.\n",
      "50000 done, time elapsed 2.17s.\n",
      "55000 done, time elapsed 2.40s.\n",
      "60000 done, time elapsed 2.61s.\n",
      "65000 done, time elapsed 2.85s.\n",
      "70000 done, time elapsed 3.06s.\n",
      "75000 done, time elapsed 3.28s.\n",
      "80000 done, time elapsed 3.50s.\n",
      "85000 done, time elapsed 3.72s.\n",
      "90000 done, time elapsed 3.95s.\n",
      "95000 done, time elapsed 4.16s.\n",
      "time 4.84s\n",
      "total length 226998.\n",
      "0 done, time elapsed 0.00s.\n",
      "5000 done, time elapsed 0.25s.\n",
      "10000 done, time elapsed 0.52s.\n",
      "15000 done, time elapsed 0.78s.\n",
      "20000 done, time elapsed 1.01s.\n",
      "25000 done, time elapsed 1.25s.\n",
      "30000 done, time elapsed 1.50s.\n",
      "35000 done, time elapsed 1.76s.\n",
      "40000 done, time elapsed 1.99s.\n",
      "45000 done, time elapsed 2.23s.\n",
      "50000 done, time elapsed 2.46s.\n",
      "55000 done, time elapsed 2.70s.\n",
      "60000 done, time elapsed 2.94s.\n",
      "65000 done, time elapsed 3.15s.\n",
      "70000 done, time elapsed 3.38s.\n",
      "75000 done, time elapsed 3.62s.\n",
      "80000 done, time elapsed 3.86s.\n",
      "85000 done, time elapsed 4.11s.\n",
      "90000 done, time elapsed 4.33s.\n",
      "95000 done, time elapsed 4.56s.\n",
      "100000 done, time elapsed 4.77s.\n",
      "105000 done, time elapsed 5.02s.\n",
      "110000 done, time elapsed 5.26s.\n",
      "115000 done, time elapsed 5.49s.\n",
      "120000 done, time elapsed 5.73s.\n",
      "125000 done, time elapsed 5.99s.\n",
      "130000 done, time elapsed 6.23s.\n",
      "135000 done, time elapsed 6.48s.\n",
      "140000 done, time elapsed 6.73s.\n",
      "145000 done, time elapsed 6.97s.\n",
      "150000 done, time elapsed 7.21s.\n",
      "155000 done, time elapsed 7.45s.\n",
      "160000 done, time elapsed 7.69s.\n",
      "165000 done, time elapsed 7.99s.\n",
      "170000 done, time elapsed 8.22s.\n",
      "175000 done, time elapsed 8.46s.\n",
      "180000 done, time elapsed 8.70s.\n",
      "185000 done, time elapsed 8.94s.\n",
      "190000 done, time elapsed 9.19s.\n",
      "195000 done, time elapsed 9.42s.\n",
      "200000 done, time elapsed 9.67s.\n",
      "205000 done, time elapsed 9.92s.\n",
      "210000 done, time elapsed 10.17s.\n",
      "215000 done, time elapsed 10.43s.\n",
      "220000 done, time elapsed 10.67s.\n",
      "225000 done, time elapsed 10.92s.\n",
      "time 12.78s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # for aristhmetic computing\n",
    "import numpy as np \n",
    "import math\n",
    "import datetime # for timing\n",
    "import os,sys,gc # for debuging\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n",
    "import re\n",
    "\n",
    "# replacements\n",
    "replacement_patterns = [\n",
    "     (r'won\\'t', 'will not'),\n",
    "     (r'can\\'t', 'cannot'),\n",
    "     (r'i\\'m', 'i am'),\n",
    "     (r'ain\\'t', 'is not'),\n",
    "     (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "     (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "     (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "     (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "     (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "     (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s\n",
    "# stop words\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "# lemmatization\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(sentence):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "# process rows\n",
    "def ApplyBOW(ColValues):\n",
    "    ''''''\n",
    "    n = len(ColValues)\n",
    "    result = ['' for i in range(n)]\n",
    "    i = 0\n",
    "    print('total length %s.' % n)\n",
    "    start = time.time()\n",
    "    for i in range(n):\n",
    "        end = time.time()\n",
    "        if(i % 5000 == 0):\n",
    "            print('%s done, time elapsed %.2fs.' % (i, (end - start)))\n",
    "        row = str(ColValues[i])\n",
    "        # replacement\n",
    "        replacer = RegexpReplacer()\n",
    "        row = replacer.replace(row)\n",
    "        #Remove Special Characters\n",
    "        row = special_character_removal.sub('', row)\n",
    "        #Replace Numbers\n",
    "        row = replace_numbers.sub('n', row)\n",
    "        # remove punctations\n",
    "        tokenizer = RegexpTokenizer(r\"[\\w']+\")\n",
    "        if(row == np.nan):\n",
    "            word_vec = []\n",
    "        try:\n",
    "            word_vec = tokenizer.tokenize(row)\n",
    "        except Exception as e:\n",
    "            print(type(row))\n",
    "            sys.exit(1)\n",
    "        # to lower\n",
    "        word_vec = [word.lower() for word in word_vec]\n",
    "        # remove stopwords\n",
    "        word_vec = [word for word in word_vec if(word not in stopwords_set)]\n",
    "        # lemmatization\n",
    "        word_vec = lemmatize_all(word_vec)\n",
    "        # length > 1\n",
    "        word_vec = [w for w in word_vec if(len(w) > 1)]\n",
    "        # join\n",
    "        text = ' '.join(word_vec)\n",
    "        result[i] = text\n",
    "        i += 1\n",
    "    return result\n",
    "# data source   \n",
    "DataBase = '../data'\n",
    "DataSet = {\n",
    "    'train': pd.read_csv('%s/raw/train.csv' % DataBase),\n",
    "    'test': pd.read_csv('%s/raw/test.csv' % DataBase),\n",
    "    #'sub': pd.read_csv('%s/raw/sample_submission.csv' % DataBase)\n",
    "}\n",
    "# save\n",
    "OutputDir = '%s/clean' % DataBase\n",
    "if(os.path.exists(OutputDir) == False):\n",
    "    os.makedirs(OutputDir)\n",
    "COMMENT = 'comment_text'\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for col in label_cols:\n",
    "    DataSet['test'][col] = 0\n",
    "# entry\n",
    "for mod in ['train', 'test']:\n",
    "    s = time.time()\n",
    "    # fill null comment text\n",
    "    DataSet[mod][COMMENT].fillna(\"\", inplace= True)\n",
    "    # process rows\n",
    "    DataSet[mod][COMMENT] = ApplyBOW(DataSet[mod][COMMENT].values)\n",
    "    # log1p length\n",
    "    #DataSet[mod]['total_log1p_len'] = np.log1p(DataSet[mod][COMMENT].map(lambda x: len(x.split(' '))))\n",
    "    # none label\n",
    "    DataSet[mod]['none'] = 1 - DataSet[mod][label_cols].max(axis=1)\n",
    "    # save\n",
    "    DataSet[mod][:200].to_csv('%s/%s.csv' % (OutputDir, mod), index= False)\n",
    "    DataUtil2.save(DataSet[mod], '%s/%s.%s' % (OutputDir, mod, 'hdf'), 'hdf')\n",
    "    e = time.time()\n",
    "    print('time %.2fs' % ((e - s)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
