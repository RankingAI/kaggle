{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T09:10:01.247289Z",
     "start_time": "2017-12-20T09:09:29.011895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============\n",
      "tra data: unique stores 829, total 252108, time elased 0.75s.\n",
      "tes data: unique stores 821, total 32019, time elased 0.14s.\n",
      "============= process date related done.\n",
      "\n",
      " ============= process store data done.\n",
      "\n",
      " ============= process holiday data done.\n",
      "\n",
      "================ join holiday, store data done.\n",
      " ============= process reservation data done.\n",
      "\n",
      "============= join reservation data done.\n",
      "\n",
      "total groups 829 \n",
      "time elapsed 480.32026290893555\n",
      " ============= add rolling features done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numba\n",
    "import os,sys\n",
    "import gc\n",
    "\n",
    "def LoadData(InputDir):\n",
    "    \"\"\"\"\"\"\n",
    "    ## load raw data\n",
    "    data = {\n",
    "        'tra': pd.read_csv('%s/air_visit_data.csv' % InputDir, parse_dates= ['visit_date']),\n",
    "        'as': pd.read_csv('%s/air_store_info.csv' % InputDir),\n",
    "        'hs': pd.read_csv('%s/hpg_store_info.csv' % InputDir),\n",
    "        'ar': pd.read_csv('%s/air_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'hr': pd.read_csv('%s/hpg_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'id': pd.read_csv('%s/store_id_relation.csv' % InputDir),\n",
    "        'tes': pd.read_csv('%s/sample_submission.csv' % InputDir),\n",
    "        'hol': pd.read_csv('%s/date_info.csv' % InputDir, parse_dates=['calendar_date']).rename(columns={'calendar_date': 'visit_date'})\n",
    "    }\n",
    "    return data\n",
    "\n",
    "@numba.jit\n",
    "def ApplyDayoff(VisitCols, ReserveCols):\n",
    "    \"\"\"\"\"\"\n",
    "    n = len(VisitCols)\n",
    "    result = np.zeros((n, 1), dtype= 'int8')\n",
    "    for i in range(n):\n",
    "        d = (VisitCols[i]- ReserveCols[i]).days\n",
    "        if(d > 0):\n",
    "            result[i] = d\n",
    "    return result\n",
    "\n",
    "reserve2id = {'ar': 'air', 'hr': 'hpg'}\n",
    "reserve2store = {'ar': 'as', 'hr': 'hs'}# load data set\n",
    "InputDir = '../../data/raw'\n",
    "DataSet = LoadData(InputDir)\n",
    "#### \n",
    "# date related features\n",
    "print('\\n============')\n",
    "for mod in ['tra', 'tes']:\n",
    "    start0 = time.time()\n",
    "    if (mod == 'tes'):\n",
    "        DataSet[mod]['visit_date'] = DataSet[mod]['id'].map(lambda x: str(x).split('_')[2])\n",
    "        DataSet[mod]['air_store_id'] = DataSet[mod]['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "        DataSet[mod]['visit_date'] = pd.to_datetime(DataSet[mod]['visit_date'])\n",
    "    DataSet[mod]['dow'] = DataSet[mod]['visit_date'].dt.dayofweek\n",
    "    DataSet[mod]['year'] = DataSet[mod]['visit_date'].dt.year\n",
    "    DataSet[mod]['month'] = DataSet[mod]['visit_date'].dt.month\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_date'].dt.date\n",
    "    end0 = time.time()\n",
    "    print('%s data: unique stores %s, total %s, time elased %.2fs.' %\n",
    "            (mod, len(DataSet[mod]['air_store_id'].unique()), len(DataSet[mod]['air_store_id']), (end0 - start0)))\n",
    "print('============= process date related done.\\n')\n",
    "######## store data\n",
    "# add city feature\n",
    "for mod in ['ar', 'hr']:\n",
    "    DataSet[reserve2store[mod]]['%s_city' % reserve2id[mod]] = DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]].str[:5]\n",
    "# area (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_area_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g\n",
    "        ac['%s_area_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        #ac['%s_area_store_ratio' % reserve2id[mod]] = ac['%s_area_store_count' % reserve2id[mod]]/len(DataSet[reserve2store[mod]])\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_area_name' % reserve2id[mod]])\n",
    "# genre (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_genre_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g\n",
    "        ac['%s_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_genre_name' % reserve2id[mod]])\n",
    "#  area_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_area_name' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_area_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "# city (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_city' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g\n",
    "        ac['%s_city_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        #ac['%s_area_store_ratio' % reserve2id[mod]] = ac['%s_area_store_count' % reserve2id[mod]]/len(DataSet[reserve2store[mod]])\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_city' % reserve2id[mod]])\n",
    "#  city_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_city' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_city_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "print(' ============= process store data done.\\n')\n",
    "######### holiday data\n",
    "data = DataSet['hol']\n",
    "data['visit_date'] = data['visit_date'].dt.date\n",
    "data = data.sort_values(by= 'visit_date')\n",
    "def TagHoliday(df):\n",
    "    ''''''\n",
    "    n = len(df)\n",
    "    result = ['' for x in range(n)]\n",
    "    for i in range(n):\n",
    "        if(i == 0):\n",
    "            result[i] = 'hid_%s' % 0\n",
    "        elif((df[i] - df[i-1]).days == 1):\n",
    "            result[i] = result[i - 1]\n",
    "        else:\n",
    "            result[i] = 'hid_%s' % (int(result[i - 1].split('_')[1]) + 1)\n",
    "    return result\n",
    "holidays = data[data['holiday_flg'] == 1][['visit_date']]\n",
    "holidays['hol_l0'] = TagHoliday(holidays['visit_date'].values)\n",
    "groupped = holidays.groupby(['hol_l0'])\n",
    "recs = []\n",
    "for g in groupped.groups:\n",
    "    hol_days = {}\n",
    "    hol_days['hol_l0'] = g\n",
    "    hol_days['hol_days'] = len(groupped.get_group(g))\n",
    "    recs.append(hol_days)\n",
    "tmpdf = pd.DataFrame(data= recs, index= range(len(recs)))\n",
    "holidays = holidays.merge(tmpdf, how= 'left', on= 'hol_l0')\n",
    "data = data.merge(holidays, how= 'left', on= 'visit_date')\n",
    "data.drop(['hol_l0'], axis= 1, inplace= True)\n",
    "data['hol_days'].fillna(0, inplace= True)\n",
    "DataSet['hol'] = data\n",
    "print(' ============= process holiday data done.\\n')\n",
    "######## join \n",
    "# join holiday data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    data = data.merge(DataSet['hol'], how='left', on=['visit_date'])\n",
    "    data.drop(['day_of_week', 'year'], axis=1, inplace=True)\n",
    "    DataSet[mod] = data\n",
    "# join store data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[reserve2store[rtype]], how= 'left', on= ['%s_store_id' % reserve2id[rtype]])\n",
    "    DataSet[mod] = data\n",
    "print('================ join holiday, store data done.')\n",
    "######### reservation data\n",
    "for mod in ['hr', 'ar']:\n",
    "    start1 = time.time()\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_datetime'].dt.date\n",
    "    DataSet[mod]['reserve_date'] = DataSet[mod]['reserve_datetime'].dt.date\n",
    "    DataSet[mod].drop(['reserve_datetime', 'visit_datetime'], axis= 1, inplace= True)\n",
    "    tmpdf = pd.DataFrame(data=ApplyDayoff(DataSet[mod]['visit_date'].values, DataSet[mod]['reserve_date'].values),index=DataSet[mod].index, columns=['reserve_date_diff'])\n",
    "    tmpdf = pd.concat([DataSet[mod], tmpdf], axis=1)\n",
    "    tmpdf = tmpdf.groupby(['%s_store_id' % reserve2id[mod], 'visit_date'], as_index=False).agg({'reserve_visitors': sum, 'reserve_date_diff': ['mean', 'median']})\n",
    "    tmpdf.columns = ['%s_store_id' % reserve2id[mod], \n",
    "                   'visit_date', \n",
    "                   '%s_reserved_visitors' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_mean' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_median' % reserve2id[mod]\n",
    "                  ]\n",
    "    end1 = time.time()\n",
    "    DataSet[mod] = tmpdf\n",
    "    DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]] = np.log1p(DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]])\n",
    "print(' ============= process reservation data done.\\n')\n",
    "# join reservation data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[rtype], how= 'left', on= ['%s_store_id' % reserve2id[rtype], 'visit_date'])\n",
    "    DataSet[mod] = data\n",
    "#print(DataSet['tra'][['air_store_id', 'visit_date', 'air_reserved_visitors', 'air_reserved_dayoff_mean']].head(100))\n",
    "print('============= join reservation data done.\\n')\n",
    "####### add rolling features\n",
    "s = time.time()\n",
    "\n",
    "# mix train with test\n",
    "DataSet['tra']['is_train'] = 1\n",
    "DataSet['tes']['is_train'] = 0\n",
    "AllData = pd.concat([DataSet['tra'], DataSet['tes']], axis= 0, ignore_index= True)\n",
    "groupped = AllData.groupby(['air_store_id'])\n",
    "visitor_ticks = [39, 46, 60, 74]\n",
    "reservation_ticks = [7, 14, 21, 28]\n",
    "print('total groups %s ' % len(groupped.groups))\n",
    "dfs = []\n",
    "# rolling sum\n",
    "for g in groupped.groups: \n",
    "    gdf = groupped.get_group(g).sort_values(by= ['visit_date'])\n",
    "    for t in visitor_ticks:\n",
    "        gdf['visitor_tick_%s' % t] = np.log1p(gdf['visitors']).rolling(window= t).sum()\n",
    "        gdf['visitor_tick_%s' % t].fillna(0, inplace= True)\n",
    "#     for t in reservation_ticks:\n",
    "#         for mod in ['air', 'hpg']:\n",
    "#             gdf['reservation_%s_rolling_%s_sum' % (mod, t)] = gdf['%s_reserved_visitors' % mod].rolling(window= t).sum()\n",
    "#             gdf['reservation_%s_rolling_%s_sum' % (mod, t)].fillna(0, inplace= True)\n",
    "    gdf['holiday_rolling_3'] = gdf['holiday_flg'].rolling(window= 3).sum()\n",
    "    gdf['holiday_rolling_3'].fillna(0, inplace= True)\n",
    "#     gdf['holiday_rolling_2'] = gdf['holiday_flg'].rolling(window= 2).sum()\n",
    "#     gdf['holiday_rolling_2'].fillna(0, inplace= True)\n",
    "    dfs.append(gdf)\n",
    "# concate\n",
    "tmpdf = pd.concat(dfs, axis= 0, ignore_index= True)\n",
    "join_cols = ['air_store_id', 'visit_date', 'holiday_rolling_3']\n",
    "for i in range(len(visitor_ticks)):\n",
    "    if(i == 0):\n",
    "        continue\n",
    "    k = 'visitor_rolling_%s_%s_mean' % (visitor_ticks[i], visitor_ticks[i - 1])\n",
    "    tmpdf[k] = (tmpdf['visitor_tick_%s' % visitor_ticks[i]] - tmpdf['visitor_tick_%s' % visitor_ticks[i - 1]]) / (visitor_ticks[i] - visitor_ticks[i - 1])\n",
    "    tmpdf[k].fillna(0, inplace= True)\n",
    "    join_cols.append(k)\n",
    "# for t in reservation_ticks:\n",
    "#     for mod in ['air', 'hpg']:\n",
    "#         join_cols.append('reservation_%s_rolling_%s_sum' % (mod, t))\n",
    "# merge\n",
    "tmpdf.drop(['visitor_tick_%s' % col for col in visitor_ticks], axis= 1, inplace= True)\n",
    "AllData = AllData.merge(tmpdf[join_cols], how= 'left', on= ['air_store_id', 'visit_date'])\n",
    "# restore\n",
    "DataSet['tra'] = AllData[AllData['is_train'] == 1]\n",
    "DataSet['tes'] = AllData[AllData['is_train'] == 0]\n",
    "DataSet['tra'].drop(['is_train'], axis= 1, inplace= True)\n",
    "DataSet['tes'].drop(['is_train'], axis= 1, inplace= True)\n",
    "del AllData\n",
    "vdf = DataSet['tra'][DataSet['tra']['air_store_id'] == 'air_ba937bf13d40fb24']\n",
    "#print(vdf[join_cols].head(200))\n",
    "\n",
    "e = time.time()\n",
    "print('time elapsed %s' % ((e - s) * 60))\n",
    "print(' ============= add rolling features done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T09:10:06.299912Z",
     "start_time": "2017-12-20T09:10:01.249924Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features  ['air_genre_name', 'air_area_name', 'air_city', 'hpg_genre_name', 'hpg_area_name', 'hpg_city']\n"
     ]
    }
   ],
   "source": [
    "#### Label encoding for categorial features\n",
    "from sklearn import *\n",
    "\n",
    "cate_feats = ['genre_name', 'area_name', 'city']\n",
    "cate_cols = ['%s_%s' % (m, cf) for m in ['air', 'hpg'] for cf in cate_feats]\n",
    "for mod in ['tra', 'tes']:\n",
    "    for col in DataSet[mod].columns:\n",
    "        if(col in cate_cols):\n",
    "            DataSet[mod][col].fillna('unknown', inplace= True)\n",
    "        else:\n",
    "            DataSet[mod][col].fillna(-1, inplace= True)\n",
    "print('Categorical features ', cate_cols)\n",
    "TrainData = DataSet['tra']\n",
    "TestData = DataSet['tes']\n",
    "for col in cate_cols:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    TrainData[col] = lbl.fit_transform(TrainData[col])\n",
    "    TestData[col] = lbl.transform(TestData[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T11:26:44.053206Z",
     "start_time": "2017-12-20T11:26:17.092197Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys,os\n",
    "\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred) ** 0.5\n",
    "\n",
    "# split TrainData into train and holdout with random strategy\n",
    "np.random.seed(2017)\n",
    "msk = np.random.rand(len(TrainData)) < 0.1\n",
    "holdout = TrainData[msk]\n",
    "train = TrainData[~msk]\n",
    "# Set up folds\n",
    "K = 5\n",
    "kf = model_selection.KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(1)\n",
    "OutputDir = '../../data/l0'\n",
    "if(os.path.exists('%s/kfold' % OutputDir) == False):\n",
    "    os.makedirs('%s/kfold' % OutputDir)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "    FoldTrain, FoldValid = train.iloc[train_index].copy(), train.iloc[test_index].copy()\n",
    "    FoldHoldout = holdout.copy()\n",
    "    FoldTest = test.copy()\n",
    "    FoldTrain['visitors'] = np.log1p(FoldTrain['visitors'])\n",
    "    FoldValid['visitors'] = np.log1p(FoldValid['visitors'])\n",
    "    FoldHoldout['visitors'] = np.log1p(FoldHoldout['visitors'])\n",
    "    FoldTest['visitors'] = np.log1p(FoldTest['visitors'])\n",
    "    #### dependent features which is extreemly subtle to data-leak\n",
    "    # percentiles features\n",
    "    tickles = ['mean', 'median', 'max', 'min', 'count']\n",
    "    for feat in ['air_store_id']:\n",
    "        gkeys = [feat, 'dow']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'visitors': tickles})    \n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(-1, inplace= True)\n",
    "        FoldHoldout.fillna(-1, inplace= True)\n",
    "        FoldTest.fillna(-1, inplace= True)\n",
    "    for feat in ['air_city']:\n",
    "        gkeys = [feat, 'air_genre_name']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'visitors': tickles})    \n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(-1, inplace= True)\n",
    "        FoldHoldout.fillna(-1, inplace= True)\n",
    "        FoldTest.fillna(-1, inplace= True)\n",
    "    FoldOutputDir = '%s/kfold/%s' % (OutputDir, i)\n",
    "    if(os.path.exists(FoldOutputDir) == False):\n",
    "        os.makedirs(FoldOutputDir)\n",
    "    #FoldTrain.to_csv('%s/train.csv' % FoldOutputDir)\n",
    "    FoldValid.to_csv('%s/valid.csv' % FoldOutputDir, index= False)\n",
    "    FoldHoldout.to_csv('%s/holdout.csv' % FoldOutputDir, index= False)\n",
    "    FoldTest.to_csv('%s/test.csv' % FoldOutputDir, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
