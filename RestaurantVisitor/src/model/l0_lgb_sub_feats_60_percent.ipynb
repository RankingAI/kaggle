{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-13T15:47:48.187195Z",
     "start_time": "2018-01-13T15:47:47.597300Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "\n",
    "class DataUtil2:\n",
    "    \"\"\"\"\"\"\n",
    "    @classmethod\n",
    "    def load(cls, file, format, date_cols= None):\n",
    "        \"\"\"\"\"\"\n",
    "        data = ''\n",
    "        if(format== 'csv'):\n",
    "            data = pd.read_csv(file, parse_dates= date_cols)\n",
    "        elif(format== 'json'):\n",
    "            with open(file, 'r') as i_file:\n",
    "                data = json.load(file)\n",
    "            i_file.close()\n",
    "        elif(format== 'pkl'):\n",
    "            with open(file, 'rb') as i_file:\n",
    "                data = pickle.load(i_file)\n",
    "            i_file.close()\n",
    "        elif(format == 'hdf'):\n",
    "            data = pd.read_hdf(path_or_buf= file, key='undefined')\n",
    "\n",
    "        return  data\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls, data, file, format, precision= 8):\n",
    "        \"\"\"\"\"\"\n",
    "        if(format == 'csv'):\n",
    "            data.to_csv(file, float_format= '%%.%df' % precision, index= False)\n",
    "        elif(format == 'json'):\n",
    "            with open(file, 'w') as o_file:\n",
    "                json.dump(data, o_file, ensure_ascii= True, indent= 4)\n",
    "            o_file.close()\n",
    "        elif(format == 'pkl'):\n",
    "            with open(file, 'wb') as o_file:\n",
    "                pickle.dump(data, o_file, -1)\n",
    "            o_file.close()\n",
    "        elif(format== 'hdf'):\n",
    "            data.to_hdf(path_or_buf= file, key='undefined', mode='w', complib='blosc')\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-13T15:48:47.292918Z",
     "start_time": "2018-01-13T15:47:48.189672Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for params, l2 200, lr 0.4, depth 12 done, cv score 0.50088, time elapsed 19.38s\n",
      "running for params, l2 200, lr 0.4, depth 16 done, cv score 0.50112, time elapsed 38.41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-f3df39f31540>\", line 132, in <module>\n",
      "    model = lightgbm.train(param, d_cv)\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/engine.py\", line 199, in train\n",
      "    booster.update(fobj=fobj)\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/basic.py\", line 1507, in update\n",
      "    ctypes.byref(is_finished)))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/inspect.py\", line 1411, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/inspect.py\", line 666, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/inspect.py\", line 709, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/inspect.py\", line 678, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/Users/yuanpingzhou/miniconda3/lib/python3.6/inspect.py\", line 656, in getsourcefile\n",
      "    all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:]\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# LigthGBM Regression #\n",
    "#######################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import os,sys\n",
    "import gc\n",
    "from sklearn import *\n",
    "import lightgbm\n",
    "import random\n",
    "import json\n",
    "\n",
    "drop_cols = ['id', 'visit_date', 'visitors', 'hpg_store_id', 'fold', 'air_store_id']\n",
    "\n",
    "cate_cols = ['store_id_encoded', 'area_name', 'city', 'genre_name']\n",
    "cate_feats = ['dow', 'hol_days', 'day', 'pom', 'prev_is_holiday', 'next_is_holiday', \n",
    "              'wom', 'woy', 'is_weekends', 'holiday_flg', 'month', 'is_up_corner']\n",
    "for mod in ['air', 'hpg']:\n",
    "    cate_feats.extend(['%s_%s' % (mod, c) for c in cate_cols])\n",
    "    \n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred) ** 0.5\n",
    "\n",
    "DataBaseDir = '../../data'\n",
    "InputDir = '%s/l0/kfold' % DataBaseDir\n",
    "kfold = 5\n",
    "strategy = 'lgb_l2'\n",
    "#### load data\n",
    "valid_dfs = []\n",
    "holdout_dfs = []\n",
    "test_dfs = []\n",
    "trees = ['NN_EF']\n",
    "for fold in range(kfold):\n",
    "    FoldInputDir = '%s/%s' % (InputDir, fold)\n",
    "    valid = pd.read_csv('%s/valid.csv' % FoldInputDir, parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "    holdout = pd.read_csv('%s/holdout.csv' % FoldInputDir, parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "    test = pd.read_csv('%s/test.csv' % FoldInputDir, parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "    for t in trees:\n",
    "        # load cb_ef\n",
    "        FoldOutputDir = '%s/l1/kfold/%s' % (DataBaseDir, fold)\n",
    "        valid_cb_ef = pd.read_csv('%s/valid_%s.csv' % (FoldOutputDir, t), parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "        holdout_cb_ef = pd.read_csv('%s/holdout_%s.csv' % (FoldOutputDir, t), parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "        test_cb_ef = pd.read_csv('%s/test_%s.csv' % (FoldOutputDir, t), parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "        # concate\n",
    "        valid = pd.concat([valid, valid_cb_ef[[t]]], axis= 1)\n",
    "        holdout = pd.concat([holdout, holdout_cb_ef[[t]]], axis= 1)\n",
    "        test = pd.concat([test, test_cb_ef[[t]]], axis= 1)\n",
    "    #\n",
    "    valid['fold'] = fold\n",
    "    valid_dfs.append(valid)\n",
    "    holdout_dfs.append(holdout)\n",
    "    test_dfs.append(test)\n",
    "TrainData = pd.concat(valid_dfs, axis= 0, ignore_index= True)\n",
    "##### model selection with CV\n",
    "# score\n",
    "holdout_score = .0\n",
    "# parameters\n",
    "params = {\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"objective\": \"regression_l2\",\n",
    "    \"lambda_l2\": [200, 80, 20],\n",
    "    \"learning_rate\": [0.4, 0.2, 0.1],\n",
    "    \"max_depth\": [12, 16, 20],\n",
    "    \n",
    "#     \"lambda_l2\": [20],\n",
    "#     \"learning_rate\": [0.2],\n",
    "#     \"max_depth\": [16],\n",
    "#     \"device\": \"gpu\",\n",
    "    \n",
    "    \"num_iterations\": 150,\n",
    "    \"min_data_in_leaf\": 20,\n",
    "    'num_leaves': 255,\n",
    "\n",
    "    \"feature_fraction\": 0.90,\n",
    "    \"bagging_fraction\": 0.85,\n",
    "    \"bagging_freq\": 20,\n",
    "    \"min_hessian\": 0.001,\n",
    "\n",
    "    \"max_bin\": 63,\n",
    "}\n",
    "total_cols = [c for c in TrainData.columns if c not in drop_cols]\n",
    "K = int(0.6 * len(total_cols))\n",
    "N = 200\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for idx in range(N): \n",
    "    selected_cols = [total_cols[i] for i in sorted(random.sample(range(len(total_cols)), K))]\n",
    "    OutputDir = '%s/MM2/l1/%s' % (DataBaseDir, idx)\n",
    "    if(os.path.exists(OutputDir) == False):\n",
    "        os.makedirs(OutputDir)\n",
    "    # save feature space\n",
    "    with open('%s/sub_feats.txt' % OutputDir, 'w') as o_file:\n",
    "        for feat in selected_cols:\n",
    "            o_file.write('%s\\n' % feat)\n",
    "    o_file.close()\n",
    "    ##\n",
    "    BestParmas = {}\n",
    "    BestScore = 1.0\n",
    "    cv_score = .0\n",
    "    for l2 in params['lambda_l2']:\n",
    "        for lr in params['learning_rate']:\n",
    "            for depth in params['max_depth']:\n",
    "                cv_rmsle = .0\n",
    "                for fold in range(kfold):\n",
    "                    FoldData = {\n",
    "                        'train': TrainData[TrainData['fold'] != fold],\n",
    "                        'valid': TrainData[TrainData['fold'] == fold]\n",
    "                    }\n",
    "                    d_cv = lightgbm.Dataset(FoldData['train'][selected_cols], \n",
    "                                        label= FoldData['train']['visitors'], \n",
    "                                        #max_bin= params['max_bin'], \n",
    "                                        silent= True, \n",
    "                                        free_raw_data= True)\n",
    "                    param = {\n",
    "                        'boosting': 'gbdt',\n",
    "                        'objective': 'regression_l2',\n",
    "\n",
    "                        'lambda_l2': l2,\n",
    "                        'learning_rate': lr,\n",
    "                        'max_depth': depth,\n",
    "\n",
    "                        'num_iterations': params['num_iterations'],\n",
    "                        'feature_fraction': params['feature_fraction'],\n",
    "                        'bagging_fraction': params['bagging_fraction'],\n",
    "                        'bagging_freq': params['bagging_freq'],\n",
    "                        'min_hessian': params['min_hessian'],\n",
    "                        'max_bin': params['max_bin'],\n",
    "                    }\n",
    "                    model = lightgbm.train(param, d_cv)\n",
    "                    FoldData['valid'][strategy] = model.predict(FoldData['valid'][selected_cols])\n",
    "                    rmsle_valid = RMSLE(FoldData['valid']['visitors'], FoldData['valid'][strategy])\n",
    "                    cv_rmsle += rmsle_valid\n",
    "                cv_rmsle /= kfold\n",
    "                if(cv_rmsle < BestScore):\n",
    "                    BestScore = cv_rmsle\n",
    "                    BestParmas['lambda_l2'] = l2\n",
    "                    BestParmas['learning_rate'] = lr\n",
    "                    BestParmas['max_depth'] = depth\n",
    "                end = time.time()\n",
    "                print('running for params, l2 %s, lr %s, depth %s done, cv score %.5f, time elapsed %.2fs' % (l2, lr, depth, cv_rmsle, (end - start)))\n",
    "    end = time.time()\n",
    "    print('grid search done, time elapsed %.2fs' % (end - start))\n",
    "    print('best params, %s' % BestParmas)\n",
    "    ## retrain and store\n",
    "    cv_score = .0\n",
    "    holdout_score = .0\n",
    "    for fold in range(kfold):\n",
    "        FoldData = {\n",
    "            'train': TrainData[TrainData['fold'] != fold],\n",
    "            'valid': TrainData[TrainData['fold'] == fold],\n",
    "            'holdout': holdout_dfs[fold],\n",
    "            'test': test_dfs[fold]\n",
    "        }\n",
    "        # train\n",
    "        d_cv = lightgbm.Dataset(FoldData['train'][selected_cols], \n",
    "                            label= FoldData['train']['visitors'].values, \n",
    "                            #max_bin= params['max_bin'], \n",
    "                            silent= True, \n",
    "                            free_raw_data= True)\n",
    "        param = {\n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'regression_l2',\n",
    "                \n",
    "            'lambda_l2': BestParmas['lambda_l2'],\n",
    "            'learning_rate': BestParmas['learning_rate'],\n",
    "            'max_depth': BestParmas['max_depth'],\n",
    "                        \n",
    "            'num_iterations': params['num_iterations'],\n",
    "            'feature_fraction': params['feature_fraction'],\n",
    "            'bagging_fraction': params['bagging_fraction'],\n",
    "            'bagging_freq': params['bagging_freq'],\n",
    "            'min_hessian': params['min_hessian'],\n",
    "            'max_bin': params['max_bin'],\n",
    "        }\n",
    "        model = lightgbm.train(param, d_cv)\n",
    "        # for valid\n",
    "        FoldData['valid'][strategy] = model.predict(FoldData['valid'][selected_cols])\n",
    "        rmsle_valid = RMSLE(FoldData['valid']['visitors'].values, FoldData['valid'][strategy])\n",
    "        cv_score += rmsle_valid\n",
    "        # for holdout\n",
    "        FoldData['holdout'][strategy] = model.predict(FoldData['holdout'][selected_cols])\n",
    "        rmsle_holdout = RMSLE(FoldData['holdout']['visitors'].values, FoldData['holdout'][strategy])\n",
    "        holdout_score += rmsle_holdout\n",
    "        # for test\n",
    "        FoldData['test'][strategy] = model.predict(FoldData['test'][selected_cols])\n",
    "\n",
    "        print('fold %s: valid score %.6f, holdout score %.6f, valid length %s' % (fold, rmsle_valid, rmsle_holdout, len(FoldData['valid'])))  \n",
    "        #### output\n",
    "        FoldOutputDir = '%s/kfold/%s' % (OutputDir, fold)\n",
    "        if(os.path.exists(FoldOutputDir) == False):\n",
    "            os.makedirs(FoldOutputDir)\n",
    "        for mod in FoldData.keys():\n",
    "            if(mod == 'train'):\n",
    "                continue\n",
    "            OutCols = []\n",
    "            if(mod == 'test'):\n",
    "                OutCols.append('id')\n",
    "            OutCols.extend(['air_store_id', 'visit_date', 'visitors', strategy])\n",
    "            OutputFile = '%s/%s_%s.csv' % (FoldOutputDir, mod, strategy)\n",
    "            OutFoldData = FoldData[mod][OutCols]\n",
    "            OutFoldData.to_csv(OutputFile, index= False)\n",
    "    \n",
    "    cv_score /= kfold # Average valid set predictions\n",
    "    holdout_score /= kfold # Average holdout set predictions\n",
    "\n",
    "    end = time.time()\n",
    "    print('\\n======================')\n",
    "    print(\"CV score %.4f, Holdout score %.4f, Elapsed time: %.2fs\" % (cv_score, holdout_score, (end - start)))\n",
    "    print('======================\\n')\n",
    "    \n",
    "    with open('%s/result.txt' % OutputDir, 'w') as o_file:\n",
    "        o_file.write('%s\\n' % json.dumps(BestParmas))\n",
    "        o_file.write('cv score %.4f, holdout score %.4f' % (cv_score, holdout_score))\n",
    "    o_file.close()\n",
    "    \n",
    "    end = time.time()\n",
    "    print('tree %s done, time elapsed %.2f' % (idx, (end - start)))\n",
    "    \n",
    "#     if(idx == 0):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
