{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T17:03:03.308066Z",
     "start_time": "2018-01-16T17:02:59.158151Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numba\n",
    "import os,sys\n",
    "import gc\n",
    "import math\n",
    "from math import ceil\n",
    "from sklearn import *\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def week_of_month(dt):\n",
    "    \"\"\" Returns the week of the month for the specified date.\n",
    "    \"\"\"\n",
    "\n",
    "    first_day = dt.replace(day=1)\n",
    "\n",
    "    dom = dt.day\n",
    "    adjusted_dom = dom + first_day.weekday()\n",
    "\n",
    "    return int(ceil(adjusted_dom/7.0))\n",
    "\n",
    "def LoadData(InputDir):\n",
    "    \"\"\"\"\"\"\n",
    "    ## load raw data\n",
    "    data = {\n",
    "        'tra': pd.read_csv('%s/air_visit_data.csv' % InputDir, parse_dates= ['visit_date']),\n",
    "        'as': pd.read_csv('%s/air_store_info.csv' % InputDir),\n",
    "        'hs': pd.read_csv('%s/hpg_store_info.csv' % InputDir),\n",
    "        'ar': pd.read_csv('%s/air_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'hr': pd.read_csv('%s/hpg_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'id': pd.read_csv('%s/store_id_relation.csv' % InputDir),\n",
    "        'tes': pd.read_csv('%s/sample_submission.csv' % InputDir),\n",
    "        'hol': pd.read_csv('%s/date_info.csv' % InputDir, parse_dates=['calendar_date']).rename(columns={'calendar_date': 'visit_date'})\n",
    "    }\n",
    "    return data\n",
    "\n",
    "@numba.jit\n",
    "def ApplyDayoff(VisitCols, ReserveCols):\n",
    "    \"\"\"\"\"\"\n",
    "    n = len(VisitCols)\n",
    "    result = np.zeros((n, 1), dtype= 'int8')\n",
    "    for i in range(n):\n",
    "        d = (VisitCols[i]- ReserveCols[i]).days\n",
    "        if(d > 0):\n",
    "            result[i] = d\n",
    "    return result\n",
    "\n",
    "reserve2id = {'ar': 'air', 'hr': 'hpg'}\n",
    "reserve2store = {'ar': 'as', 'hr': 'hs'}# load data set\n",
    "InputDir = '../../data/raw'\n",
    "DataSet = LoadData(InputDir)\n",
    "###\n",
    "# print('before missing filling, size %s ' % len(DataSet['tra']))\n",
    "# #Fill in Nans where possible with average in cluster on that day adjusted by the size of the particular restaurants \n",
    "# def fill_nans_in_cluster(genre_name,area_name):\n",
    "#     #get list of the same type of restaurants in the neighborhood\n",
    "#     neighbors_bool = DataSet['as'].apply(lambda x:(x.air_genre_name==genre_name and x.air_area_name==area_name), axis=1)\n",
    "#     neighbors_ids=pd.DataFrame((DataSet['as'][neighbors_bool]))\n",
    "#     neighbors_restaurants= DataSet['tra'].merge(neighbors_ids,on='air_store_id',how='inner')[['air_store_id','visit_date','visitors']]\n",
    " \n",
    "#     #pivot neighbors_restaurants to easy fill in possible missing dates.\n",
    "#     neighbors_restaurants=neighbors_restaurants.pivot_table(index='visit_date',columns='air_store_id', values='visitors',aggfunc=sum)\n",
    "    \n",
    "#     #Fill in missing dates(if any) with Nans\n",
    "#     idx = pd.date_range('2016-01-01', '2017-04-22')\n",
    "#     neighbors_restaurants.index = pd.DatetimeIndex(neighbors_restaurants.index)\n",
    "#     neighbors_restaurants = neighbors_restaurants.reindex(idx, fill_value=np.nan)\n",
    "\n",
    "#     # Get visitors rate, normalized to the avarage number of visitors per day \n",
    "#     neighbors_restaurants_average= neighbors_restaurants.mean(axis=0).tolist()\n",
    "#     normalized_neighbors_restaurants = neighbors_restaurants.div(neighbors_restaurants_average,axis=1)\n",
    "\n",
    "#     # Fill in Nans with avarge number of visiotrs in nighbour restaurants \n",
    "#     #axis argument to fillna is Not Implemented, so have to use transpond\n",
    "#     normalized_neighbors_restaurants_with_filled_nans=normalized_neighbors_restaurants.T.fillna(normalized_neighbors_restaurants.mean(axis=1))\n",
    "    \n",
    "#     #replace normalized values with real vistors by multipliyng back on average per restaurant\n",
    "#     neighbors_restaurants_with_filled_nans = normalized_neighbors_restaurants_with_filled_nans.mul(neighbors_restaurants_average,axis=0).reset_index()\n",
    "\n",
    "#     #return visit data in the original format \n",
    "#     df_columns = neighbors_restaurants_with_filled_nans.columns[1:]\n",
    "#     return  pd.melt(neighbors_restaurants_with_filled_nans,id_vars=['air_store_id'], value_vars=df_columns)\n",
    "\n",
    "# clusters_names= DataSet['as'].apply(lambda x:(x.air_genre_name + '_' + x.air_area_name), axis=1).unique().tolist()\n",
    "# full_data = pd.DataFrame(columns= DataSet['tra'].columns)\n",
    "\n",
    "# cnt = 0\n",
    "# for cluster in clusters_names:\n",
    "#     if(cnt % 200 == 0):\n",
    "#         print('%s processing done.' % cnt)\n",
    "#     cluster_data = fill_nans_in_cluster (cluster.split('_')[0],cluster.split('_')[1])\n",
    "#     cluster_data.rename(columns={'variable':'visit_date','value':'visitors'},inplace=True )\n",
    "#     full_data=full_data.append(cluster_data,ignore_index=True)\n",
    "#     cnt += 1\n",
    "# print('after missing filling, size %s' % len(full_data))\n",
    "# print('%s still missing.' % full_data['visitors'].isnull().sum())\n",
    "# full_data.dropna(inplace= True)\n",
    "# DataSet['tra'] = full_data # reset train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T17:03:49.606330Z",
     "start_time": "2018-01-16T17:03:03.309805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============\n",
      "before:  252108\n",
      "after:  252108\n",
      "tra data: unique stores 829, total 252108, time elased 2.32s.\n",
      "before:  32019\n",
      "after:  32019\n",
      "tes data: unique stores 821, total 32019, time elased 0.31s.\n",
      "============= process date related done.\n",
      "\n",
      "add city feature done.\n",
      " ================ add count features done.\n",
      "\n",
      "add holiday type done.\n",
      "========== reset holiday done.\n",
      "\n",
      "================ join holiday, store data done.\n",
      " process reservation data done.\n",
      "\n",
      "============= join reservation data done.\n",
      "\n",
      "total groups 316 \n",
      "part 0 rolling done.\n",
      "total groups 829 \n",
      "part 1 rolling done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:287: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:288: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== add time series features done. time elapsed 37.12605905532837\n",
      "add date int features done.\n",
      " ============= add time series related features done. time elapsed 37.12605905532837\n",
      "============ add is_up_corner feature done.\n",
      "=========== add interaction count features done. time elapsed 39.97710585594177.\n",
      "day 31\n",
      "pom 3\n",
      "wom 6\n",
      "woy 53\n",
      "prev_is_holiday 2\n",
      "next_is_holiday 2\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# date related features\n",
    "print('\\n============')\n",
    "start = time.time()\n",
    "for mod in ['tra', 'tes']:\n",
    "    start0 = time.time()\n",
    "#     if(mod == 'tra'):\n",
    "#         outliers = DataSet[mod].groupby(['air_store_id'])['visitors'].transform(zscore) > 3\n",
    "#         DataSet[mod][outliers]= np.nan\n",
    "#         print('%s outliers have been removed.' % DataSet[mod].isnull().sum()['air_store_id'])\n",
    "#         DataSet[mod].dropna(inplace=True)       \n",
    "    if (mod == 'tes'):\n",
    "        DataSet[mod]['visit_date'] = DataSet[mod]['id'].map(lambda x: str(x).split('_')[2])\n",
    "        DataSet[mod]['air_store_id'] = DataSet[mod]['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "        DataSet[mod]['visit_date'] = pd.to_datetime(DataSet[mod]['visit_date'])\n",
    "    DataSet[mod]['dow'] = DataSet[mod]['visit_date'].dt.dayofweek\n",
    "    DataSet[mod]['year'] = DataSet[mod]['visit_date'].dt.year\n",
    "    DataSet[mod]['month'] = DataSet[mod]['visit_date'].dt.month\n",
    "    DataSet[mod]['wom'] = DataSet[mod]['visit_date'].apply(week_of_month)\n",
    "    ## remove noisy days, updated 2018/1/4 22:45\n",
    "    print('before: ', len(DataSet[mod]))\n",
    "    print('after: ', len(DataSet[mod]))\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_date'].dt.date\n",
    "    DataSet[mod]['woy'] = DataSet[mod]['visit_date'].apply(lambda x: datetime.date.isocalendar(x)[1])\n",
    "    end0 = time.time()\n",
    "    print('%s data: unique stores %s, total %s, time elased %.2fs.' %\n",
    "            (mod, len(DataSet[mod]['air_store_id'].unique()), len(DataSet[mod]['air_store_id']), (end0 - start0)))\n",
    "print('============= process date related done.\\n')\n",
    "######## store data\n",
    "# add city feature\n",
    "for mod in ['ar', 'hr']:\n",
    "    DataSet[reserve2store[mod]]['%s_city' % reserve2id[mod]] = DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]].map(lambda x: str(x).split(' ')[0])\n",
    "    DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]] = DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]].map(lambda x: '_'.join(x.split(' ')[1:]))\n",
    "print('add city feature done.')\n",
    "# area (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_area_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g\n",
    "        ac['%s_area_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_area_name' % reserve2id[mod]])\n",
    "# genre (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_genre_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g\n",
    "        ac['%s_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_genre_name' % reserve2id[mod]])\n",
    "#  area_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_area_name' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_area_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "# city (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_city' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g\n",
    "        ac['%s_city_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        #ac['%s_area_store_ratio' % reserve2id[mod]] = ac['%s_area_store_count' % reserve2id[mod]]/len(DataSet[reserve2store[mod]])\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_city' % reserve2id[mod]])\n",
    "#  city_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_city' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_city_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "print(' ================ add count features done.\\n')\n",
    "######### holiday data\n",
    "data = DataSet['hol']\n",
    "# updated 2018/1/4 21:45\n",
    "data['day'] = data['visit_date'].dt.day # day of month\n",
    "data['pom'] = data['day'].apply(lambda x: 'start' if(x < 10) else('mid' if(x < 20) else 'end')) # peroid of month\n",
    "### add holiday days\n",
    "data['visit_date'] = data['visit_date'].dt.date\n",
    "data = data.sort_values(by= 'visit_date')\n",
    "def TagHoliday(df):\n",
    "    ''''''\n",
    "    n = len(df)\n",
    "    result = ['' for x in range(n)]\n",
    "    for i in range(n):\n",
    "        if(i == 0):\n",
    "            result[i] = 'hid_%s' % 0\n",
    "        elif((df[i] - df[i-1]).days == 1):\n",
    "            result[i] = result[i - 1]\n",
    "        else:\n",
    "            result[i] = 'hid_%s' % (int(result[i - 1].split('_')[1]) + 1)\n",
    "    return result\n",
    "holidays = data[data['holiday_flg'] == 1][['visit_date']]\n",
    "holidays['hol_l0'] = TagHoliday(holidays['visit_date'].values)\n",
    "groupped = holidays.groupby(['hol_l0'])\n",
    "recs = []\n",
    "for g in groupped.groups:\n",
    "    hol_days = {}\n",
    "    hol_days['hol_l0'] = g\n",
    "    hol_days['hol_days'] = len(groupped.get_group(g))\n",
    "    recs.append(hol_days)\n",
    "tmpdf = pd.DataFrame(data= recs, index= range(len(recs)))\n",
    "holidays = holidays.merge(tmpdf, how= 'left', on= 'hol_l0')\n",
    "data = data.merge(holidays, how= 'left', on= 'visit_date')\n",
    "data.drop(['hol_l0'], axis= 1, inplace= True)\n",
    "data['hol_days'].fillna(0, inplace= True)\n",
    "print('add holiday type done.')\n",
    "### reset holiday\n",
    "wkend_holidays = data.apply((lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "data['is_weekends'] = (data['day_of_week'] == 'Sunday') | (data['day_of_week'] == 'Saturday')\n",
    "data['is_weekends'] = data['is_weekends'].astype(int)\n",
    "data.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "\n",
    "# updated 2018/1/4 21:45\n",
    "data['prevday'] = data['visit_date'] - datetime.timedelta(days= 1)\n",
    "data['nextday'] = data['visit_date'] + datetime.timedelta(days= 1)\n",
    "data.set_index('visit_date', inplace= True)\n",
    "data['prev_is_holiday'] = 0\n",
    "data['prev_is_holiday'] = data[data['prevday'] > datetime.datetime(2016, 1, 1).date()]['prevday'].apply(lambda x: data.loc[x, 'holiday_flg'])\n",
    "data['next_is_holiday'] = 0\n",
    "data['next_is_holiday'] = data[data['nextday'] < datetime.datetime(2017, 5, 31).date()]['nextday'].apply(lambda x: data.loc[x, 'holiday_flg'])\n",
    "data.reset_index(inplace= True)\n",
    "data.drop(['prevday', 'nextday'], axis= 1, inplace= True)\n",
    "\n",
    "DataSet['hol'] = data\n",
    "print('========== reset holiday done.\\n')\n",
    "######## join \n",
    "# join holiday data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    data = data.merge(DataSet['hol'], how='left', on=['visit_date'])\n",
    "    data.drop(['day_of_week', 'year'], axis=1, inplace=True)\n",
    "    DataSet[mod] = data\n",
    "# join store data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[reserve2store[rtype]], how= 'left', on= ['%s_store_id' % reserve2id[rtype]])\n",
    "    DataSet[mod] = data\n",
    "print('================ join holiday, store data done.')\n",
    "######### reservation data\n",
    "for mod in ['hr', 'ar']:\n",
    "    start1 = time.time()\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_datetime'].dt.date\n",
    "    DataSet[mod]['reserve_date'] = DataSet[mod]['reserve_datetime'].dt.date\n",
    "    DataSet[mod].drop(['reserve_datetime', 'visit_datetime'], axis= 1, inplace= True)\n",
    "    #### !!! delete dayoff while restain reserve visitors for other features later on, updated 2018/1/26 22:37\n",
    "    #tmpdf = pd.DataFrame(data=ApplyDayoff(DataSet[mod]['visit_date'].values, DataSet[mod]['reserve_date'].values),index=DataSet[mod].index, columns=['reserve_date_diff'])\n",
    "    #tmpdf = pd.concat([DataSet[mod], tmpdf], axis=1)\n",
    "    #tmpdf = tmpdf.groupby(['%s_store_id' % reserve2id[mod], 'visit_date'], as_index=False).agg({'reserve_visitors': sum, 'reserve_date_diff': ['mean', 'median']})\n",
    "    #tmpdf.columns = ['%s_store_id' % reserve2id[mod], \n",
    "#                    'visit_date', \n",
    "#                    '%s_reserved_visitors' % reserve2id[mod], \n",
    "#                    '%s_reserved_dayoff_mean' % reserve2id[mod], \n",
    "#                    '%s_reserved_dayoff_median' % reserve2id[mod]\n",
    "#                   ]\n",
    "    tmpdf = DataSet[mod].groupby(['%s_store_id' % reserve2id[mod], 'visit_date'], as_index=False).agg({'reserve_visitors': sum})\n",
    "    tmpdf.columns = ['%s_store_id' % reserve2id[mod], 'visit_date', '%s_reserved_visitors' % reserve2id[mod]]\n",
    "    end1 = time.time()\n",
    "    DataSet[mod] = tmpdf\n",
    "    DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]] = np.log1p(DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]])\n",
    "print(' process reservation data done.\\n')\n",
    "# join reservation data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    # merge\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[rtype], how= 'left', on= ['%s_store_id' % reserve2id[rtype], 'visit_date'])\n",
    "#     data['air_reserved_visitors'] = np.log1p(data['air_reserved_visitors'])\n",
    "#     data['hpg_reserved_visitors'] = np.log1p(data['hpg_reserved_visitors'])\n",
    "    data['reserved_visitors'] = (data['air_reserved_visitors'] + data['hpg_reserved_visitors'])/2\n",
    "#     data['reserved_dayoff_mean'] = (data['air_reserved_dayoff_mean'] + data['hpg_reserved_dayoff_mean'])/2\n",
    "#     data['reserved_dayoff_median'] = (data['air_reserved_dayoff_median'] + data['hpg_reserved_dayoff_median'])/2\n",
    "    for rtype in ['ar', 'hr']:\n",
    "        # updated 2017/12/29 13:00\n",
    "        data['%s_reserved_visitors' % reserve2id[rtype]].fillna(0, inplace= True)\n",
    "#         data['%s_reserved_dayoff_mean' % reserve2id[rtype]].fillna(0, inplace= True)\n",
    "#         data['%s_reserved_dayoff_median' % reserve2id[rtype]].fillna(0, inplace= True)\n",
    "    data['reserved_visitors'].fillna(0, inplace= True)\n",
    "#     data['reserved_dayoff_mean'].fillna(0, inplace= True)\n",
    "#     data['reserved_dayoff_median'].fillna(0, inplace= True)\n",
    "    DataSet[mod] = data\n",
    "print('============= join reservation data done.\\n')\n",
    "####### time series related\n",
    "s = time.time()\n",
    "# mix train with test\n",
    "DataSet['tra']['is_train'] = 1\n",
    "DataSet['tes']['is_train'] = 0\n",
    "AllData = pd.concat([DataSet['tra'], DataSet['tes']], axis= 0, ignore_index= True)\n",
    "# !!! dividing into two pieces since 2016/7/1 is a corner point, update time 2017/12/22 15:45\n",
    "DataParts = {\n",
    "    '0': AllData[AllData['visit_date'] < datetime.date(2016, 7, 1)],\n",
    "    '1': AllData[AllData['visit_date'] >= datetime.date(2016, 7, 1)]\n",
    "}\n",
    "## features for rolling average visitors\n",
    "for pidx in DataParts.keys(): \n",
    "    groupped = DataParts[pidx].groupby(['air_store_id'])\n",
    "    visitor_ticks = [39, 46, 53, 60, 67, 74]#, 81, 88, 95, 102, 109, 116, 123]  # for days\n",
    "    print('total groups %s ' % len(groupped.groups))\n",
    "    dfs = []\n",
    "    for g in groupped.groups: \n",
    "        gdf = groupped.get_group(g).sort_values(by= ['visit_date'])\n",
    "        for t in visitor_ticks:\n",
    "            gdf['visitor_tick_sum_%s' % t] = np.log1p(gdf['visitors']).rolling(window= t).sum()\n",
    "            gdf['visitor_tick_sum_%s' % t].fillna(0, inplace= True)\n",
    "            gdf['reserve_visitor_tick_sum_%s' % t] = np.log1p(gdf['air_reserved_visitors']).rolling(window= t).sum()\n",
    "            gdf['reserve_visitor_tick_sum_%s' % t].fillna(0, inplace= True)\n",
    "        dfs.append(gdf)\n",
    "    # concate\n",
    "    tmpdf = pd.concat(dfs, axis= 0, ignore_index= True)\n",
    "    join_cols = ['air_store_id', 'visit_date']\n",
    "    rolling_visitors_cols = []\n",
    "    rolling_reserve_visitors_cols = []\n",
    "    for i in range(len(visitor_ticks)):\n",
    "        if(i == 0):\n",
    "            continue\n",
    "        # rolling visitors mean for one week\n",
    "        k_mean = 'rolling_visitors_%s_%s' % (visitor_ticks[i], visitor_ticks[i - 1])\n",
    "        tmpdf[k_mean] = (tmpdf['visitor_tick_sum_%s' % visitor_ticks[i]] - tmpdf['visitor_tick_sum_%s' % visitor_ticks[i - 1]]) / (visitor_ticks[i] - visitor_ticks[i - 1])\n",
    "        tmpdf.loc[tmpdf[k_mean] < 0, k_mean] = -1  ## negative values exists, need to be set zero, updated 2016/12/22 20:30\n",
    "        join_cols.append(k_mean)\n",
    "        rolling_visitors_cols.append(k_mean)\n",
    "        # rolling reserve visitors mean for one week\n",
    "        k_mean = 'rolling_reserve_visitors_%s_%s' % (visitor_ticks[i], visitor_ticks[i - 1])\n",
    "        tmpdf[k_mean] = (tmpdf['reserve_visitor_tick_sum_%s' % visitor_ticks[i]] - tmpdf['reserve_visitor_tick_sum_%s' % visitor_ticks[i - 1]]) / (visitor_ticks[i] - visitor_ticks[i - 1])\n",
    "        tmpdf.loc[tmpdf[k_mean] < 0, k_mean] = -1  ## negative values exists, need to be set zero, updated 2016/12/22 20:30\n",
    "        join_cols.append(k_mean)\n",
    "        rolling_reserve_visitors_cols.append(k_mean)\n",
    "    tmpdf.drop(['visitor_tick_sum_%s' % col for col in visitor_ticks], axis= 1, inplace= True)\n",
    "    tmpdf.drop(['reserve_visitor_tick_sum_%s' % col for col in visitor_ticks], axis= 1, inplace= True)\n",
    "    #### !!! updated 2018/1/16 22:55\n",
    "    ## gap for rolling_visitors/rolling_reserve_visitors respectively, grasp variance of rolling features\n",
    "    for i in range(len(rolling_visitors_cols) - 1):\n",
    "        rolling_gap = '%s_%s_var' % (rolling_visitors_cols[i], rolling_visitors_cols[i + 1])\n",
    "        smooth_val = tmpdf[rolling_visitors_cols[i + 1]] + 1\n",
    "        tmpdf[rolling_gap] = (tmpdf[rolling_visitors_cols[i]] - tmpdf[rolling_visitors_cols[i + 1]])/smooth_val\n",
    "        join_cols.append(rolling_gap)\n",
    "    for i in range(len(rolling_reserve_visitors_cols) - 1):\n",
    "        rolling_gap = '%s_%s_var' % (rolling_reserve_visitors_cols[i], rolling_reserve_visitors_cols[i + 1])\n",
    "        smooth_val = tmpdf[rolling_reserve_visitors_cols[i + 1]] + 1\n",
    "        tmpdf[rolling_gap] = (tmpdf[rolling_reserve_visitors_cols[i]] - tmpdf[rolling_reserve_visitors_cols[i + 1]])/smooth_val\n",
    "        join_cols.append(rolling_gap)\n",
    "    ## difference between rolling_visitors and rolling_reserve_visitors, grasp difference between rolling visitors and reserve visitors\n",
    "    rolling_diff_cols = []\n",
    "    for i in range(len(rolling_visitors_cols)):\n",
    "        rolling_diff = '%s_%s_diff' % (rolling_visitors_cols[i], rolling_reserve_visitors_cols[i])\n",
    "        smooth_val = tmpdf[rolling_reserve_visitors_cols[i]] + 1\n",
    "        tmpdf[rolling_diff] = (tmpdf[rolling_visitors_cols[i]] - tmpdf[rolling_reserve_visitors_cols[i]])/smooth_val\n",
    "        join_cols.append(rolling_diff)\n",
    "        rolling_diff_cols.append(rolling_diff)\n",
    "    ## gap for rolling_visitors_reserve_visitors_gap, grasp variance of rolling diff\n",
    "    for i in range(len(rolling_diff_cols) - 1):\n",
    "        rolling_diff_gap = '%s_%s_var' % (rolling_diff_cols[i], rolling_diff_cols[i + 1])\n",
    "        smooth_val = tmpdf[rolling_diff_cols[i + 1]] + 1\n",
    "        tmpdf[rolling_diff_gap] = (tmpdf[rolling_diff_cols[i]] - tmpdf[rolling_diff_cols[i + 1]])/smooth_val\n",
    "        join_cols.append(rolling_diff_gap)\n",
    "    DataParts[pidx] = DataParts[pidx].merge(tmpdf[join_cols], how= 'left', on= ['air_store_id', 'visit_date'])\n",
    "    print('part %s rolling done.' % pidx)\n",
    "# concat after all is done\n",
    "AllData = pd.concat([DataParts['0'], DataParts['1']], axis= 0, ignore_index= True)\n",
    "# restore\n",
    "DataSet['tra'] = AllData[AllData['is_train'] == 1]\n",
    "DataSet['tes'] = AllData[AllData['is_train'] == 0]\n",
    "DataSet['tra'].drop(['is_train'], axis= 1, inplace= True)\n",
    "DataSet['tes'].drop(['is_train'], axis= 1, inplace= True)\n",
    "del AllData\n",
    "gc.collect()\n",
    "end = time.time()\n",
    "print('======== add time series features done. time elapsed %s' % (end - start))\n",
    "\n",
    "#### add date_int\n",
    "for mod in ['tra', 'tes']:\n",
    "    DataSet[mod]['date_int'] = DataSet[mod]['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "    DataSet[mod]['date_int'] = DataSet[mod]['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "print('add date int features done.')\n",
    "### add var_max_lat/var_max_long\n",
    "for mod in ['tra', 'tes']:\n",
    "    DataSet[mod]['lon_plus_lat_x'] = DataSet[mod]['longitude_x'] + DataSet[mod]['latitude_x'] \n",
    "    DataSet[mod]['var_max_long_x'] = DataSet[mod]['longitude_x'].max() - DataSet[mod]['longitude_x']\n",
    "    DataSet[mod]['var_max_lat_x'] = DataSet[mod]['latitude_x'].max() - DataSet[mod]['latitude_x']\n",
    "e = time.time()\n",
    "print(' ============= add time series related features done. time elapsed %s' % (end - start))\n",
    "\n",
    "## !!! add corner tag since 2016/7/1 is a corner point, update time 2017/12/22 15:45\n",
    "for mod in ['tra', 'tes']:\n",
    "    DataSet[mod]['is_up_corner'] = DataSet[mod]['visit_date'] < datetime.date(2016, 7, 1)\n",
    "    DataSet[mod]['is_up_corner'] = DataSet[mod]['is_up_corner'].astype(int)\n",
    "print('============ add is_up_corner feature done.')\n",
    "\n",
    "## interactions\n",
    "pairs = [('air_area_genre_store_count', 'air_area_store_count'), \n",
    "         ('air_city_genre_store_count', 'air_city_store_count'), \n",
    "         ('air_genre_store_count', 'air_area_genre_store_count'), \n",
    "         ('air_genre_store_count', 'air_city_genre_store_count'), \n",
    "         ('air_genre_store_count', 'air_city_store_count'), \n",
    "         ('air_genre_store_count', 'air_area_store_count'), \n",
    "         ('air_area_store_count', 'air_city_store_count'), \n",
    "         ('hpg_area_genre_store_count', 'hpg_area_store_count'), \n",
    "         ('hpg_city_genre_store_count', 'hpg_city_store_count'), \n",
    "         ('hpg_genre_store_count', 'hpg_city_genre_store_count'), \n",
    "         ('hpg_genre_store_count', 'hpg_area_genre_store_count')]\n",
    "for mod in ['tra', 'tes']:\n",
    "    for pair in pairs:\n",
    "        DataSet[mod]['inter_%s_%s_multiply' % (pair[0], pair[1])] = DataSet[mod][pair[0]] * DataSet[mod][pair[1]]\n",
    "        DataSet[mod]['inter_%s_%s_divide' % (pair[0], pair[1])] = DataSet[mod][pair[0]] // (1 + DataSet[mod][pair[1]])\n",
    "pairs = [('air_area_genre_store_count', 'hpg_area_genre_store_count'), \n",
    "         ('air_area_store_count', 'hpg_area_store_count'), \n",
    "         ('air_city_genre_store_count', 'hpg_city_genre_store_count'), \n",
    "         ('air_city_store_count', 'hpg_city_store_count'), \n",
    "         ('air_genre_store_count', 'hpg_genre_store_count')]\n",
    "for mod in ['tra', 'tes']:\n",
    "    for pair in pairs:\n",
    "        DataSet[mod]['inter_%s_%s_plus' % (pair[0], pair[1])] = DataSet[mod][pair[0]] + DataSet[mod][pair[1]]\n",
    "        DataSet[mod]['inter_%s_%s_divide' % (pair[0], pair[1])] = DataSet[mod][pair[0]] // (1 + DataSet[mod][pair[1]])\n",
    "end = time.time()\n",
    "print('=========== add interaction count features done. time elapsed %s.' % (end - start))\n",
    "###  remove outliers\n",
    "# data_dfs = []\n",
    "# astores = DataSet['tra']['air_store_id'].unique()\n",
    "# print('before removing outliers, size %s, unique stores %s' % (len(DataSet['tra']), len(astores)))\n",
    "# low = .00\n",
    "# high = .99\n",
    "# delete_num = []\n",
    "# groupped = DataSet['tra'].groupby(['air_store_id', 'dow', 'is_up_corner'])\n",
    "# for g in groupped.groups:\n",
    "#     gdata = groupped.get_group(g)\n",
    "#     n1 = len(gdata)\n",
    "#     filt_df = gdata[['visitors']]\n",
    "#     quant_df = filt_df.quantile([low, high])\n",
    "#     filt_df = filt_df.apply(lambda x: x[(x>= quant_df.loc[low,x.name]) & (x <= quant_df.loc[high,x.name])], axis=0)\n",
    "#     gdata = pd.concat([gdata.loc[:,['air_store_id', 'visit_date']], filt_df], axis=1)\n",
    "#     gdata.dropna(inplace=True)\n",
    "#     data_dfs.append(gdata)\n",
    "#     n2 = len(gdata)\n",
    "#     delete_num.append(n1 - n2)\n",
    "# DataSet['tra'] = pd.concat(data_dfs, axis= 0, ignore_index= True)\n",
    "# print(DataSet['tra']['visitors'].isnull().sum())\n",
    "# bstores = DataSet['tra']['air_store_id'].unique()\n",
    "# print('delete mean %s, sum %s' % (np.mean(delete_num), np.sum(delete_num)))\n",
    "# print('after removing outliers, size %s, unique stores %s' % (len(DataSet['tra']), len(bstores)))\n",
    "# \n",
    "# def calc_shifted_ewm(series, alpha, adjust=True):\n",
    "#     return series.shift().ewm(alpha=alpha, adjust=adjust).mean()\n",
    "# \n",
    "# for mod in ['tra', 'tes']:\n",
    "#     DataSet[mod]['ewm'] = DataSet[mod].groupby(['air_store_id', 'is_up_corner', 'dow']).apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1)).sort_index(level=['air_store_id', 'is_up_corner', 'dow']).values\n",
    "# \n",
    "for c in ['day', 'pom', 'wom', 'woy', 'prev_is_holiday', 'next_is_holiday']:\n",
    "    print(c, len(DataSet['tra'][c].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T17:03:50.717296Z",
     "start_time": "2018-01-16T17:03:49.608009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filling missings done.\n",
      "air_area_genre_store_count                                            0\n",
      "air_area_name                                                         0\n",
      "air_area_store_count                                                  0\n",
      "air_city                                                              0\n",
      "air_city_genre_store_count                                            0\n",
      "air_city_store_count                                                  0\n",
      "air_genre_name                                                        0\n",
      "air_genre_store_count                                                 0\n",
      "air_reserved_visitors                                                 0\n",
      "air_store_id                                                          0\n",
      "day                                                                   0\n",
      "dow                                                                   0\n",
      "hol_days                                                              0\n",
      "holiday_flg                                                           0\n",
      "hpg_area_genre_store_count                                            0\n",
      "hpg_area_name                                                         0\n",
      "hpg_area_store_count                                                  0\n",
      "hpg_city                                                              0\n",
      "hpg_city_genre_store_count                                            0\n",
      "hpg_city_store_count                                                  0\n",
      "hpg_genre_name                                                        0\n",
      "hpg_genre_store_count                                                 0\n",
      "hpg_reserved_visitors                                                 0\n",
      "hpg_store_id                                                          0\n",
      "id                                                                    0\n",
      "is_weekends                                                           0\n",
      "latitude_x                                                            0\n",
      "latitude_y                                                            0\n",
      "longitude_x                                                           0\n",
      "longitude_y                                                           0\n",
      "                                                                     ..\n",
      "inter_air_genre_store_count_air_area_genre_store_count_multiply       0\n",
      "inter_air_genre_store_count_air_area_genre_store_count_divide         0\n",
      "inter_air_genre_store_count_air_city_genre_store_count_multiply       0\n",
      "inter_air_genre_store_count_air_city_genre_store_count_divide         0\n",
      "inter_air_genre_store_count_air_city_store_count_multiply             0\n",
      "inter_air_genre_store_count_air_city_store_count_divide               0\n",
      "inter_air_genre_store_count_air_area_store_count_multiply             0\n",
      "inter_air_genre_store_count_air_area_store_count_divide               0\n",
      "inter_air_area_store_count_air_city_store_count_multiply              0\n",
      "inter_air_area_store_count_air_city_store_count_divide                0\n",
      "inter_hpg_area_genre_store_count_hpg_area_store_count_multiply        0\n",
      "inter_hpg_area_genre_store_count_hpg_area_store_count_divide          0\n",
      "inter_hpg_city_genre_store_count_hpg_city_store_count_multiply        0\n",
      "inter_hpg_city_genre_store_count_hpg_city_store_count_divide          0\n",
      "inter_hpg_genre_store_count_hpg_city_genre_store_count_multiply       0\n",
      "inter_hpg_genre_store_count_hpg_city_genre_store_count_divide         0\n",
      "inter_hpg_genre_store_count_hpg_area_genre_store_count_multiply       0\n",
      "inter_hpg_genre_store_count_hpg_area_genre_store_count_divide         0\n",
      "inter_air_area_genre_store_count_hpg_area_genre_store_count_plus      0\n",
      "inter_air_area_genre_store_count_hpg_area_genre_store_count_divide    0\n",
      "inter_air_area_store_count_hpg_area_store_count_plus                  0\n",
      "inter_air_area_store_count_hpg_area_store_count_divide                0\n",
      "inter_air_city_genre_store_count_hpg_city_genre_store_count_plus      0\n",
      "inter_air_city_genre_store_count_hpg_city_genre_store_count_divide    0\n",
      "inter_air_city_store_count_hpg_city_store_count_plus                  0\n",
      "inter_air_city_store_count_hpg_city_store_count_divide                0\n",
      "inter_air_genre_store_count_hpg_genre_store_count_plus                0\n",
      "inter_air_genre_store_count_hpg_genre_store_count_divide              0\n",
      "air_store_id_encoded                                                  0\n",
      "hpg_store_id_encoded                                                  0\n",
      "Length: 105, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### fill nulls, updated 2016/12/26 14:58\n",
    "# from fancyimpute.knn import KNN\n",
    "# from fancyimpute import MatrixFactorization\n",
    "# import time\n",
    "# \n",
    "# knn_filled_cols = ['latitude_y', 'longitude_y']\n",
    "# start = time.time()\n",
    "# for mod in ['tra', 'tes']:\n",
    "#     solver = MatrixFactorization(\n",
    "#         learning_rate=0.01,\n",
    "#         rank=3,\n",
    "#         l2_penalty=0,\n",
    "#         min_improvement=1e-6)\n",
    "#     DataSet[mod][knn_filled_cols] = solver.complete(DataSet[mod][knn_filled_cols])\n",
    "#     end = time.time()\n",
    "#     print('%s done. time elapsed %.2fs' % (mod, (end - start)))\n",
    "# print(DataSet['tra'].isnull().sum())\n",
    "# end = time.time()\n",
    "# print('Fill missings with MF done, time elapsed %.2fs' % (end - start))\n",
    "\n",
    "from sklearn import *\n",
    "cate_feats = ['genre_name', 'area_name', 'city']\n",
    "cate_cols = ['%s_%s' % (m, cf) for m in ['air', 'hpg'] for cf in cate_feats]\n",
    "DataSet['tra']['air_store_id_encoded'] = DataSet['tra']['air_store_id']\n",
    "DataSet['tra']['hpg_store_id_encoded'] = DataSet['tra']['hpg_store_id']\n",
    "DataSet['tes']['air_store_id_encoded'] = DataSet['tes']['air_store_id']\n",
    "DataSet['tes']['hpg_store_id_encoded'] = DataSet['tes']['hpg_store_id']\n",
    "cate_cols.extend(['air_store_id_encoded', 'hpg_store_id_encoded', 'pom'])#, 'day', 'wom', 'woy'])\n",
    "for mod in ['tra', 'tes']:\n",
    "    for col in DataSet[mod].columns:\n",
    "        if(col in cate_cols):\n",
    "            DataSet[mod][col].fillna('unknown', inplace= True)\n",
    "        elif(col == 'latitude_y'):\n",
    "            DataSet[mod][col].fillna(DataSet[mod]['latitude_x'], inplace= True)\n",
    "        elif(col == 'longitude_y'):\n",
    "            DataSet[mod][col].fillna(DataSet[mod]['longitude_x'], inplace= True)\n",
    "        else:\n",
    "            DataSet[mod][col].fillna(0, inplace= True)\n",
    "print('filling missings done.')\n",
    "\n",
    "print(DataSet['tra'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T17:03:50.726537Z",
     "start_time": "2018-01-16T17:03:50.719308Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### transformat skewed features\n",
    "# from scipy.stats import norm, skew\n",
    "# from scipy.special import boxcox1p\n",
    "\n",
    "# drop_cols = ['id', 'air_store_id', 'visit_date', 'visitors', 'hpg_store_id', \n",
    "#              'is_train', 'hol_days', 'holiday_flg', 'is_weekends', 'latitude_x', \n",
    "#              'latitude_y']\n",
    "\n",
    "# DataSet['tra']['is_train'] = 1\n",
    "# DataSet['tes']['is_train'] = 0\n",
    "# all_cols = DataSet['tra'].columns\n",
    "# all_data = pd.concat([DataSet['tra'], DataSet['tes'][all_cols]], axis= 0)\n",
    "# tmp_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "# numeric_feats = [col for col in tmp_feats if col not in drop_cols]\n",
    "# # Check the skew of all numerical features\n",
    "# skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "# print(\"\\nSkew in numerical features: \\n\")\n",
    "# skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "# print(skewness)\n",
    "\n",
    "# skewness = skewness[abs(skewness) > 0.75]\n",
    "# print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "# skewed_features = skewness.index\n",
    "# lam = 0.15\n",
    "# for feat in skewed_features:\n",
    "#     all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "# DataSet['tra'] = all_data[all_data['is_train'] == 1]\n",
    "# DataSet['tra'].drop(['is_train'], axis= 1, inplace= True)\n",
    "# DataSet['test'] = all_data[all_data['is_train'] == 0]\n",
    "# DataSet['tes'].drop(['is_train'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T17:05:39.451651Z",
     "start_time": "2018-01-16T17:03:50.728500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_genre_name\n",
      "air_area_name\n",
      "air_city\n",
      "hpg_genre_name\n",
      "hpg_area_name\n",
      "hpg_city\n",
      "air_store_id_encoded\n",
      "hpg_store_id_encoded\n",
      "pom\n",
      "encoding for categorial features done.\n",
      "air_area_genre_store_count\n",
      "air_area_name\n",
      "air_area_store_count\n",
      "air_city\n",
      "air_city_genre_store_count\n",
      "air_city_store_count\n",
      "air_genre_name\n",
      "air_genre_store_count\n",
      "air_reserved_visitors\n",
      "air_store_id\n",
      "day\n",
      "dow\n",
      "hol_days\n",
      "holiday_flg\n",
      "hpg_area_genre_store_count\n",
      "hpg_area_name\n",
      "hpg_area_store_count\n",
      "hpg_city\n",
      "hpg_city_genre_store_count\n",
      "hpg_city_store_count\n",
      "hpg_genre_name\n",
      "hpg_genre_store_count\n",
      "hpg_reserved_visitors\n",
      "hpg_store_id\n",
      "id\n",
      "is_weekends\n",
      "latitude_x\n",
      "latitude_y\n",
      "longitude_x\n",
      "longitude_y\n",
      "month\n",
      "next_is_holiday\n",
      "pom\n",
      "prev_is_holiday\n",
      "reserved_visitors\n",
      "visit_date\n",
      "visitors\n",
      "wom\n",
      "woy\n",
      "rolling_visitors_46_39\n",
      "rolling_reserve_visitors_46_39\n",
      "rolling_visitors_53_46\n",
      "rolling_reserve_visitors_53_46\n",
      "rolling_visitors_60_53\n",
      "rolling_reserve_visitors_60_53\n",
      "rolling_visitors_67_60\n",
      "rolling_reserve_visitors_67_60\n",
      "rolling_visitors_74_67\n",
      "rolling_reserve_visitors_74_67\n",
      "rolling_visitors_46_39_rolling_visitors_53_46_var\n",
      "rolling_visitors_53_46_rolling_visitors_60_53_var\n",
      "rolling_visitors_60_53_rolling_visitors_67_60_var\n",
      "rolling_visitors_67_60_rolling_visitors_74_67_var\n",
      "rolling_reserve_visitors_46_39_rolling_reserve_visitors_53_46_var\n",
      "rolling_reserve_visitors_53_46_rolling_reserve_visitors_60_53_var\n",
      "rolling_reserve_visitors_60_53_rolling_reserve_visitors_67_60_var\n",
      "rolling_reserve_visitors_67_60_rolling_reserve_visitors_74_67_var\n",
      "rolling_visitors_46_39_rolling_reserve_visitors_46_39_diff\n",
      "rolling_visitors_53_46_rolling_reserve_visitors_53_46_diff\n",
      "rolling_visitors_60_53_rolling_reserve_visitors_60_53_diff\n",
      "rolling_visitors_67_60_rolling_reserve_visitors_67_60_diff\n",
      "rolling_visitors_74_67_rolling_reserve_visitors_74_67_diff\n",
      "rolling_visitors_46_39_rolling_reserve_visitors_46_39_diff_rolling_visitors_53_46_rolling_reserve_visitors_53_46_diff_var\n",
      "rolling_visitors_53_46_rolling_reserve_visitors_53_46_diff_rolling_visitors_60_53_rolling_reserve_visitors_60_53_diff_var\n",
      "rolling_visitors_60_53_rolling_reserve_visitors_60_53_diff_rolling_visitors_67_60_rolling_reserve_visitors_67_60_diff_var\n",
      "rolling_visitors_67_60_rolling_reserve_visitors_67_60_diff_rolling_visitors_74_67_rolling_reserve_visitors_74_67_diff_var\n",
      "date_int\n",
      "lon_plus_lat_x\n",
      "var_max_long_x\n",
      "var_max_lat_x\n",
      "is_up_corner\n",
      "inter_air_area_genre_store_count_air_area_store_count_multiply\n",
      "inter_air_area_genre_store_count_air_area_store_count_divide\n",
      "inter_air_city_genre_store_count_air_city_store_count_multiply\n",
      "inter_air_city_genre_store_count_air_city_store_count_divide\n",
      "inter_air_genre_store_count_air_area_genre_store_count_multiply\n",
      "inter_air_genre_store_count_air_area_genre_store_count_divide\n",
      "inter_air_genre_store_count_air_city_genre_store_count_multiply\n",
      "inter_air_genre_store_count_air_city_genre_store_count_divide\n",
      "inter_air_genre_store_count_air_city_store_count_multiply\n",
      "inter_air_genre_store_count_air_city_store_count_divide\n",
      "inter_air_genre_store_count_air_area_store_count_multiply\n",
      "inter_air_genre_store_count_air_area_store_count_divide\n",
      "inter_air_area_store_count_air_city_store_count_multiply\n",
      "inter_air_area_store_count_air_city_store_count_divide\n",
      "inter_hpg_area_genre_store_count_hpg_area_store_count_multiply\n",
      "inter_hpg_area_genre_store_count_hpg_area_store_count_divide\n",
      "inter_hpg_city_genre_store_count_hpg_city_store_count_multiply\n",
      "inter_hpg_city_genre_store_count_hpg_city_store_count_divide\n",
      "inter_hpg_genre_store_count_hpg_city_genre_store_count_multiply\n",
      "inter_hpg_genre_store_count_hpg_city_genre_store_count_divide\n",
      "inter_hpg_genre_store_count_hpg_area_genre_store_count_multiply\n",
      "inter_hpg_genre_store_count_hpg_area_genre_store_count_divide\n",
      "inter_air_area_genre_store_count_hpg_area_genre_store_count_plus\n",
      "inter_air_area_genre_store_count_hpg_area_genre_store_count_divide\n",
      "inter_air_area_store_count_hpg_area_store_count_plus\n",
      "inter_air_area_store_count_hpg_area_store_count_divide\n",
      "inter_air_city_genre_store_count_hpg_city_genre_store_count_plus\n",
      "inter_air_city_genre_store_count_hpg_city_genre_store_count_divide\n",
      "inter_air_city_store_count_hpg_city_store_count_plus\n",
      "inter_air_city_store_count_hpg_city_store_count_divide\n",
      "inter_air_genre_store_count_hpg_genre_store_count_plus\n",
      "inter_air_genre_store_count_hpg_genre_store_count_divide\n",
      "air_store_id_encoded\n",
      "hpg_store_id_encoded\n",
      "air_store_id_dow_is_up_corner_visitors_mean\n",
      "air_store_id_dow_is_up_corner_visitors_median\n",
      "air_store_id_dow_is_up_corner_visitors_max\n",
      "air_store_id_dow_is_up_corner_visitors_min\n",
      "air_store_id_dow_is_up_corner_visitors_count\n",
      "air_store_id_dow_is_up_corner_reserve_visitors_mean\n",
      "air_store_id_dow_is_up_corner_reserve_visitors_median\n",
      "air_store_id_dow_is_up_corner_reserve_visitors_max\n",
      "air_store_id_dow_is_up_corner_reserve_visitors_min\n",
      "air_store_id_dow_is_up_corner_reserve_visitors_count\n",
      "air_store_id_dow_is_up_corner_visitors_mean_air_store_id_dow_is_up_corner_reserve_visitors_mean_diff\n",
      "air_store_id_dow_is_up_corner_visitors_median_air_store_id_dow_is_up_corner_reserve_visitors_median_diff\n",
      "air_store_id_dow_is_up_corner_visitors_max_air_store_id_dow_is_up_corner_reserve_visitors_max_diff\n",
      "air_store_id_dow_is_up_corner_visitors_min_air_store_id_dow_is_up_corner_reserve_visitors_min_diff\n",
      "air_store_id_dow_is_up_corner_visitors_count_air_store_id_dow_is_up_corner_reserve_visitors_count_diff\n",
      "air_store_id_is_weekends_is_up_corner_visitors_mean\n",
      "air_store_id_is_weekends_is_up_corner_visitors_median\n",
      "air_store_id_is_weekends_is_up_corner_visitors_max\n",
      "air_store_id_is_weekends_is_up_corner_visitors_min\n",
      "air_store_id_is_weekends_is_up_corner_visitors_count\n",
      "air_store_id_is_weekends_is_up_corner_reserve_visitors_mean\n",
      "air_store_id_is_weekends_is_up_corner_reserve_visitors_median\n",
      "air_store_id_is_weekends_is_up_corner_reserve_visitors_max\n",
      "air_store_id_is_weekends_is_up_corner_reserve_visitors_min\n",
      "air_store_id_is_weekends_is_up_corner_reserve_visitors_count\n",
      "air_store_id_is_weekends_is_up_corner_visitors_mean_air_store_id_is_weekends_is_up_corner_reserve_visitors_mean_diff\n",
      "air_store_id_is_weekends_is_up_corner_visitors_median_air_store_id_is_weekends_is_up_corner_reserve_visitors_median_diff\n",
      "air_store_id_is_weekends_is_up_corner_visitors_max_air_store_id_is_weekends_is_up_corner_reserve_visitors_max_diff\n",
      "air_store_id_is_weekends_is_up_corner_visitors_min_air_store_id_is_weekends_is_up_corner_reserve_visitors_min_diff\n",
      "air_store_id_is_weekends_is_up_corner_visitors_count_air_store_id_is_weekends_is_up_corner_reserve_visitors_count_diff\n",
      "air_store_id_hol_days_is_up_corner_visitors_mean\n",
      "air_store_id_hol_days_is_up_corner_visitors_median\n",
      "air_store_id_hol_days_is_up_corner_visitors_max\n",
      "air_store_id_hol_days_is_up_corner_visitors_min\n",
      "air_store_id_hol_days_is_up_corner_visitors_count\n",
      "air_store_id_hol_days_is_up_corner_reserve_visitors_mean\n",
      "air_store_id_hol_days_is_up_corner_reserve_visitors_median\n",
      "air_store_id_hol_days_is_up_corner_reserve_visitors_max\n",
      "air_store_id_hol_days_is_up_corner_reserve_visitors_min\n",
      "air_store_id_hol_days_is_up_corner_reserve_visitors_count\n",
      "air_store_id_hol_days_is_up_corner_visitors_mean_air_store_id_hol_days_is_up_corner_reserve_visitors_mean_diff\n",
      "air_store_id_hol_days_is_up_corner_visitors_median_air_store_id_hol_days_is_up_corner_reserve_visitors_median_diff\n",
      "air_store_id_hol_days_is_up_corner_visitors_max_air_store_id_hol_days_is_up_corner_reserve_visitors_max_diff\n",
      "air_store_id_hol_days_is_up_corner_visitors_min_air_store_id_hol_days_is_up_corner_reserve_visitors_min_diff\n",
      "air_store_id_hol_days_is_up_corner_visitors_count_air_store_id_hol_days_is_up_corner_reserve_visitors_count_diff\n",
      "----------------------------------\n",
      "Fold 0 done.\n",
      "Fold 1 done.\n",
      "Fold 2 done.\n",
      "Fold 3 done.\n",
      "Fold 4 done.\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "from sklearn import model_selection\n",
    "\n",
    "pca_factor = 10\n",
    "\n",
    "def PCAFitTransform(data):\n",
    "    n = len(data)\n",
    "    rec_dict = {}\n",
    "    for i in data.index:\n",
    "        rid = data.loc[i, 'air_store_id']\n",
    "        vdate = str(data.loc[i, 'visit_date'])\n",
    "        visitor = data.loc[i, 'visitors']\n",
    "        if(rid not in rec_dict):\n",
    "            rec_dict[rid] = {}\n",
    "        rec_dict[rid][vdate] = visitor\n",
    "    records = []\n",
    "    for r in rec_dict:\n",
    "        rec = {'air_store_id': r}\n",
    "        for d in rec_dict[r]:\n",
    "            rec[d] = rec_dict[r][d]\n",
    "        records.append(rec)\n",
    "    tmpdf = pd.DataFrame(data= records, index= range(len(records)))    \n",
    "    #tmpdf = tmpdf.dropna(axis= 1, how= 'any')\n",
    "    tmpdf.fillna(0, inplace= True)\n",
    "    pca= decomposition.PCA(n_components= pca_factor)\n",
    "    pca_cols = [c for c in tmpdf.columns if(c != 'air_store_id')]\n",
    "    transformed = pca.fit_transform(tmpdf[pca_cols])\n",
    "    pcadf = pd.DataFrame(data= transformed, index= range(len(transformed)), columns= ['pca_%s' % i for i in range(pca_factor)])\n",
    "    pcadf['air_store_id'] = tmpdf['air_store_id']\n",
    "    return pcadf\n",
    "    \n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred) ** 0.5\n",
    "\n",
    "#### Label encoding for categorial features\n",
    "TrainData = DataSet['tra']\n",
    "TestData = DataSet['tes']\n",
    "for col in cate_cols:\n",
    "    print(col)\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    TrainData[col] = lbl.fit_transform(TrainData[col])\n",
    "    TestData[col] = lbl.transform(TestData[col])\n",
    "    \n",
    "print('encoding for categorial features done.')\n",
    "\n",
    "# split TrainData into train and holdout with random strategy\n",
    "np.random.seed(2017)\n",
    "msk = np.random.rand(len(TrainData)) < 0.1\n",
    "holdout = TrainData[msk]\n",
    "train = TrainData[~msk]\n",
    "test = TestData\n",
    "# Set up folds\n",
    "K = 5\n",
    "low = 0.001\n",
    "high = 0.999\n",
    "kf = model_selection.KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(1)\n",
    "OutputDir = '../../data/l0'\n",
    "if(os.path.exists('%s/kfold' % OutputDir) == False):\n",
    "    os.makedirs('%s/kfold' % OutputDir)\n",
    "#cate_cols.append('air_store_id')\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "    FoldTrain, FoldValid = train.iloc[train_index].copy(), train.iloc[test_index].copy()\n",
    "    FoldHoldout = holdout.copy()\n",
    "    FoldTest = test.copy()\n",
    "    \n",
    "    FoldTrain['visitors'] = np.log1p(FoldTrain['visitors'])\n",
    "    FoldValid['visitors'] = np.log1p(FoldValid['visitors'])\n",
    "    FoldHoldout['visitors'] = np.log1p(FoldHoldout['visitors'])\n",
    "    FoldTest['visitors'] = np.log1p(FoldTest['visitors'])\n",
    "    \n",
    "    FoldTrain['air_reserved_visitors'] = np.log1p(FoldTrain['air_reserved_visitors'])\n",
    "    FoldValid['air_reserved_visitors'] = np.log1p(FoldValid['air_reserved_visitors'])\n",
    "    FoldHoldout['air_reserved_visitors'] = np.log1p(FoldHoldout['air_reserved_visitors'])\n",
    "    FoldTest['air_reserved_visitors'] = np.log1p(FoldTest['air_reserved_visitors'])\n",
    "    #### dependent features which is extreemly subtle to data-leak\n",
    "#     # pca transform\n",
    "#     pca_cols = ['air_store_id', 'visit_date', 'visitors']\n",
    "#     sd = datetime.date(2016, 4, 23)\n",
    "#     ed = datetime.date(2016, 5, 31)\n",
    "#     pcadf = PCAFitTransform(FoldTrain[(FoldTrain['visit_date'] >= sd) & (FoldTrain['visit_date'] <= ed)][pca_cols])\n",
    "#     FoldValid = FoldValid.merge(pcadf, how= 'left', on= ['air_store_id'])\n",
    "#     FoldHoldout = FoldHoldout.merge(pcadf, how= 'left', on= ['air_store_id'])\n",
    "#     FoldTest = FoldTest.merge(pcadf, how= 'left', on= ['air_store_id'])\n",
    "    # percentiles features\n",
    "    agg_visitors_cols = []\n",
    "    agg_reserve_visitors_cols = []\n",
    "    tickles = ['mean', 'median', 'max', 'min', 'count']\n",
    "    for feat in ['air_store_id']:\n",
    "        gkeys = [feat, 'dow', 'is_up_corner']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'visitors': tickles})\n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        agg_visitors_cols.extend(['%s_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### data-leak, prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(0, inplace= True)\n",
    "        FoldHoldout.fillna(0, inplace= True)\n",
    "        FoldTest.fillna(0, inplace= True)\n",
    "    for feat in ['air_store_id']:\n",
    "        gkeys = [feat, 'dow', 'is_up_corner']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'air_reserved_visitors': tickles})\n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_reserve_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        agg_reserve_visitors_cols.extend(['%s_reserve_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### data-leak, prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(0, inplace= True)\n",
    "        FoldHoldout.fillna(0, inplace= True)\n",
    "        FoldTest.fillna(0, inplace= True)\n",
    "    for idx in range(len(agg_visitors_cols)):\n",
    "        agg_var = '%s_%s_diff' % (agg_visitors_cols[idx], agg_reserve_visitors_cols[idx])\n",
    "        FoldValid[agg_var] = (FoldValid[agg_visitors_cols[idx]] - FoldValid[agg_reserve_visitors_cols[idx]])/(FoldValid[agg_reserve_visitors_cols[idx]] + 1)\n",
    "        FoldHoldout[agg_var] = (FoldHoldout[agg_visitors_cols[idx]] - FoldHoldout[agg_reserve_visitors_cols[idx]])/(FoldHoldout[agg_reserve_visitors_cols[idx]] + 1)\n",
    "        FoldTest[agg_var] = (FoldTest[agg_visitors_cols[idx]] - FoldTest[agg_reserve_visitors_cols[idx]])/(FoldTest[agg_reserve_visitors_cols[idx]] + 1)\n",
    "    agg_visitors_cols = []\n",
    "    agg_reserve_visitors_cols = []\n",
    "    for feat in ['air_store_id']:\n",
    "        gkeys = [feat, 'is_weekends', 'is_up_corner']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'visitors': tickles})\n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        agg_visitors_cols.extend(['%s_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### data-leak, prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(0, inplace= True)\n",
    "        FoldHoldout.fillna(0, inplace= True)\n",
    "        FoldTest.fillna(0, inplace= True)\n",
    "    for feat in ['air_store_id']:\n",
    "        gkeys = [feat, 'is_weekends', 'is_up_corner']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'air_reserved_visitors': tickles})\n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_reserve_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        agg_reserve_visitors_cols.extend(['%s_reserve_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### data-leak, prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(0, inplace= True)\n",
    "        FoldHoldout.fillna(0, inplace= True)\n",
    "        FoldTest.fillna(0, inplace= True)\n",
    "    for idx in range(len(agg_visitors_cols)):\n",
    "        agg_var = '%s_%s_diff' % (agg_visitors_cols[idx], agg_reserve_visitors_cols[idx])\n",
    "        FoldValid[agg_var] = (FoldValid[agg_visitors_cols[idx]] - FoldValid[agg_reserve_visitors_cols[idx]])/(FoldValid[agg_reserve_visitors_cols[idx]] + 1)\n",
    "        FoldHoldout[agg_var] = (FoldHoldout[agg_visitors_cols[idx]] - FoldHoldout[agg_reserve_visitors_cols[idx]])/(FoldHoldout[agg_reserve_visitors_cols[idx]] + 1)\n",
    "        FoldTest[agg_var] = (FoldTest[agg_visitors_cols[idx]] - FoldTest[agg_reserve_visitors_cols[idx]])/(FoldTest[agg_reserve_visitors_cols[idx]] + 1)\n",
    "    agg_visitors_cols = []\n",
    "    agg_reserve_visitors_cols = []\n",
    "    for feat in ['air_store_id']:\n",
    "        gkeys = [feat, 'hol_days', 'is_up_corner']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'visitors': tickles})\n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        agg_visitors_cols.extend(['%s_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### data-leak, prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(0, inplace= True)\n",
    "        FoldHoldout.fillna(0, inplace= True)\n",
    "        FoldTest.fillna(0, inplace= True)\n",
    "    for feat in ['air_store_id']:\n",
    "        gkeys = [feat, 'hol_days', 'is_up_corner']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'air_reserved_visitors': tickles})\n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_reserve_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        agg_reserve_visitors_cols.extend(['%s_reserve_visitors_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### data-leak, prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(0, inplace= True)\n",
    "        FoldHoldout.fillna(0, inplace= True)\n",
    "        FoldTest.fillna(0, inplace= True)\n",
    "    for idx in range(len(agg_visitors_cols)):\n",
    "        agg_var = '%s_%s_diff' % (agg_visitors_cols[idx], agg_reserve_visitors_cols[idx])\n",
    "        FoldValid[agg_var] = (FoldValid[agg_visitors_cols[idx]] - FoldValid[agg_reserve_visitors_cols[idx]])/(FoldValid[agg_reserve_visitors_cols[idx]] + 1)\n",
    "        FoldHoldout[agg_var] = (FoldHoldout[agg_visitors_cols[idx]] - FoldHoldout[agg_reserve_visitors_cols[idx]])/(FoldHoldout[agg_reserve_visitors_cols[idx]] + 1)\n",
    "        FoldTest[agg_var] = (FoldTest[agg_visitors_cols[idx]] - FoldTest[agg_reserve_visitors_cols[idx]])/(FoldTest[agg_reserve_visitors_cols[idx]] + 1)\n",
    "    if(i == 0):\n",
    "        for c in FoldValid.columns:\n",
    "            print(c)\n",
    "        print('----------------------------------')\n",
    "#         print(FoldValid.isnull().sum())\n",
    "    FoldOutputDir = '%s/kfold/%s' % (OutputDir, i)\n",
    "    if(os.path.exists(FoldOutputDir) == False):\n",
    "        os.makedirs(FoldOutputDir)\n",
    "    FoldValid.to_csv('%s/valid.csv' % FoldOutputDir, index= False)\n",
    "    FoldHoldout.to_csv('%s/holdout.csv' % FoldOutputDir, index= False)\n",
    "    FoldTest.to_csv('%s/test.csv' % FoldOutputDir, index= False)\n",
    "    print('Fold %s done.' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
