{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-19T12:40:24.875215Z",
     "start_time": "2017-12-19T12:39:47.563664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============\n",
      "tra data: unique stores 829, total 252108, time elased 1.45s.\n",
      "tes data: unique stores 821, total 32019, time elased 0.13s.\n",
      "============= process date related done.\n",
      "\n",
      " ============= process store data done.\n",
      "\n",
      " ============= process holiday data done.\n",
      "\n",
      "================ join holiday, store data done.\n",
      " ============= process reservation data done.\n",
      "\n",
      "============= join reservation data done.\n",
      "\n",
      "total groups 829 \n",
      "time elapsed 596.9517660140991\n",
      " ============= add rolling features done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:243: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:244: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numba\n",
    "import os,sys\n",
    "import gc\n",
    "\n",
    "def LoadData(InputDir):\n",
    "    \"\"\"\"\"\"\n",
    "    ## load raw data\n",
    "    data = {\n",
    "        'tra': pd.read_csv('%s/air_visit_data.csv' % InputDir, parse_dates= ['visit_date']),\n",
    "        'as': pd.read_csv('%s/air_store_info.csv' % InputDir),\n",
    "        'hs': pd.read_csv('%s/hpg_store_info.csv' % InputDir),\n",
    "        'ar': pd.read_csv('%s/air_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'hr': pd.read_csv('%s/hpg_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'id': pd.read_csv('%s/store_id_relation.csv' % InputDir),\n",
    "        'tes': pd.read_csv('%s/sample_submission.csv' % InputDir),\n",
    "        'hol': pd.read_csv('%s/date_info.csv' % InputDir, parse_dates=['calendar_date']).rename(columns={'calendar_date': 'visit_date'})\n",
    "    }\n",
    "    return data\n",
    "\n",
    "@numba.jit\n",
    "def ApplyDayoff(VisitCols, ReserveCols):\n",
    "    \"\"\"\"\"\"\n",
    "    n = len(VisitCols)\n",
    "    result = np.zeros((n, 1), dtype= 'int8')\n",
    "    for i in range(n):\n",
    "        result[i] = (VisitCols[i]- ReserveCols[i]).days\n",
    "    return result\n",
    "\n",
    "reserve2id = {'ar': 'air', 'hr': 'hpg'}\n",
    "reserve2store = {'ar': 'as', 'hr': 'hs'}# load data set\n",
    "InputDir = '../../data/raw'\n",
    "DataSet = LoadData(InputDir)\n",
    "#### \n",
    "# date related features\n",
    "print('\\n============')\n",
    "for mod in ['tra', 'tes']:\n",
    "    start0 = time.time()\n",
    "    if (mod == 'tes'):\n",
    "        DataSet[mod]['visit_date'] = DataSet[mod]['id'].map(lambda x: str(x).split('_')[2])\n",
    "        DataSet[mod]['air_store_id'] = DataSet[mod]['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "        DataSet[mod]['visit_date'] = pd.to_datetime(DataSet[mod]['visit_date'])\n",
    "    DataSet[mod]['dow'] = DataSet[mod]['visit_date'].dt.dayofweek\n",
    "    DataSet[mod]['year'] = DataSet[mod]['visit_date'].dt.year\n",
    "    DataSet[mod]['month'] = DataSet[mod]['visit_date'].dt.month\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_date'].dt.date\n",
    "    end0 = time.time()\n",
    "    print('%s data: unique stores %s, total %s, time elased %.2fs.' %\n",
    "            (mod, len(DataSet[mod]['air_store_id'].unique()), len(DataSet[mod]['air_store_id']), (end0 - start0)))\n",
    "print('============= process date related done.\\n')\n",
    "######## store data\n",
    "# add city feature\n",
    "for mod in ['ar', 'hr']:\n",
    "    DataSet[reserve2store[mod]]['%s_city' % reserve2id[mod]] = DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]].str[:5]\n",
    "# area (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_area_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g\n",
    "        ac['%s_area_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        #ac['%s_area_store_ratio' % reserve2id[mod]] = ac['%s_area_store_count' % reserve2id[mod]]/len(DataSet[reserve2store[mod]])\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_area_name' % reserve2id[mod]])\n",
    "# genre (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_genre_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g\n",
    "        ac['%s_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_genre_name' % reserve2id[mod]])\n",
    "#  area_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_area_name' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_area_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "# city (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_city' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g\n",
    "        ac['%s_city_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        #ac['%s_area_store_ratio' % reserve2id[mod]] = ac['%s_area_store_count' % reserve2id[mod]]/len(DataSet[reserve2store[mod]])\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_city' % reserve2id[mod]])\n",
    "#  city_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_city' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_city_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "print(' ============= process store data done.\\n')\n",
    "######### holiday data\n",
    "data = DataSet['hol']\n",
    "data['visit_date'] = data['visit_date'].dt.date\n",
    "data = data.sort_values(by= 'visit_date')\n",
    "def TagHoliday(df):\n",
    "    ''''''\n",
    "    n = len(df)\n",
    "    result = ['' for x in range(n)]\n",
    "    for i in range(n):\n",
    "        if(i == 0):\n",
    "            result[i] = 'hid_%s' % 0\n",
    "        elif((df[i] - df[i-1]).days == 1):\n",
    "            result[i] = result[i - 1]\n",
    "        else:\n",
    "            result[i] = 'hid_%s' % (int(result[i - 1].split('_')[1]) + 1)\n",
    "    return result\n",
    "holidays = data[data['holiday_flg'] == 1][['visit_date']]\n",
    "holidays['hol_l0'] = TagHoliday(holidays['visit_date'].values)\n",
    "groupped = holidays.groupby(['hol_l0'])\n",
    "recs = []\n",
    "for g in groupped.groups:\n",
    "    hol_days = {}\n",
    "    hol_days['hol_l0'] = g\n",
    "    hol_days['hol_days'] = len(groupped.get_group(g))\n",
    "    recs.append(hol_days)\n",
    "tmpdf = pd.DataFrame(data= recs, index= range(len(recs)))\n",
    "holidays = holidays.merge(tmpdf, how= 'left', on= 'hol_l0')\n",
    "data = data.merge(holidays, how= 'left', on= 'visit_date')\n",
    "data.drop(['hol_l0'], axis= 1, inplace= True)\n",
    "data['hol_days'].fillna(0, inplace= True)\n",
    "DataSet['hol'] = data\n",
    "print(' ============= process holiday data done.\\n')\n",
    "######## join \n",
    "# join holiday data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    data = data.merge(DataSet['hol'], how='left', on=['visit_date'])\n",
    "    data.drop(['day_of_week', 'year'], axis=1, inplace=True)\n",
    "    DataSet[mod] = data\n",
    "# join store data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[reserve2store[rtype]], how= 'left', on= ['%s_store_id' % reserve2id[rtype]])\n",
    "    DataSet[mod] = data\n",
    "print('================ join holiday, store data done.')\n",
    "######### reservation data\n",
    "for mod in ['hr', 'ar']:\n",
    "    start1 = time.time()\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_datetime'].dt.date\n",
    "    DataSet[mod]['reserve_date'] = DataSet[mod]['reserve_datetime'].dt.date\n",
    "    DataSet[mod].drop(['reserve_datetime', 'visit_datetime'], axis= 1, inplace= True)\n",
    "    tmpdf = pd.DataFrame(data=ApplyDayoff(DataSet[mod]['visit_date'].values, DataSet[mod]['reserve_date'].values),index=DataSet[mod].index, columns=['reserve_date_diff'])\n",
    "    tmpdf = pd.concat([DataSet[mod], tmpdf], axis=1)\n",
    "    tmpdf = tmpdf.groupby(['%s_store_id' % reserve2id[mod], 'visit_date'], as_index=False).agg({'reserve_visitors': sum, 'reserve_date_diff': ['mean', 'median']})\n",
    "    tmpdf.columns = ['%s_store_id' % reserve2id[mod], \n",
    "                   'visit_date', \n",
    "                   '%s_reserved_visitors' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_mean' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_median' % reserve2id[mod]\n",
    "                  ]\n",
    "    end1 = time.time()\n",
    "    DataSet[mod] = tmpdf\n",
    "    DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]] = np.log1p(DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]])\n",
    "print(' ============= process reservation data done.\\n')\n",
    "# join reservation data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[rtype], how= 'left', on= ['%s_store_id' % reserve2id[rtype], 'visit_date'])\n",
    "    DataSet[mod] = data\n",
    "#print(DataSet['tra'][['air_store_id', 'visit_date', 'air_reserved_visitors', 'air_reserved_dayoff_mean']].head(100))\n",
    "print('============= join reservation data done.\\n')\n",
    "####### add rolling features\n",
    "s = time.time()\n",
    "\n",
    "# mix train with test\n",
    "DataSet['tra']['is_train'] = 1\n",
    "DataSet['tes']['is_train'] = 0\n",
    "AllData = pd.concat([DataSet['tra'], DataSet['tes']], axis= 0, ignore_index= True)\n",
    "groupped = AllData.groupby(['air_store_id'])\n",
    "visitor_ticks = [39, 46, 60, 74]\n",
    "reservation_ticks = [7, 14, 21, 28]\n",
    "print('total groups %s ' % len(groupped.groups))\n",
    "dfs = []\n",
    "# rolling sum\n",
    "for g in groupped.groups: \n",
    "    gdf = groupped.get_group(g).sort_values(by= ['visit_date'])\n",
    "    for t in visitor_ticks:\n",
    "        gdf['visitor_tick_%s' % t] = np.log1p(gdf['visitors']).rolling(window= t).sum()\n",
    "        gdf['visitor_tick_%s' % t].fillna(0, inplace= True)\n",
    "#     for t in reservation_ticks:\n",
    "#         for mod in ['air', 'hpg']:\n",
    "#             gdf['reservation_%s_rolling_%s_sum' % (mod, t)] = gdf['%s_reserved_visitors' % mod].rolling(window= t).sum()\n",
    "#             gdf['reservation_%s_rolling_%s_sum' % (mod, t)].fillna(0, inplace= True)\n",
    "    gdf['holiday_rolling_3'] = gdf['holiday_flg'].rolling(window= 3).sum()\n",
    "    gdf['holiday_rolling_3'].fillna(0, inplace= True)\n",
    "#     gdf['holiday_rolling_2'] = gdf['holiday_flg'].rolling(window= 2).sum()\n",
    "#     gdf['holiday_rolling_2'].fillna(0, inplace= True)\n",
    "    dfs.append(gdf)\n",
    "# concate\n",
    "tmpdf = pd.concat(dfs, axis= 0, ignore_index= True)\n",
    "join_cols = ['air_store_id', 'visit_date', 'holiday_rolling_3']\n",
    "for i in range(len(visitor_ticks)):\n",
    "    if(i == 0):\n",
    "        continue\n",
    "    k = 'visitor_rolling_%s_%s_mean' % (visitor_ticks[i], visitor_ticks[i - 1])\n",
    "    tmpdf[k] = (tmpdf['visitor_tick_%s' % visitor_ticks[i]] - tmpdf['visitor_tick_%s' % visitor_ticks[i - 1]]) / (visitor_ticks[i] - visitor_ticks[i - 1])\n",
    "    tmpdf[k].fillna(0, inplace= True)\n",
    "    join_cols.append(k)\n",
    "# for t in reservation_ticks:\n",
    "#     for mod in ['air', 'hpg']:\n",
    "#         join_cols.append('reservation_%s_rolling_%s_sum' % (mod, t))\n",
    "# merge\n",
    "tmpdf.drop(['visitor_tick_%s' % col for col in visitor_ticks], axis= 1, inplace= True)\n",
    "AllData = AllData.merge(tmpdf[join_cols], how= 'left', on= ['air_store_id', 'visit_date'])\n",
    "# restore\n",
    "DataSet['tra'] = AllData[AllData['is_train'] == 1]\n",
    "DataSet['tes'] = AllData[AllData['is_train'] == 0]\n",
    "DataSet['tra'].drop(['is_train'], axis= 1, inplace= True)\n",
    "DataSet['tes'].drop(['is_train'], axis= 1, inplace= True)\n",
    "del AllData\n",
    "vdf = DataSet['tra'][DataSet['tra']['air_store_id'] == 'air_ba937bf13d40fb24']\n",
    "#print(vdf[join_cols].head(200))\n",
    "\n",
    "e = time.time()\n",
    "print('time elapsed %s' % ((e - s) * 60))\n",
    "print(' ============= add rolling features done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-19T12:42:14.213854Z",
     "start_time": "2017-12-19T12:41:41.839764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features  ['air_genre_name', 'air_area_name', 'air_city', 'hpg_genre_name', 'hpg_area_name', 'hpg_city']\n"
     ]
    }
   ],
   "source": [
    "#### Label encoding for categorial features\n",
    "from sklearn import *\n",
    "\n",
    "cate_feats = ['genre_name', 'area_name', 'city']\n",
    "cate_cols = ['%s_%s' % (m, cf) for m in ['air', 'hpg'] for cf in cate_feats]\n",
    "for mod in ['tra', 'tes']:\n",
    "    for col in DataSet[mod].columns:\n",
    "        if(col in cate_cols):\n",
    "            DataSet[mod][col].fillna('unknown', inplace= True)\n",
    "        else:\n",
    "            DataSet[mod][col].fillna(-1, inplace= True)\n",
    "print('Categorical features ', cate_cols)\n",
    "TrainData = DataSet['tra']\n",
    "TestData = DataSet['tes']\n",
    "for col in cate_cols:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    TrainData[col] = lbl.fit_transform(TrainData[col])\n",
    "    TestData[col] = lbl.transform(TestData[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-19T12:54:47.732861Z",
     "start_time": "2017-12-19T12:54:22.794273Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0: valid score 0.505957, holdout score 0.506731, valid length 45371\n",
      "fold 1: valid score 0.510253, holdout score 0.513754, valid length 45371\n",
      "fold 2: valid score 0.507397, holdout score 0.512314, valid length 45371\n",
      "fold 3: valid score 0.511127, holdout score 0.512553, valid length 45370\n",
      "fold 4: valid score 0.509971, holdout score 0.513811, valid length 45370\n",
      "\n",
      "======================\n",
      "CV score 0.508941, Holdout score 0.511833, Elapsed time: 24.00s\n",
      "======================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "import sys,os\n",
    "\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred) ** 0.5\n",
    "\n",
    "#print('All features ', col)\n",
    "test = TestData\n",
    "\n",
    "# split TrainData into train and holdout with random strategy\n",
    "np.random.seed(2017)\n",
    "msk = np.random.rand(len(TrainData)) < 0.1\n",
    "holdout = TrainData[msk]\n",
    "train = TrainData[~msk]\n",
    "\n",
    "# split TrainData into train and holdout with date range strategy\n",
    "# holdout = TrainData[(TrainData['visit_date'] >= datetime.date(2017, 3, 16))]\n",
    "# train = TrainData[(TrainData['visit_date'] < datetime.date(2017, 3, 16)) & \n",
    "#                   (TrainData['visit_date'] >= datetime.date(2016, 4, 1))]\n",
    "\n",
    "# for test\n",
    "y_test_pred = 0\n",
    "# Set up folds\n",
    "K = 5\n",
    "kf = model_selection.KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(1)\n",
    "# parameters\n",
    "params = {\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"objective\": \"regression_l2\",\n",
    "    \"lambda_l2\": 0.0001,\n",
    "#     \"objective\": \"fair\",\n",
    "#     \"fair_c\": 200.0,\n",
    "\n",
    "    \"num_iterations\": 400,\n",
    "    \"learning_rate\": 0.2,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"max_depth\": 60, \n",
    "\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.85,\n",
    "    \"bagging_freq\": 12, \n",
    "    \"min_hessian\": 0.001,\n",
    "\n",
    "    \"max_bin\": 127,\n",
    "}\n",
    "# Run CV\n",
    "cv_score = .0\n",
    "holdout_score = .0\n",
    "start_time = datetime.datetime.now()\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "    FoldTrain, FoldValid = train.iloc[train_index].copy(), train.iloc[test_index].copy()\n",
    "    FoldHoldout = holdout.copy()\n",
    "    FoldTest = test.copy()\n",
    "    FoldTrain['visitors'] = np.log1p(FoldTrain['visitors'])\n",
    "    FoldValid['visitors'] = np.log1p(FoldValid['visitors'])\n",
    "    FoldHoldout['visitors'] = np.log1p(FoldHoldout['visitors'])\n",
    "    FoldTest['visitors'] = np.log1p(FoldTest['visitors'])\n",
    "    #### dependent features which is extreemly subtle to data-leak\n",
    "    # percentiles features\n",
    "    tickles = ['mean', 'median', 'max', 'min', 'count']\n",
    "    for feat in ['air_store_id']:\n",
    "        gkeys = [feat, 'dow']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'visitors': tickles})    \n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "    for feat in ['air_city']:\n",
    "        gkeys = [feat, 'air_genre_name']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'visitors': tickles})    \n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "    \n",
    "    col = [c for c in FoldTrain.columns if c not in ['id', 'air_store_id', 'visit_date', 'visitors', 'hpg_store_id']]\n",
    "    # train\n",
    "    d_cv = lightgbm.Dataset(FoldTrain[col], label= FoldTrain['visitors'].values, max_bin= params['max_bin'], silent= True, free_raw_data= True)\n",
    "    model = lightgbm.train(params, d_cv)\n",
    "    # for valid\n",
    "    pred = model.predict(FoldValid[col])\n",
    "    rmsle_valid = RMSLE(FoldValid['visitors'].values, pred)\n",
    "    cv_score += rmsle_valid\n",
    "    # for holdout\n",
    "    pred = model.predict(FoldHoldout[col])\n",
    "    rmsle_holdout = RMSLE(FoldHoldout['visitors'].values, pred)\n",
    "    holdout_score += rmsle_holdout\n",
    "    # for test\n",
    "    pred = model.predict(FoldTest[col])\n",
    "    y_test_pred += pred\n",
    "\n",
    "    print('fold %s: valid score %.6f, holdout score %.6f, valid length %s' % (i, rmsle_valid, rmsle_holdout, len(FoldValid)))\n",
    "y_test_pred /= K  # Average test set predictions\n",
    "cv_score /= K # Average valid set predictions\n",
    "holdout_score /= K # Average holdout set predictions\n",
    "\n",
    "# Create submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['visitors'] = np.expm1(y_test_pred)\n",
    "OutputFileName = 'lgb_submit_%s' % (datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "sub.to_csv('%s.csv' % OutputFileName, float_format='%.6f', index=False)\n",
    "os.system('zip %s.zip %s.csv' % (OutputFileName, OutputFileName))\n",
    "\n",
    "finish_time = datetime.datetime.now()\n",
    "elapsed = (finish_time - start_time).seconds\n",
    "print('\\n======================')\n",
    "print(\"CV score %.6f, Holdout score %.6f, Elapsed time: %.2fs\" % (cv_score, holdout_score, elapsed))\n",
    "print('======================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-19T13:08:03.722973Z",
     "start_time": "2017-12-19T13:07:59.167699Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = DataSet['tra']\n",
    "# train['visitors'] = np.log1p(train['visitors'])\n",
    "# test = DataSet['tes']\n",
    "# #### dependent features which is extreemly subtle to data-leak\n",
    "# # percentiles features\n",
    "# tickles = ['mean', 'median', 'max', 'min', 'count']\n",
    "# for feat in ['air_store_id']:\n",
    "#     gkeys = [feat, 'dow']\n",
    "#     gprefix = '_'.join(gkeys)\n",
    "#     TmpDOW = train.groupby(gkeys, as_index= False).agg({'visitors': tickles})    \n",
    "#     tmpcols = gkeys.copy()\n",
    "#     tmpcols.extend(['%s_%s' % (gprefix, m) for m in tickles])\n",
    "#     TmpDOW.columns = tmpcols\n",
    "#     train = train.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "#     test = test.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "# for feat in ['air_city']:\n",
    "#     gkeys = [feat, 'air_genre_name']\n",
    "#     gprefix = '_'.join(gkeys)\n",
    "#     TmpDOW = train.groupby(gkeys, as_index= False).agg({'visitors': tickles})    \n",
    "#     tmpcols = gkeys.copy()\n",
    "#     tmpcols.extend(['%s_%s' % (gprefix, m) for m in tickles])\n",
    "#     TmpDOW.columns = tmpcols\n",
    "#     train = train.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "#     test = test.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "    \n",
    "# col = [c for c in train.columns if c not in ['id', 'air_store_id', 'visit_date', 'visitors', 'hpg_store_id']]\n",
    "# # train\n",
    "# d_cv = lightgbm.Dataset(train[col], label= train['visitors'].values, max_bin= params['max_bin'], silent= True, free_raw_data= True)\n",
    "# model = lightgbm.train(params, d_cv)\n",
    "# #\n",
    "# pred = model.predict(test[col])\n",
    "# # Create submission file\n",
    "# sub = pd.DataFrame()\n",
    "# sub['id'] = test['id']\n",
    "# sub['visitors'] = np.expm1(y_test_pred)\n",
    "# OutputFileName = 'lgb_submit_%s' % (datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "# sub.to_csv('%s.csv' % OutputFileName, float_format='%.6f', index=False)\n",
    "# os.system('zip %s.zip %s.csv' % (OutputFileName, OutputFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
