{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T03:20:58.174507Z",
     "start_time": "2017-12-28T03:20:16.733673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============\n",
      "tra data: unique stores 829, total 252108, time elased 0.79s.\n",
      "tes data: unique stores 821, total 32019, time elased 0.14s.\n",
      "============= process date related done.\n",
      "\n",
      "add city feature done.\n",
      " ================ add count features done.\n",
      "\n",
      "add holiday type done.\n",
      "========== reset holiday done.\n",
      "\n",
      "================ join holiday, store data done.\n",
      " process reservation data done.\n",
      "\n",
      "============= join reservation data done.\n",
      "\n",
      "total groups 316 \n",
      "part 0 rolling done.\n",
      "total groups 829 \n",
      "part 1 rolling done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:257: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:258: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add time series features done.\n",
      "add date int features done.\n",
      "time elapsed 992.9583549499512s\n",
      " ============= add time series related features done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numba\n",
    "import os,sys\n",
    "import gc\n",
    "import math\n",
    "\n",
    "def LoadData(InputDir):\n",
    "    \"\"\"\"\"\"\n",
    "    ## load raw data\n",
    "    data = {\n",
    "        'tra': pd.read_csv('%s/air_visit_data.csv' % InputDir, parse_dates= ['visit_date']),\n",
    "        'as': pd.read_csv('%s/air_store_info.csv' % InputDir),\n",
    "        'hs': pd.read_csv('%s/hpg_store_info.csv' % InputDir),\n",
    "        'ar': pd.read_csv('%s/air_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'hr': pd.read_csv('%s/hpg_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'id': pd.read_csv('%s/store_id_relation.csv' % InputDir),\n",
    "        'tes': pd.read_csv('%s/sample_submission.csv' % InputDir),\n",
    "        'hol': pd.read_csv('%s/date_info.csv' % InputDir, parse_dates=['calendar_date']).rename(columns={'calendar_date': 'visit_date'})\n",
    "    }\n",
    "    return data\n",
    "\n",
    "@numba.jit\n",
    "def ApplyDayoff(VisitCols, ReserveCols):\n",
    "    \"\"\"\"\"\"\n",
    "    n = len(VisitCols)\n",
    "    result = np.zeros((n, 1), dtype= 'int8')\n",
    "    for i in range(n):\n",
    "        d = (VisitCols[i]- ReserveCols[i]).days\n",
    "        if(d > 0):\n",
    "            result[i] = d\n",
    "    return result\n",
    "\n",
    "reserve2id = {'ar': 'air', 'hr': 'hpg'}\n",
    "reserve2store = {'ar': 'as', 'hr': 'hs'}# load data set\n",
    "InputDir = '../../data/raw'\n",
    "DataSet = LoadData(InputDir)\n",
    "#### \n",
    "# date related features\n",
    "print('\\n============')\n",
    "for mod in ['tra', 'tes']:\n",
    "    start0 = time.time()\n",
    "    if (mod == 'tes'):\n",
    "        DataSet[mod]['visit_date'] = DataSet[mod]['id'].map(lambda x: str(x).split('_')[2])\n",
    "        DataSet[mod]['air_store_id'] = DataSet[mod]['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "        DataSet[mod]['visit_date'] = pd.to_datetime(DataSet[mod]['visit_date'])\n",
    "    DataSet[mod]['dow'] = DataSet[mod]['visit_date'].dt.dayofweek\n",
    "    DataSet[mod]['year'] = DataSet[mod]['visit_date'].dt.year\n",
    "    DataSet[mod]['month'] = DataSet[mod]['visit_date'].dt.month\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_date'].dt.date\n",
    "    end0 = time.time()\n",
    "    print('%s data: unique stores %s, total %s, time elased %.2fs.' %\n",
    "            (mod, len(DataSet[mod]['air_store_id'].unique()), len(DataSet[mod]['air_store_id']), (end0 - start0)))\n",
    "print('============= process date related done.\\n')\n",
    "######## store data\n",
    "# add city feature\n",
    "for mod in ['ar', 'hr']:\n",
    "    DataSet[reserve2store[mod]]['%s_city' % reserve2id[mod]] = DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]].str[:5]\n",
    "    DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]] = DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]].map(lambda x: '_'.join(x.split(' ')[1:]))\n",
    "print('add city feature done.')\n",
    "# area (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_area_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g\n",
    "        ac['%s_area_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_area_name' % reserve2id[mod]])\n",
    "# genre (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_genre_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g\n",
    "        ac['%s_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_genre_name' % reserve2id[mod]])\n",
    "#  area_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_area_name' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_area_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "# city (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_city' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g\n",
    "        ac['%s_city_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        #ac['%s_area_store_ratio' % reserve2id[mod]] = ac['%s_area_store_count' % reserve2id[mod]]/len(DataSet[reserve2store[mod]])\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_city' % reserve2id[mod]])\n",
    "#  city_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_city' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_city_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "print(' ================ add count features done.\\n')\n",
    "######### holiday data\n",
    "data = DataSet['hol']\n",
    "### add holiday days\n",
    "data['visit_date'] = data['visit_date'].dt.date\n",
    "data = data.sort_values(by= 'visit_date')\n",
    "def TagHoliday(df):\n",
    "    ''''''\n",
    "    n = len(df)\n",
    "    result = ['' for x in range(n)]\n",
    "    for i in range(n):\n",
    "        if(i == 0):\n",
    "            result[i] = 'hid_%s' % 0\n",
    "        elif((df[i] - df[i-1]).days == 1):\n",
    "            result[i] = result[i - 1]\n",
    "        else:\n",
    "            result[i] = 'hid_%s' % (int(result[i - 1].split('_')[1]) + 1)\n",
    "    return result\n",
    "holidays = data[data['holiday_flg'] == 1][['visit_date']]\n",
    "holidays['hol_l0'] = TagHoliday(holidays['visit_date'].values)\n",
    "groupped = holidays.groupby(['hol_l0'])\n",
    "recs = []\n",
    "for g in groupped.groups:\n",
    "    hol_days = {}\n",
    "    hol_days['hol_l0'] = g\n",
    "    hol_days['hol_days'] = len(groupped.get_group(g))\n",
    "    recs.append(hol_days)\n",
    "tmpdf = pd.DataFrame(data= recs, index= range(len(recs)))\n",
    "holidays = holidays.merge(tmpdf, how= 'left', on= 'hol_l0')\n",
    "data = data.merge(holidays, how= 'left', on= 'visit_date')\n",
    "data.drop(['hol_l0'], axis= 1, inplace= True)\n",
    "data['hol_days'].fillna(0, inplace= True)\n",
    "print('add holiday type done.')\n",
    "### reset holiday\n",
    "wkend_holidays = data.apply((lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "data['is_weekends'] = (data['day_of_week'] == 'Sunday') | (data['day_of_week'] == 'Saturday')\n",
    "data.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "DataSet['hol'] = data\n",
    "print('========== reset holiday done.\\n')\n",
    "######## join \n",
    "# join holiday data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    data = data.merge(DataSet['hol'], how='left', on=['visit_date'])\n",
    "    data.drop(['day_of_week', 'year'], axis=1, inplace=True)\n",
    "    DataSet[mod] = data\n",
    "# join store data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[reserve2store[rtype]], how= 'left', on= ['%s_store_id' % reserve2id[rtype]])\n",
    "    DataSet[mod] = data\n",
    "print('================ join holiday, store data done.')\n",
    "######### reservation data\n",
    "for mod in ['hr', 'ar']:\n",
    "    start1 = time.time()\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_datetime'].dt.date\n",
    "    DataSet[mod]['reserve_date'] = DataSet[mod]['reserve_datetime'].dt.date\n",
    "    DataSet[mod].drop(['reserve_datetime', 'visit_datetime'], axis= 1, inplace= True)\n",
    "    tmpdf = pd.DataFrame(data=ApplyDayoff(DataSet[mod]['visit_date'].values, DataSet[mod]['reserve_date'].values),index=DataSet[mod].index, columns=['reserve_date_diff'])\n",
    "    tmpdf = pd.concat([DataSet[mod], tmpdf], axis=1)\n",
    "    tmpdf = tmpdf.groupby(['%s_store_id' % reserve2id[mod], 'visit_date'], as_index=False).agg({'reserve_visitors': sum, 'reserve_date_diff': ['mean', 'median']})\n",
    "    tmpdf.columns = ['%s_store_id' % reserve2id[mod], \n",
    "                   'visit_date', \n",
    "                   '%s_reserved_visitors' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_mean' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_median' % reserve2id[mod]\n",
    "                  ]\n",
    "    end1 = time.time()\n",
    "    DataSet[mod] = tmpdf\n",
    "    DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]] = np.log1p(DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]])\n",
    "print(' process reservation data done.\\n')\n",
    "# join reservation data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[rtype], how= 'left', on= ['%s_store_id' % reserve2id[rtype], 'visit_date'])\n",
    "        # updated 2017/12/26 14:32\n",
    "        data['%s_reserved_visitors' % reserve2id[rtype]].fillna(-1, inplace= True)\n",
    "        data['%s_reserved_dayoff_mean' % reserve2id[rtype]].fillna(-1, inplace= True)\n",
    "        data['%s_reserved_dayoff_median' % reserve2id[rtype]].fillna(-1, inplace= True)\n",
    "    data['reserved_visitors'] = (data['air_reserved_visitors'] + data['hpg_reserved_visitors'])/2\n",
    "    data['reserved_dayoff_mean'] = (data['air_reserved_dayoff_mean'] + data['hpg_reserved_dayoff_mean'])/2\n",
    "    data['reserved_dayoff_median'] = (data['air_reserved_dayoff_median'] + data['hpg_reserved_dayoff_median'])/2\n",
    "    DataSet[mod] = data\n",
    "print('============= join reservation data done.\\n')\n",
    "####### time series related\n",
    "s = time.time()\n",
    "\n",
    "# mix train with test\n",
    "DataSet['tra']['is_train'] = 1\n",
    "DataSet['tes']['is_train'] = 0\n",
    "AllData = pd.concat([DataSet['tra'], DataSet['tes']], axis= 0, ignore_index= True)\n",
    "# !!! dividing into two pieces since 2016/7/1 is a corner point, update time 2017/12/22 15:45\n",
    "DataParts = {\n",
    "    '0': AllData[AllData['visit_date'] < datetime.date(2016, 7, 1)],\n",
    "    '1': AllData[AllData['visit_date'] >= datetime.date(2016, 7, 1)]\n",
    "}\n",
    "for pidx in DataParts.keys():\n",
    "    ## rolling sum by days\n",
    "    groupped = DataParts[pidx].groupby(['air_store_id'])\n",
    "    visitor_ticks = [39, 46, 53, 60, 67, 74, 81]#, 88, 95, 102, 109, 116, 123]  # for days\n",
    "    print('total groups %s ' % len(groupped.groups))\n",
    "    dfs = []\n",
    "    for g in groupped.groups: \n",
    "        gdf = groupped.get_group(g).sort_values(by= ['visit_date'])\n",
    "        for t in visitor_ticks:\n",
    "            gdf['visitor_tick_sum_%s' % t] = np.log1p(gdf['visitors']).rolling(window= t).sum()\n",
    "            gdf['visitor_tick_sum_%s' % t].fillna(0, inplace= True)\n",
    "        dfs.append(gdf)\n",
    "    # concate\n",
    "    tmpdf = pd.concat(dfs, axis= 0, ignore_index= True)\n",
    "    join_cols = ['air_store_id', 'visit_date']\n",
    "    for i in range(len(visitor_ticks)):\n",
    "        if(i == 0):\n",
    "            continue\n",
    "        # rolling mean for one week\n",
    "        k_mean = 'visitor_rolling_%s_%s_mean' % (visitor_ticks[i], visitor_ticks[i - 1])\n",
    "        tmpdf[k_mean] = (tmpdf['visitor_tick_sum_%s' % visitor_ticks[i]] - tmpdf['visitor_tick_sum_%s' % visitor_ticks[i - 1]]) / (visitor_ticks[i] - visitor_ticks[i - 1])\n",
    "        tmpdf.loc[tmpdf[k_mean] < 0, k_mean] = -1  ## negative values exists, need to be set zero, updated 2016/12/22 20:30\n",
    "        join_cols.append(k_mean)\n",
    "    # merge\n",
    "    tmpdf.drop(['visitor_tick_sum_%s' % col for col in visitor_ticks], axis= 1, inplace= True)\n",
    "    DataParts[pidx] = DataParts[pidx].merge(tmpdf[join_cols], how= 'left', on= ['air_store_id', 'visit_date'])\n",
    "    print('part %s rolling done.' % pidx)\n",
    "# concat after all is done\n",
    "AllData = pd.concat([DataParts['0'], DataParts['1']], axis= 0, ignore_index= True)\n",
    "# restore\n",
    "DataSet['tra'] = AllData[AllData['is_train'] == 1]\n",
    "DataSet['tes'] = AllData[AllData['is_train'] == 0]\n",
    "DataSet['tra'].drop(['is_train'], axis= 1, inplace= True)\n",
    "DataSet['tes'].drop(['is_train'], axis= 1, inplace= True)\n",
    "del AllData\n",
    "gc.collect()\n",
    "print('add time series features done.')\n",
    "#### add date_int\n",
    "for mod in ['tra', 'tes']:\n",
    "    DataSet[mod]['date_int'] = DataSet[mod]['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "    DataSet[mod]['date_int'] = DataSet[mod]['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "print('add date int features done.')\n",
    "### add var_max_lat/var_max_long\n",
    "for mod in ['tra', 'tes']:\n",
    "    DataSet[mod]['lon_plus_lat_x'] = DataSet[mod]['longitude_x'] + DataSet[mod]['latitude_x'] \n",
    "    DataSet[mod]['var_max_long_x'] = DataSet[mod]['longitude_x'].max() - DataSet[mod]['longitude_x']\n",
    "    DataSet[mod]['var_max_lat_x'] = DataSet[mod]['latitude_x'].max() - DataSet[mod]['latitude_x']\n",
    "e = time.time()\n",
    "print('time elapsed %ss' % ((e - s) * 60))\n",
    "print(' ============= add time series related features done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T03:21:44.837069Z",
     "start_time": "2017-12-28T03:21:44.195365Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filling missings done.\n"
     ]
    }
   ],
   "source": [
    "### fill nulls, updated 2016/12/26 14:58\n",
    "from sklearn import *\n",
    "cate_feats = ['genre_name', 'area_name', 'city']\n",
    "cate_cols = ['%s_%s' % (m, cf) for m in ['air', 'hpg'] for cf in cate_feats]\n",
    "cate_cols.append('air_store_id')\n",
    "for mod in ['tra', 'tes']:\n",
    "    for col in DataSet[mod].columns:\n",
    "        if(col in cate_cols):\n",
    "            DataSet[mod][col].fillna('unknown', inplace= True)\n",
    "        elif(col == 'latitude_y'):\n",
    "            DataSet[mod][col].fillna(DataSet[mod]['latitude_x'], inplace= True)\n",
    "        elif(col == 'longitude_y'):\n",
    "            DataSet[mod][col].fillna(DataSet[mod]['longitude_x'], inplace= True)\n",
    "        else:\n",
    "            DataSet[mod][col].fillna(-1, inplace= True)\n",
    "print('filling missings done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T03:22:29.643364Z",
     "start_time": "2017-12-28T03:22:29.639235Z"
    }
   },
   "outputs": [],
   "source": [
    "#### Label encoding for categorial features\n",
    "TrainData = DataSet['tra']\n",
    "TestData = DataSet['tes']\n",
    "# for col in cate_cols:\n",
    "#     lbl = preprocessing.LabelEncoder()\n",
    "#     TrainData[col] = lbl.fit_transform(TrainData[col])\n",
    "#     TestData[col] = lbl.transform(TestData[col])\n",
    "# print('encoding for categorial features done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T14:30:02.587794Z",
     "start_time": "2017-12-25T14:29:24.049387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding for fold 0 done.\n",
      "add pencentile features for fold 0 done.\n",
      "Fold 0 done.\n",
      "encoding for fold 1 done.\n",
      "add pencentile features for fold 1 done.\n",
      "Fold 1 done.\n",
      "encoding for fold 2 done.\n",
      "add pencentile features for fold 2 done.\n",
      "Fold 2 done.\n",
      "encoding for fold 3 done.\n",
      "add pencentile features for fold 3 done.\n",
      "Fold 3 done.\n",
      "encoding for fold 4 done.\n",
      "add pencentile features for fold 4 done.\n",
      "Fold 4 done.\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred) ** 0.5\n",
    "\n",
    "## !!! add corner tag since 2016/7/1 is a corner point, update time 2017/12/22 15:45\n",
    "TrainData['is_up_corner'] = TrainData['visit_date'] < datetime.date(2016, 7, 1)\n",
    "TestData['is_up_corner'] = TestData['visit_date'] < datetime.date(2016, 7, 1)\n",
    "# split TrainData into train and holdout with random strategy\n",
    "np.random.seed(2017)\n",
    "msk = np.random.rand(len(TrainData)) < 0.1\n",
    "holdout = TrainData[msk]\n",
    "train = TrainData[~msk]\n",
    "test = TestData\n",
    "# Set up folds\n",
    "K = 5\n",
    "kf = model_selection.KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(1)\n",
    "OutputDir = '../../data/l0'\n",
    "if(os.path.exists('%s/kfold' % OutputDir) == False):\n",
    "    os.makedirs('%s/kfold' % OutputDir)\n",
    "cate_cols.append('air_store_id')\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "    FoldTrain, FoldValid = train.iloc[train_index].copy(), train.iloc[test_index].copy()\n",
    "    FoldHoldout = holdout.copy()\n",
    "    FoldTest = test.copy()\n",
    "    FoldTrain['visitors'] = np.log1p(FoldTrain['visitors'])\n",
    "    FoldValid['visitors'] = np.log1p(FoldValid['visitors'])\n",
    "    FoldHoldout['visitors'] = np.log1p(FoldHoldout['visitors'])\n",
    "    FoldTest['visitors'] = np.log1p(FoldTest['visitors'])\n",
    "    #### one hot encoding\n",
    "    for col in cate_cols:\n",
    "        label_binarizer = LabelBinarizer()\n",
    "        label_binarizer.fit(FoldTrain[col])\n",
    "        encoded_cols = ['%s:%s' % (col, c) for c in label_binarizer.classes_]\n",
    "        # for valid\n",
    "        encoded = pd.DataFrame(data= label_binarizer.transform(FoldValid[col]), index= FoldValid.index, columns= encoded_cols)\n",
    "        FoldValid = pd.concat([FoldValid, encoded], axis= 1)\n",
    "        FoldValid.drop([col], axis= 1, inplace= True)\n",
    "        # for holdout\n",
    "        encoded = pd.DataFrame(data= label_binarizer.transform(FoldHoldout[col]), index= FoldHoldout.index, columns= encoded_cols)\n",
    "        FoldHoldout = pd.concat([FoldHoldout, encoded], axis= 1)\n",
    "        FoldHoldout.drop([col], axis= 1, inplace= True)\n",
    "        # for test\n",
    "        encoded = pd.DataFrame(data= label_binarizer.transform(FoldTest[col]), index= FoldTest.index, columns= encoded_cols)\n",
    "        FoldTest = pd.concat([FoldTest, encoded], axis= 1)\n",
    "        FoldTest.drop([col], axis= 1, inplace= True)\n",
    "        #### Label encoding for categorial features\n",
    "#     for col in cate_cols:\n",
    "#         lbl = preprocessing.LabelEncoder()\n",
    "#         lbl.fit(FoldTrain[col])\n",
    "#         FoldValid[col] = lbl.transform(FoldValid[col])\n",
    "#         FoldHoldout[col] = lbl.transform(FoldHoldout[col])\n",
    "#         FoldTest[col] = lbl.transform(FoldTest[col])\n",
    "    ## Target encoding for categorial features\n",
    "#     FoldTrain['log1p_visitors'] = np.log1p(FoldTrain['visitors'])\n",
    "#     for col in cate_cols:\n",
    "#         gkeys = [col, 'is_up_corner']\n",
    "#         tmpdf = FoldTrain.groupby(gkeys, as_index= False).agg({'log1p_visitors': np.mean})\n",
    "#         tmpcols = gkeys.copy()\n",
    "#         tmpcols.extend(['mean_log1p_visitors'])\n",
    "#         tmpdf.columns = tmpcols\n",
    "#         FoldValid = FoldValid.merge(tmpdf, how= 'left', on= gkeys)\n",
    "#         FoldHoldout = FoldHoldout.merge(tmpdf, how= 'left', on= gkeys)\n",
    "#         FoldTest = FoldTest.merge(tmpdf, how= 'left', on= gkeys)\n",
    "#         FoldValid[col] = FoldValid['mean_log1p_visitors']\n",
    "#         FoldHoldout[col] = FoldHoldout['mean_log1p_visitors']\n",
    "#         FoldTest[col] = FoldTest['mean_log1p_visitors']\n",
    "#         FoldValid.drop(['mean_log1p_visitors'], axis= 1, inplace= True)\n",
    "#         FoldHoldout.drop(['mean_log1p_visitors'], axis= 1, inplace= True)\n",
    "#         FoldTest.drop(['mean_log1p_visitors'], axis= 1, inplace= True)\n",
    "#         FoldValid[col].fillna(0, inplace= True)\n",
    "#         FoldHoldout[col].fillna(0, inplace= True)\n",
    "#         FoldTest[col].fillna(0, inplace= True)\n",
    "#     FoldTrain.drop(['log1p_visitors'], axis= 1, inplace= True)\n",
    "    \n",
    "    print('encoding for fold %s done.' % i)\n",
    "    #### dependent features which is extreemly subtle to data-leak\n",
    "    # percentiles features\n",
    "    tickles = ['mean', 'median', 'max', 'min', 'count']\n",
    "    for feat in ['air_store_id']:\n",
    "        gkeys = [feat, 'dow', 'is_up_corner']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'visitors': tickles})    \n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### data-leak, prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(0, inplace= True)\n",
    "        FoldHoldout.fillna(0, inplace= True)\n",
    "        FoldTest.fillna(0, inplace= True)\n",
    "    for feat in ['air_city']:\n",
    "        gkeys = [feat, 'air_genre_name', 'is_up_corner']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'visitors': tickles})    \n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### data-leak, prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(0, inplace= True)\n",
    "        FoldHoldout.fillna(0, inplace= True)\n",
    "        FoldTest.fillna(0, inplace= True)\n",
    "    print('add pencentile features for fold %s done.' % i)\n",
    "    FoldOutputDir = '%s/kfold/%s' % (OutputDir, i)\n",
    "    if(os.path.exists(FoldOutputDir) == False):\n",
    "        os.makedirs(FoldOutputDir)\n",
    "    #FoldTrain.to_csv('%s/train.csv' % FoldOutputDir)\n",
    "    FoldValid.to_csv('%s/valid.csv' % FoldOutputDir, index= False)\n",
    "    FoldHoldout.to_csv('%s/holdout.csv' % FoldOutputDir, index= False)\n",
    "    FoldTest.to_csv('%s/test.csv' % FoldOutputDir, index= False)\n",
    "    \n",
    "    print('Fold %s done.' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
