{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T11:34:21.761255Z",
     "start_time": "2018-01-17T11:34:21.703131Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "\n",
    "class DataUtil2:\n",
    "    \"\"\"\"\"\"\n",
    "    @classmethod\n",
    "    def load(cls, file, format, date_cols= None):\n",
    "        \"\"\"\"\"\"\n",
    "        data = ''\n",
    "        if(format== 'csv'):\n",
    "            data = pd.read_csv(file, parse_dates= date_cols)\n",
    "        elif(format== 'json'):\n",
    "            with open(file, 'r') as i_file:\n",
    "                data = json.load(file)\n",
    "            i_file.close()\n",
    "        elif(format== 'pkl'):\n",
    "            with open(file, 'rb') as i_file:\n",
    "                data = pickle.load(i_file)\n",
    "            i_file.close()\n",
    "        elif(format == 'hdf'):\n",
    "            data = pd.read_hdf(path_or_buf= file, key='undefined')\n",
    "\n",
    "        return  data\n",
    "\n",
    "    @classmethod\n",
    "    def save(cls, data, file, format, precision= 8):\n",
    "        \"\"\"\"\"\"\n",
    "        if(format == 'csv'):\n",
    "            data.to_csv(file, float_format= '%%.%df' % precision, index= False)\n",
    "        elif(format == 'json'):\n",
    "            with open(file, 'w') as o_file:\n",
    "                json.dump(data, o_file, ensure_ascii= True, indent= 4)\n",
    "            o_file.close()\n",
    "        elif(format == 'pkl'):\n",
    "            with open(file, 'wb') as o_file:\n",
    "                pickle.dump(data, o_file, -1)\n",
    "            o_file.close()\n",
    "        elif(format== 'hdf'):\n",
    "            data.to_hdf(path_or_buf= file, key='undefined', mode='w', complib='blosc')\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T14:02:19.871722Z",
     "start_time": "2018-01-17T13:59:11.933852Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data done. time elapsed 35.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:131: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0: valid score 0.542873, holdout score 0.535327, valid length 45371\n",
      "fold 1: valid score 0.534834, holdout score 0.536187, valid length 45371\n",
      "fold 2: valid score 0.538920, holdout score 0.535676, valid length 45371\n",
      "fold 3: valid score 0.544150, holdout score 0.536912, valid length 45370\n",
      "fold 4: valid score 0.539321, holdout score 0.537339, valid length 45370\n",
      "\n",
      "======================\n",
      "CV score 0.5400, Holdout score 0.5363, Elapsed time: 63.44s\n",
      "======================\n",
      "\n",
      "------- part 0 done. time elapsed 63.44s ----------\n",
      "\n",
      "fold 0: valid score 0.584129, holdout score 0.575554, valid length 45371\n",
      "fold 1: valid score 0.577911, holdout score 0.574825, valid length 45371\n",
      "fold 2: valid score 0.579364, holdout score 0.575528, valid length 45371\n",
      "fold 3: valid score 0.582203, holdout score 0.578405, valid length 45370\n",
      "fold 4: valid score 0.576305, holdout score 0.579006, valid length 45370\n",
      "\n",
      "======================\n",
      "CV score 0.5800, Holdout score 0.5767, Elapsed time: 133.14s\n",
      "======================\n",
      "\n",
      "------- part 1 done. time elapsed 133.14s ----------\n",
      "\n",
      "fold 0: valid score 0.543030, holdout score 0.535378, valid length 45371\n",
      "fold 1: valid score 0.535052, holdout score 0.536212, valid length 45371\n",
      "fold 2: valid score 0.538856, holdout score 0.535739, valid length 45371\n",
      "fold 3: valid score 0.544282, holdout score 0.536626, valid length 45370\n",
      "fold 4: valid score 0.539268, holdout score 0.537269, valid length 45370\n",
      "\n",
      "======================\n",
      "CV score 0.5401, Holdout score 0.5362, Elapsed time: 159.87s\n",
      "======================\n",
      "\n",
      "------- part 2 done. time elapsed 159.87s ----------\n",
      "\n",
      "fold 0: valid score 0.546178, holdout score 0.538942, valid length 45371\n",
      "fold 1: valid score 0.540111, holdout score 0.539848, valid length 45371\n",
      "fold 2: valid score 0.543702, holdout score 0.541346, valid length 45371\n",
      "fold 3: valid score 0.548263, holdout score 0.539538, valid length 45370\n",
      "fold 4: valid score 0.543277, holdout score 0.540632, valid length 45370\n",
      "\n",
      "======================\n",
      "CV score 0.5443, Holdout score 0.5401, Elapsed time: 187.26s\n",
      "======================\n",
      "\n",
      "------- part 3 done. time elapsed 187.26s ----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# meta-tree models    #\n",
    "#######################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import os,sys\n",
    "import gc\n",
    "from sklearn import *\n",
    "import lightgbm\n",
    "import random\n",
    "import json\n",
    "\n",
    "drop_cols = ['id', 'visit_date', 'visitors', 'hpg_store_id', 'fold', 'air_store_id', 'air_reserved_visitors', 'hpg_reserved_visitors','reserved_visitors']\n",
    "    \n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred) ** 0.5\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'rmsle', RMSLE(labels, preds), True\n",
    "\n",
    "start = time.time()\n",
    "DataBaseDir = '../../data'\n",
    "InputDir = '%s/l0/kfold' % DataBaseDir\n",
    "MetaInputDir = '%s/meta/kfold' % DataBaseDir\n",
    "kfold = 5\n",
    "meta_ratio = 0.75\n",
    "TreeNum = 200\n",
    "#### load data\n",
    "valid_dfs = []\n",
    "holdout_dfs = []\n",
    "test_dfs = []\n",
    "meta_feats = ['NN_EF', 'knn_2', 'knn_4', 'knn_8', 'knn_16', 'knn_32', 'knn_64', 'knn_128', 'knn_256', 'knn_512', 'knn_1024']\n",
    "for fold in range(kfold):\n",
    "    FoldInputDir = '%s/%s' % (InputDir, fold)\n",
    "    valid = pd.read_csv('%s/valid.csv' % FoldInputDir, parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "    holdout = pd.read_csv('%s/holdout.csv' % FoldInputDir, parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "    test = pd.read_csv('%s/test.csv' % FoldInputDir, parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "    for t in meta_feats:\n",
    "        # load meta-feature\n",
    "        FoldOutputDir = '%s/%s' % (MetaInputDir, fold)\n",
    "        valid_cb_ef = pd.read_csv('%s/valid_%s.csv' % (FoldOutputDir, t), parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "        holdout_cb_ef = pd.read_csv('%s/holdout_%s.csv' % (FoldOutputDir, t), parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "        test_cb_ef = pd.read_csv('%s/test_%s.csv' % (FoldOutputDir, t), parse_dates= ['visit_date']).reset_index(drop= True)\n",
    "        # concate\n",
    "        valid = pd.concat([valid, valid_cb_ef[[t]]], axis= 1)\n",
    "        holdout = pd.concat([holdout, holdout_cb_ef[[t]]], axis= 1)\n",
    "        test = pd.concat([test, test_cb_ef[[t]]], axis= 1)\n",
    "    #\n",
    "    valid['fold'] = fold\n",
    "    valid_dfs.append(valid)\n",
    "    holdout_dfs.append(holdout)\n",
    "    test_dfs.append(test)\n",
    "TrainData = pd.concat(valid_dfs, axis= 0, ignore_index= True)\n",
    "end = time.time()\n",
    "print('Load data done. time elapsed %.2fs' % (end - start))\n",
    "##### model selection with CV\n",
    "# score\n",
    "holdout_score = .0\n",
    "# parameters\n",
    "params = {\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"objective\": \"regression_l2\",\n",
    "    \"lambda_l2\": [1e-3],\n",
    "    \"learning_rate\": [0.8],\n",
    "    \n",
    "    \"num_iterations\": 500,\n",
    "    \"min_data_in_leaf\": 150,\n",
    "    #'num_leaves': 255,\n",
    "    \"max_depth\": 16,\n",
    "\n",
    "    \"feature_fraction\": 0.90,\n",
    "    \"bagging_fraction\": 0.85,\n",
    "    \"bagging_freq\": 20,\n",
    "    \"min_hessian\": 0.001,\n",
    "    \n",
    "    \"verbose\": False,\n",
    "    \"max_bin\": 63,\n",
    "}\n",
    "total_cols = [c for c in TrainData.columns if(c not in drop_cols)]\n",
    "K = int(meta_ratio * len(total_cols))\n",
    "\n",
    "for idx in range(TreeNum):\n",
    "    strategy = 'lgb_l2_meta_trees_%s_%s_%s' % (int(meta_ratio * 100), TreeNum, idx)\n",
    "    selected_cols = [total_cols[i] for i in sorted(np.random.choice(range(len(total_cols)), K, replace= False))]\n",
    "    OutputDir = '%s/meta_trees_%s_%s/l1/kfold' % (DataBaseDir, int(meta_ratio * 100), TreeNum)\n",
    "    if(os.path.exists(OutputDir) == False):\n",
    "        os.makedirs(OutputDir)\n",
    "    # save feature space\n",
    "    with open('%s/sub_feats.txt' % OutputDir, 'w') as o_file:\n",
    "        for feat in selected_cols:\n",
    "            o_file.write('%s\\n' % feat)\n",
    "    o_file.close()\n",
    "    ## retrain and store\n",
    "    cv_score = .0\n",
    "    holdout_score = .0\n",
    "    for fold in range(kfold):\n",
    "        FoldData = {\n",
    "            'train': TrainData[TrainData['fold'] != fold],\n",
    "            'valid': TrainData[TrainData['fold'] == fold],\n",
    "            'holdout': holdout_dfs[fold],\n",
    "            'test': test_dfs[fold]\n",
    "        }\n",
    "        # train\n",
    "        dtrain = lightgbm.Dataset(FoldData['train'][selected_cols], label= FoldData['train']['visitors'], silent= True, free_raw_data= True)\n",
    "        dvalid = lightgbm.Dataset(FoldData['valid'][selected_cols], FoldData['valid']['visitors'], reference=dtrain)\n",
    "        param = {\n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'regression_l2',\n",
    "                \n",
    "            'lambda_l2': params['lambda_l2'][0],\n",
    "            'learning_rate': params['learning_rate'][0],\n",
    "                        \n",
    "            'num_iterations': params['num_iterations'],\n",
    "            'feature_fraction': params['feature_fraction'],\n",
    "            'bagging_fraction': params['bagging_fraction'],\n",
    "            'bagging_freq': params['bagging_freq'],\n",
    "            'min_hessian': params['min_hessian'],\n",
    "            'max_bin': params['max_bin'],\n",
    "        }\n",
    "        model = lightgbm.train(param, \n",
    "                        train_set= dtrain, \n",
    "                        num_boost_round= params['num_iterations'], \n",
    "                        valid_sets= dvalid,\n",
    "                        feval= evalerror,\n",
    "                        verbose_eval= False,\n",
    "                        early_stopping_rounds= 100)        \n",
    "        # for valid\n",
    "        FoldData['valid'][strategy] = model.predict(FoldData['valid'][selected_cols])\n",
    "        rmsle_valid = RMSLE(FoldData['valid']['visitors'].values, FoldData['valid'][strategy])\n",
    "        cv_score += rmsle_valid\n",
    "        # for holdout\n",
    "        FoldData['holdout'][strategy] = model.predict(FoldData['holdout'][selected_cols])\n",
    "        rmsle_holdout = RMSLE(FoldData['holdout']['visitors'].values, FoldData['holdout'][strategy])\n",
    "        holdout_score += rmsle_holdout\n",
    "        # for test\n",
    "        FoldData['test'][strategy] = model.predict(FoldData['test'][selected_cols])\n",
    "\n",
    "        print('fold %s: valid score %.6f, holdout score %.6f, valid length %s' % (fold, rmsle_valid, rmsle_holdout, len(FoldData['valid'])))  \n",
    "        #### output\n",
    "        FoldOutputDir = '%s/%s' % (OutputDir, fold)\n",
    "        if(os.path.exists(FoldOutputDir) == False):\n",
    "            os.makedirs(FoldOutputDir)\n",
    "        for mod in FoldData.keys():\n",
    "            if(mod == 'train'):\n",
    "                continue\n",
    "            OutCols = []\n",
    "            if(mod == 'test'):\n",
    "                OutCols.append('id')\n",
    "            OutCols.extend(['air_store_id', 'visit_date', 'visitors', strategy])\n",
    "            OutputFile = '%s/%s_%s.csv' % (FoldOutputDir, mod, strategy)\n",
    "            OutFoldData = FoldData[mod][OutCols]\n",
    "            OutFoldData.to_csv(OutputFile, index= False)\n",
    "    \n",
    "    cv_score /= kfold # Average valid set predictions\n",
    "    holdout_score /= kfold # Average holdout set predictions\n",
    "\n",
    "    end = time.time()\n",
    "    print('\\n======================')\n",
    "    print(\"CV score %.4f, Holdout score %.4f, Elapsed time: %.2fs\" % (cv_score, holdout_score, (end - start)))\n",
    "    print('======================\\n')\n",
    "    \n",
    "    with open('%s/result.txt' % OutputDir, 'w') as o_file:\n",
    "        o_file.write('cv score %.4f, holdout score %.4f' % (cv_score, holdout_score))\n",
    "    o_file.close()\n",
    "    \n",
    "    print('------- part %s done. time elapsed %.2fs ----------\\n' % (idx, (end - start)))\n",
    "    if(idx == 3):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
