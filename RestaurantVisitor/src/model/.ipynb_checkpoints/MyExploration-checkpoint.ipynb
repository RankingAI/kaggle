{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:13:35.899343Z",
     "start_time": "2017-12-26T07:12:55.623127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============\n",
      "tra data: unique stores 829, total 252108, time elased 0.76s.\n",
      "tes data: unique stores 821, total 32019, time elased 0.12s.\n",
      "============= process date related done.\n",
      "\n",
      "add city feature done.\n",
      " ================ add count features done.\n",
      "\n",
      "add holiday type done.\n",
      "========== reset holiday done.\n",
      "\n",
      "================ join holiday, store data done.\n",
      " process reservation data done.\n",
      "\n",
      "============= join reservation data done.\n",
      "\n",
      "total groups 316 \n",
      "part 0 rolling done.\n",
      "total groups 829 \n",
      "part 1 rolling done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:257: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:258: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add time series features done.\n",
      "add date int features done.\n",
      "time elapsed 918.034815788269s\n",
      " ============= add time series related features done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numba\n",
    "import os,sys\n",
    "import gc\n",
    "import math\n",
    "\n",
    "def LoadData(InputDir):\n",
    "    \"\"\"\"\"\"\n",
    "    ## load raw data\n",
    "    data = {\n",
    "        'tra': pd.read_csv('%s/air_visit_data.csv' % InputDir, parse_dates= ['visit_date']),\n",
    "        'as': pd.read_csv('%s/air_store_info.csv' % InputDir),\n",
    "        'hs': pd.read_csv('%s/hpg_store_info.csv' % InputDir),\n",
    "        'ar': pd.read_csv('%s/air_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'hr': pd.read_csv('%s/hpg_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'id': pd.read_csv('%s/store_id_relation.csv' % InputDir),\n",
    "        'tes': pd.read_csv('%s/sample_submission.csv' % InputDir),\n",
    "        'hol': pd.read_csv('%s/date_info.csv' % InputDir, parse_dates=['calendar_date']).rename(columns={'calendar_date': 'visit_date'})\n",
    "    }\n",
    "    return data\n",
    "\n",
    "@numba.jit\n",
    "def ApplyDayoff(VisitCols, ReserveCols):\n",
    "    \"\"\"\"\"\"\n",
    "    n = len(VisitCols)\n",
    "    result = np.zeros((n, 1), dtype= 'int8')\n",
    "    for i in range(n):\n",
    "        d = (VisitCols[i]- ReserveCols[i]).days\n",
    "        if(d > 0):\n",
    "            result[i] = d\n",
    "    return result\n",
    "\n",
    "reserve2id = {'ar': 'air', 'hr': 'hpg'}\n",
    "reserve2store = {'ar': 'as', 'hr': 'hs'}# load data set\n",
    "InputDir = '../../data/raw'\n",
    "DataSet = LoadData(InputDir)\n",
    "#### \n",
    "# date related features\n",
    "print('\\n============')\n",
    "for mod in ['tra', 'tes']:\n",
    "    start0 = time.time()\n",
    "    if (mod == 'tes'):\n",
    "        DataSet[mod]['visit_date'] = DataSet[mod]['id'].map(lambda x: str(x).split('_')[2])\n",
    "        DataSet[mod]['air_store_id'] = DataSet[mod]['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "        DataSet[mod]['visit_date'] = pd.to_datetime(DataSet[mod]['visit_date'])\n",
    "    DataSet[mod]['dow'] = DataSet[mod]['visit_date'].dt.dayofweek\n",
    "    DataSet[mod]['year'] = DataSet[mod]['visit_date'].dt.year\n",
    "    DataSet[mod]['month'] = DataSet[mod]['visit_date'].dt.month\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_date'].dt.date\n",
    "    end0 = time.time()\n",
    "    print('%s data: unique stores %s, total %s, time elased %.2fs.' %\n",
    "            (mod, len(DataSet[mod]['air_store_id'].unique()), len(DataSet[mod]['air_store_id']), (end0 - start0)))\n",
    "print('============= process date related done.\\n')\n",
    "######## store data\n",
    "# add city feature\n",
    "for mod in ['ar', 'hr']:\n",
    "    DataSet[reserve2store[mod]]['%s_city' % reserve2id[mod]] = DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]].str[:5]\n",
    "print('add city feature done.')\n",
    "# area (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_area_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g\n",
    "        ac['%s_area_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_area_name' % reserve2id[mod]])\n",
    "# genre (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_genre_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g\n",
    "        ac['%s_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_genre_name' % reserve2id[mod]])\n",
    "#  area_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_area_name' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_area_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "# city (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_city' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g\n",
    "        ac['%s_city_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        #ac['%s_area_store_ratio' % reserve2id[mod]] = ac['%s_area_store_count' % reserve2id[mod]]/len(DataSet[reserve2store[mod]])\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_city' % reserve2id[mod]])\n",
    "#  city_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_city' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_city_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "print(' ================ add count features done.\\n')\n",
    "######### holiday data\n",
    "data = DataSet['hol']\n",
    "### add holiday days\n",
    "data['visit_date'] = data['visit_date'].dt.date\n",
    "data = data.sort_values(by= 'visit_date')\n",
    "def TagHoliday(df):\n",
    "    ''''''\n",
    "    n = len(df)\n",
    "    result = ['' for x in range(n)]\n",
    "    for i in range(n):\n",
    "        if(i == 0):\n",
    "            result[i] = 'hid_%s' % 0\n",
    "        elif((df[i] - df[i-1]).days == 1):\n",
    "            result[i] = result[i - 1]\n",
    "        else:\n",
    "            result[i] = 'hid_%s' % (int(result[i - 1].split('_')[1]) + 1)\n",
    "    return result\n",
    "holidays = data[data['holiday_flg'] == 1][['visit_date']]\n",
    "holidays['hol_l0'] = TagHoliday(holidays['visit_date'].values)\n",
    "groupped = holidays.groupby(['hol_l0'])\n",
    "recs = []\n",
    "for g in groupped.groups:\n",
    "    hol_days = {}\n",
    "    hol_days['hol_l0'] = g\n",
    "    hol_days['hol_days'] = len(groupped.get_group(g))\n",
    "    recs.append(hol_days)\n",
    "tmpdf = pd.DataFrame(data= recs, index= range(len(recs)))\n",
    "holidays = holidays.merge(tmpdf, how= 'left', on= 'hol_l0')\n",
    "data = data.merge(holidays, how= 'left', on= 'visit_date')\n",
    "data.drop(['hol_l0'], axis= 1, inplace= True)\n",
    "data['hol_days'].fillna(0, inplace= True)\n",
    "print('add holiday type done.')\n",
    "### reset holiday\n",
    "wkend_holidays = data.apply((lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "data['is_weekends'] = (data['day_of_week'] == 'Sunday') | (data['day_of_week'] == 'Saturday')\n",
    "data.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "DataSet['hol'] = data\n",
    "print('========== reset holiday done.\\n')\n",
    "######## join \n",
    "# join holiday data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    data = data.merge(DataSet['hol'], how='left', on=['visit_date'])\n",
    "    data.drop(['day_of_week', 'year'], axis=1, inplace=True)\n",
    "    DataSet[mod] = data\n",
    "# join store data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[reserve2store[rtype]], how= 'left', on= ['%s_store_id' % reserve2id[rtype]])\n",
    "    DataSet[mod] = data\n",
    "print('================ join holiday, store data done.')\n",
    "######### reservation data\n",
    "for mod in ['hr', 'ar']:\n",
    "    start1 = time.time()\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_datetime'].dt.date\n",
    "    DataSet[mod]['reserve_date'] = DataSet[mod]['reserve_datetime'].dt.date\n",
    "    DataSet[mod].drop(['reserve_datetime', 'visit_datetime'], axis= 1, inplace= True)\n",
    "    tmpdf = pd.DataFrame(data=ApplyDayoff(DataSet[mod]['visit_date'].values, DataSet[mod]['reserve_date'].values),index=DataSet[mod].index, columns=['reserve_date_diff'])\n",
    "    tmpdf = pd.concat([DataSet[mod], tmpdf], axis=1)\n",
    "    tmpdf = tmpdf.groupby(['%s_store_id' % reserve2id[mod], 'visit_date'], as_index=False).agg({'reserve_visitors': sum, 'reserve_date_diff': ['mean', 'median']})\n",
    "    tmpdf.columns = ['%s_store_id' % reserve2id[mod], \n",
    "                   'visit_date', \n",
    "                   '%s_reserved_visitors' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_mean' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_median' % reserve2id[mod]\n",
    "                  ]\n",
    "    end1 = time.time()\n",
    "    DataSet[mod] = tmpdf\n",
    "    DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]] = np.log1p(DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]])\n",
    "print(' process reservation data done.\\n')\n",
    "# join reservation data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[rtype], how= 'left', on= ['%s_store_id' % reserve2id[rtype], 'visit_date'])\n",
    "        # updated 2017/12/26 14:32\n",
    "        data['%s_reserved_visitors' % reserve2id[rtype]].fillna(-1, inplace= True)\n",
    "        data['%s_reserved_dayoff_mean' % reserve2id[rtype]].fillna(-1, inplace= True)\n",
    "        data['%s_reserved_dayoff_median' % reserve2id[rtype]].fillna(-1, inplace= True)\n",
    "    data['reserved_visitors'] = (data['air_reserved_visitors'] + data['hpg_reserved_visitors'])/2\n",
    "    data['reserved_dayoff_mean'] = (data['air_reserved_dayoff_mean'] + data['hpg_reserved_dayoff_mean'])/2\n",
    "    data['reserved_dayoff_median'] = (data['air_reserved_dayoff_median'] + data['hpg_reserved_dayoff_median'])/2\n",
    "    DataSet[mod] = data\n",
    "print('============= join reservation data done.\\n')\n",
    "####### time series related\n",
    "s = time.time()\n",
    "\n",
    "# mix train with test\n",
    "DataSet['tra']['is_train'] = 1\n",
    "DataSet['tes']['is_train'] = 0\n",
    "AllData = pd.concat([DataSet['tra'], DataSet['tes']], axis= 0, ignore_index= True)\n",
    "# !!! dividing into two pieces since 2016/7/1 is a corner point, update time 2017/12/22 15:45\n",
    "DataParts = {\n",
    "    '0': AllData[AllData['visit_date'] < datetime.date(2016, 7, 1)],\n",
    "    '1': AllData[AllData['visit_date'] >= datetime.date(2016, 7, 1)]\n",
    "}\n",
    "for pidx in DataParts.keys():\n",
    "    ## rolling sum by days\n",
    "    groupped = DataParts[pidx].groupby(['air_store_id'])\n",
    "    visitor_ticks = [39, 46, 53, 60, 67, 74, 81]#, 88, 95, 102, 109, 116, 123]  # for days\n",
    "    print('total groups %s ' % len(groupped.groups))\n",
    "    dfs = []\n",
    "    for g in groupped.groups: \n",
    "        gdf = groupped.get_group(g).sort_values(by= ['visit_date'])\n",
    "        for t in visitor_ticks:\n",
    "            gdf['visitor_tick_sum_%s' % t] = np.log1p(gdf['visitors']).rolling(window= t).sum()\n",
    "            gdf['visitor_tick_sum_%s' % t].fillna(0, inplace= True)\n",
    "        dfs.append(gdf)\n",
    "    # concate\n",
    "    tmpdf = pd.concat(dfs, axis= 0, ignore_index= True)\n",
    "    join_cols = ['air_store_id', 'visit_date']\n",
    "    for i in range(len(visitor_ticks)):\n",
    "        if(i == 0):\n",
    "            continue\n",
    "        # rolling mean for one week\n",
    "        k_mean = 'visitor_rolling_%s_%s_mean' % (visitor_ticks[i], visitor_ticks[i - 1])\n",
    "        tmpdf[k_mean] = (tmpdf['visitor_tick_sum_%s' % visitor_ticks[i]] - tmpdf['visitor_tick_sum_%s' % visitor_ticks[i - 1]]) / (visitor_ticks[i] - visitor_ticks[i - 1])\n",
    "        tmpdf.loc[tmpdf[k_mean] < 0, k_mean] = -1  ## negative values exists, need to be set zero, updated 2016/12/22 20:30\n",
    "        #tmpdf[k_mean].fillna(0, inplace= True)\n",
    "        join_cols.append(k_mean)\n",
    "    # merge\n",
    "    tmpdf.drop(['visitor_tick_sum_%s' % col for col in visitor_ticks], axis= 1, inplace= True)\n",
    "    DataParts[pidx] = DataParts[pidx].merge(tmpdf[join_cols], how= 'left', on= ['air_store_id', 'visit_date'])\n",
    "    print('part %s rolling done.' % pidx)\n",
    "# concat after all is done\n",
    "AllData = pd.concat([DataParts['0'], DataParts['1']], axis= 0, ignore_index= True)\n",
    "# restore\n",
    "DataSet['tra'] = AllData[AllData['is_train'] == 1]\n",
    "DataSet['tes'] = AllData[AllData['is_train'] == 0]\n",
    "DataSet['tra'].drop(['is_train'], axis= 1, inplace= True)\n",
    "DataSet['tes'].drop(['is_train'], axis= 1, inplace= True)\n",
    "del AllData\n",
    "gc.collect()\n",
    "print('add time series features done.')\n",
    "#### add date_int\n",
    "for mod in ['tra', 'tes']:\n",
    "    DataSet[mod]['date_int'] = DataSet[mod]['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "    DataSet[mod]['date_int'] = DataSet[mod]['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "print('add date int features done.')\n",
    "### add var_max_lat/var_max_long\n",
    "for mod in ['tra', 'tes']:\n",
    "    DataSet[mod]['lon_plus_lat_x'] = DataSet[mod]['longitude_x'] + DataSet[mod]['latitude_x'] \n",
    "    DataSet[mod]['var_max_long_x'] = DataSet[mod]['longitude_x'].max() - DataSet[mod]['longitude_x']\n",
    "    DataSet[mod]['var_max_lat_x'] = DataSet[mod]['latitude_x'].max() - DataSet[mod]['latitude_x']\n",
    "e = time.time()\n",
    "print('time elapsed %ss' % ((e - s) * 60))\n",
    "print(' ============= add time series related features done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.823444Z",
     "start_time": "2017-12-26T07:03:02.639Z"
    }
   },
   "outputs": [],
   "source": [
    "print(DataSet['tes'].dtypes)\n",
    "print(DataSet['tes'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:14:21.276889Z",
     "start_time": "2017-12-26T07:14:21.056650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filling missings done.\n"
     ]
    }
   ],
   "source": [
    "### fill nulls, updated 2016/12/26 14:58\n",
    "from sklearn import *\n",
    "cate_feats = ['genre_name', 'area_name', 'city']\n",
    "cate_cols = ['%s_%s' % (m, cf) for m in ['air', 'hpg'] for cf in cate_feats]\n",
    "for mod in ['tra', 'tes']:\n",
    "    for col in DataSet[mod].columns:\n",
    "        if(col in cate_cols):\n",
    "            DataSet[mod][col].fillna('unknown', inplace= True)\n",
    "        elif(col == 'latitude_y'):\n",
    "            DataSet[mod][col].fillna(DataSet[mod]['latitude_x'], inplace= True)\n",
    "        elif(col == 'longitude_y'):\n",
    "            DataSet[mod][col].fillna(DataSet[mod]['longitude_x'], inplace= True)\n",
    "        else:\n",
    "            DataSet[mod][col].fillna(-1, inplace= True)\n",
    "print('filling missings done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:35:45.433749Z",
     "start_time": "2017-12-26T07:35:44.890620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skew in numerical features: \n",
      "\n",
      "                                 Skew\n",
      "hpg_reserved_dayoff_median  10.823144\n",
      "hpg_reserved_dayoff_mean    10.533338\n",
      "hpg_city_genre_store_count   9.271998\n",
      "air_reserved_dayoff_median   8.868100\n",
      "air_reserved_dayoff_mean     8.109074\n",
      "reserved_dayoff_median       7.725545\n",
      "reserved_dayoff_mean         7.242985\n",
      "hpg_area_genre_store_count   6.112918\n",
      "hpg_city_store_count         5.690931\n",
      "hpg_genre_store_count        5.100201\n",
      "hpg_area_store_count         5.014142\n",
      "hpg_reserved_visitors        4.615662\n",
      "reserved_visitors            3.017850\n",
      "air_reserved_visitors        2.911299\n",
      "air_area_genre_store_count   2.789250\n",
      "var_max_long_x               0.905010\n",
      "air_area_store_count         0.810598\n",
      "date_int                     0.461862\n",
      "air_city_genre_store_count   0.245306\n",
      "month                        0.217162\n",
      "dow                         -0.024987\n",
      "lon_plus_lat_x              -0.096941\n",
      "air_city_store_count        -0.180646\n",
      "air_genre_store_count       -0.308544\n",
      "visitor_rolling_81_74_mean  -0.473355\n",
      "visitor_rolling_74_67_mean  -0.581667\n",
      "visitor_rolling_67_60_mean  -0.694906\n",
      "visitor_rolling_60_53_mean  -0.813966\n",
      "longitude_x                 -0.905010\n",
      "longitude_y                 -0.905072\n",
      "visitor_rolling_53_46_mean  -0.940424\n",
      "visitor_rolling_46_39_mean  -1.076641\n",
      "var_max_lat_x               -2.687477\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "### transformat skewed features\n",
    "from scipy.stats import norm, skew\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "drop_cols = ['id', 'air_store_id', 'visit_date', 'visitors', 'hpg_store_id', \n",
    "             'is_train', 'hol_days', 'holiday_flg', 'is_weekends', 'latitude_x', \n",
    "             'latitude_y']\n",
    "\n",
    "DataSet['tra']['is_train'] = 1\n",
    "DataSet['tes']['is_train'] = 0\n",
    "all_cols = DataSet['tra'].columns\n",
    "all_data = pd.concat([DataSet['tra'], DataSet['tes'][all_cols]], axis= 0)\n",
    "tmp_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "numeric_feats = [col for col in tmp_feats if col not in drop_cols]\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "print(skewness)\n",
    "\n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "DataSet['tra'] = all_data[all_data['is_train'] == 1]\n",
    "DataSet['tra'].drop(['is_train'], axis= 1, inplace= True)\n",
    "DataSet['test'] = all_data[all_data['is_train'] == 0]\n",
    "DataSet['tes'].drop(['is_train'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.825490Z",
     "start_time": "2017-12-26T07:03:02.642Z"
    }
   },
   "outputs": [],
   "source": [
    "#### Label encoding for categorial features\n",
    "TrainData = DataSet['tra']\n",
    "TestData = DataSet['tes']\n",
    "for col in cate_cols:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    TrainData[col] = lbl.fit_transform(TrainData[col])\n",
    "    TestData[col] = lbl.transform(TestData[col])\n",
    "print('encoding for categorial features done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.826490Z",
     "start_time": "2017-12-26T07:03:02.643Z"
    }
   },
   "outputs": [],
   "source": [
    "mod2id = {'ar': 'air', 'hr': 'hpg'}\n",
    "### load data set\n",
    "InputDir = '../../data/raw'\n",
    "DataSet = LoadData(InputDir)\n",
    "\n",
    "### date related features\n",
    "print('\\n============')\n",
    "for mod in ['tra', 'tes']:\n",
    "    start0 = time.time()\n",
    "    if (mod == 'tes'):\n",
    "        DataSet[mod]['visit_date'] = DataSet[mod]['id'].map(lambda x: str(x).split('_')[2])\n",
    "        DataSet[mod]['air_store_id'] = DataSet[mod]['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "        DataSet[mod]['visit_date'] = pd.to_datetime(DataSet[mod]['visit_date'])\n",
    "    DataSet[mod]['dow'] = DataSet[mod]['visit_date'].dt.dayofweek\n",
    "    DataSet[mod]['year'] = DataSet[mod]['visit_date'].dt.year\n",
    "    DataSet[mod]['month'] = DataSet[mod]['visit_date'].dt.month\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_date'].dt.date\n",
    "    end0 = time.time()\n",
    "    print('%s data: unique stores %s, total %s, time elased %.2fs.' %\n",
    "            (mod, len(DataSet[mod]['air_store_id'].unique()), len(DataSet[mod]['air_store_id']), (end0 - start0)))\n",
    "print('')\n",
    "# for reservation data\n",
    "for mod in ['hr', 'ar']:\n",
    "    start1 = time.time()\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_datetime'].dt.date\n",
    "    DataSet[mod].drop(['visit_datetime'], axis= 1, inplace= True)\n",
    "    DataSet[mod]['reserve_date'] = DataSet[mod]['reserve_datetime'].dt.date\n",
    "    DataSet[mod].drop(['reserve_datetime'], axis= 1, inplace= True)\n",
    "    end1 = time.time()\n",
    "    print('time-consuming part %.2f.' % (end1 - start1))\n",
    "end0 = time.time()\n",
    "print('=============')\n",
    "print('process date done, time consumed %.2f.\\n' % (end0 - start0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.827645Z",
     "start_time": "2017-12-26T07:03:02.644Z"
    }
   },
   "outputs": [],
   "source": [
    "start0 = time.time()\n",
    "def pop_std(x):\n",
    "    return x.std(ddof=0)\n",
    "for mod in ['ar', 'hr']:\n",
    "    s0 = time.time()\n",
    "    tmpdf = pd.DataFrame(data=ApplyDayoff(DataSet[mod]['visit_date'].values, DataSet[mod]['reserve_date'].values),\n",
    "                             index=DataSet[mod].index, columns=['reserve_date_diff'])\n",
    "    tmpdf = pd.concat([DataSet[mod], tmpdf], axis=1)\n",
    "    e0 = time.time()\n",
    "    s1 = time.time()\n",
    "    tmpdf = tmpdf.groupby(['%s_store_id' % mod2id[mod], 'visit_date'], as_index=False).agg({'reserve_visitors': sum, 'reserve_date_diff': ['mean', 'median', pop_std]})\n",
    "    tmpdf.columns = ['%s_store_id' % mod2id[mod], \n",
    "                   'visit_date', \n",
    "                   '%s_reserved_visitors_sum' % mod2id[mod], \n",
    "                   '%s_reserved_dayoff_mean' % mod2id[mod], \n",
    "                   '%s_reserved_visitors_median' % mod2id[mod], \n",
    "                   '%s_reserved_visitors_std' % mod2id[mod]\n",
    "                  ]\n",
    "    e1 = time.time()\n",
    "    DataSet[mod] = tmpdf\n",
    "    print(DataSet[mod].head(100))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.829111Z",
     "start_time": "2017-12-26T07:03:02.646Z"
    }
   },
   "outputs": [],
   "source": [
    "DataSet['hol'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.829961Z",
     "start_time": "2017-12-26T07:03:02.647Z"
    }
   },
   "outputs": [],
   "source": [
    "day_periods = (DataSet['tes']['visit_date'].max() - DataSet['tes']['visit_date'].min()).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.830836Z",
     "start_time": "2017-12-26T07:03:02.648Z"
    }
   },
   "outputs": [],
   "source": [
    "DataSet['tes']['visit_date'].min() - pd.to_timedelta(2,unit='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.831626Z",
     "start_time": "2017-12-26T07:03:02.649Z"
    }
   },
   "outputs": [],
   "source": [
    "groupped = DataSet['tra'].groupby(['air_store_id', 'dow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.832336Z",
     "start_time": "2017-12-26T07:03:02.651Z"
    }
   },
   "outputs": [],
   "source": [
    "groupped.get_group(('air_00a91d42b08b08d9', 0)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.833172Z",
     "start_time": "2017-12-26T07:03:02.652Z"
    }
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for g in groupped.groups:\n",
    "    tmp_dict = {}\n",
    "    tmp_dict['id'] = g[0]\n",
    "    tmp_dict['dow'] = g[1]\n",
    "    tmp_dict['mean'] = groupped.get_group(g)['visitors'].mean()\n",
    "    records.append(tmp_dict)\n",
    "#print(records)\n",
    "df = pd.DataFrame(data= records, index= range(len(records)))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.834040Z",
     "start_time": "2017-12-26T07:03:02.653Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(DataSet['tra'][DataSet['tra']['visitors'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.835015Z",
     "start_time": "2017-12-26T07:03:02.654Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp2 = tmp1.sort_values(by= ['visit_date'])\n",
    "print(tmp2.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T07:03:08.835794Z",
     "start_time": "2017-12-26T07:03:02.657Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp2.rolling(3, min_periods= 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
