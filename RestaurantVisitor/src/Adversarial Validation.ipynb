{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T02:50:15.040000Z",
     "start_time": "2017-12-26T02:49:33.673929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============\n",
      "tra data: unique stores 829, total 252108, time elased 0.79s.\n",
      "tes data: unique stores 821, total 32019, time elased 0.15s.\n",
      "============= process date related done.\n",
      "\n",
      "add city feature done.\n",
      " ================ add count features done.\n",
      "\n",
      "add holiday type done.\n",
      "========== reset holiday done.\n",
      "\n",
      "================ join holiday, store data done.\n",
      " process reservation data done.\n",
      "\n",
      "============= join reservation data done.\n",
      "\n",
      "total groups 316 \n",
      "part 0 rolling done.\n",
      "total groups 829 \n",
      "part 1 rolling done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:297: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:298: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add time series features done.\n",
      "add date int features done.\n",
      "time elapsed 778.0834865570068s\n",
      " ============= add time series related features done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numba\n",
    "import os,sys\n",
    "import gc\n",
    "import math\n",
    "\n",
    "def LoadData(InputDir):\n",
    "    \"\"\"\"\"\"\n",
    "    ## load raw data\n",
    "    data = {\n",
    "        'tra': pd.read_csv('%s/air_visit_data.csv' % InputDir, parse_dates= ['visit_date']),\n",
    "        'as': pd.read_csv('%s/air_store_info.csv' % InputDir),\n",
    "        'hs': pd.read_csv('%s/hpg_store_info.csv' % InputDir),\n",
    "        'ar': pd.read_csv('%s/air_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'hr': pd.read_csv('%s/hpg_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'id': pd.read_csv('%s/store_id_relation.csv' % InputDir),\n",
    "        'tes': pd.read_csv('%s/sample_submission.csv' % InputDir),\n",
    "        'hol': pd.read_csv('%s/date_info.csv' % InputDir, parse_dates=['calendar_date']).rename(columns={'calendar_date': 'visit_date'})\n",
    "    }\n",
    "    return data\n",
    "\n",
    "@numba.jit\n",
    "def ApplyDayoff(VisitCols, ReserveCols):\n",
    "    \"\"\"\"\"\"\n",
    "    n = len(VisitCols)\n",
    "    result = np.zeros((n, 1), dtype= 'int8')\n",
    "    for i in range(n):\n",
    "        d = (VisitCols[i]- ReserveCols[i]).days\n",
    "        if(d > 0):\n",
    "            result[i] = d\n",
    "    return result\n",
    "\n",
    "reserve2id = {'ar': 'air', 'hr': 'hpg'}\n",
    "reserve2store = {'ar': 'as', 'hr': 'hs'}# load data set\n",
    "InputDir = '../../data/raw'\n",
    "DataSet = LoadData(InputDir)\n",
    "#### \n",
    "# date related features\n",
    "print('\\n============')\n",
    "for mod in ['tra', 'tes']:\n",
    "    start0 = time.time()\n",
    "    if (mod == 'tes'):\n",
    "        DataSet[mod]['visit_date'] = DataSet[mod]['id'].map(lambda x: str(x).split('_')[2])\n",
    "        DataSet[mod]['air_store_id'] = DataSet[mod]['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "        DataSet[mod]['visit_date'] = pd.to_datetime(DataSet[mod]['visit_date'])\n",
    "    DataSet[mod]['dow'] = DataSet[mod]['visit_date'].dt.dayofweek\n",
    "    DataSet[mod]['year'] = DataSet[mod]['visit_date'].dt.year\n",
    "    DataSet[mod]['month'] = DataSet[mod]['visit_date'].dt.month\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_date'].dt.date\n",
    "    end0 = time.time()\n",
    "    print('%s data: unique stores %s, total %s, time elased %.2fs.' %\n",
    "            (mod, len(DataSet[mod]['air_store_id'].unique()), len(DataSet[mod]['air_store_id']), (end0 - start0)))\n",
    "print('============= process date related done.\\n')\n",
    "######## store data\n",
    "# add city feature\n",
    "for mod in ['ar', 'hr']:\n",
    "    DataSet[reserve2store[mod]]['%s_city' % reserve2id[mod]] = DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]].str[:5]\n",
    "print('add city feature done.')\n",
    "# area (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_area_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g\n",
    "        ac['%s_area_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_area_name' % reserve2id[mod]])\n",
    "# genre (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_genre_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g\n",
    "        ac['%s_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_genre_name' % reserve2id[mod]])\n",
    "#  area_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_area_name' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_area_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "# city (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_city' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g\n",
    "        ac['%s_city_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_city' % reserve2id[mod]])\n",
    "#  city_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_city' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_city_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "print(' ================ add count features done.\\n')\n",
    "######### holiday data\n",
    "data = DataSet['hol']\n",
    "### add holiday days\n",
    "data['visit_date'] = data['visit_date'].dt.date\n",
    "data = data.sort_values(by= 'visit_date')\n",
    "def TagHoliday(df):\n",
    "    ''''''\n",
    "    n = len(df)\n",
    "    result = ['' for x in range(n)]\n",
    "    for i in range(n):\n",
    "        if(i == 0):\n",
    "            result[i] = 'hid_%s' % 0\n",
    "        elif((df[i] - df[i-1]).days == 1):\n",
    "            result[i] = result[i - 1]\n",
    "        else:\n",
    "            result[i] = 'hid_%s' % (int(result[i - 1].split('_')[1]) + 1)\n",
    "    return result\n",
    "holidays = data[data['holiday_flg'] == 1][['visit_date']]\n",
    "holidays['hol_l0'] = TagHoliday(holidays['visit_date'].values)\n",
    "groupped = holidays.groupby(['hol_l0'])\n",
    "recs = []\n",
    "for g in groupped.groups:\n",
    "    hol_days = {}\n",
    "    hol_days['hol_l0'] = g\n",
    "    hol_days['hol_days'] = len(groupped.get_group(g))\n",
    "    recs.append(hol_days)\n",
    "tmpdf = pd.DataFrame(data= recs, index= range(len(recs)))\n",
    "holidays = holidays.merge(tmpdf, how= 'left', on= 'hol_l0')\n",
    "data = data.merge(holidays, how= 'left', on= 'visit_date')\n",
    "data.drop(['hol_l0'], axis= 1, inplace= True)\n",
    "data['hol_days'].fillna(0, inplace= True)\n",
    "print('add holiday type done.')\n",
    "### reset holiday\n",
    "wkend_holidays = data.apply((lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "data['is_weekends'] = (data['day_of_week'] == 'Sunday') | (data['day_of_week'] == 'Saturday')\n",
    "data.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "DataSet['hol'] = data\n",
    "print('========== reset holiday done.\\n')\n",
    "######## join \n",
    "# join holiday data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    data = data.merge(DataSet['hol'], how='left', on=['visit_date'])\n",
    "    data.drop(['day_of_week', 'year'], axis=1, inplace=True)\n",
    "    DataSet[mod] = data\n",
    "# join store data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[reserve2store[rtype]], how= 'left', on= ['%s_store_id' % reserve2id[rtype]])\n",
    "    DataSet[mod] = data\n",
    "print('================ join holiday, store data done.')\n",
    "######### reservation data\n",
    "for mod in ['hr', 'ar']:\n",
    "    start1 = time.time()\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_datetime'].dt.date\n",
    "    DataSet[mod]['reserve_date'] = DataSet[mod]['reserve_datetime'].dt.date\n",
    "    DataSet[mod].drop(['reserve_datetime', 'visit_datetime'], axis= 1, inplace= True)\n",
    "    tmpdf = pd.DataFrame(data=ApplyDayoff(DataSet[mod]['visit_date'].values, DataSet[mod]['reserve_date'].values),index=DataSet[mod].index, columns=['reserve_date_diff'])\n",
    "    tmpdf = pd.concat([DataSet[mod], tmpdf], axis=1)\n",
    "    tmpdf = tmpdf.groupby(['%s_store_id' % reserve2id[mod], 'visit_date'], as_index=False).agg({'reserve_visitors': sum, 'reserve_date_diff': ['mean', 'median']})\n",
    "    tmpdf.columns = ['%s_store_id' % reserve2id[mod], \n",
    "                   'visit_date', \n",
    "                   '%s_reserved_visitors' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_mean' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_median' % reserve2id[mod]\n",
    "                  ]\n",
    "    end1 = time.time()\n",
    "    DataSet[mod] = tmpdf\n",
    "    DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]] = np.log1p(DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]])\n",
    "print(' process reservation data done.\\n')\n",
    "# join reservation data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[rtype], how= 'left', on= ['%s_store_id' % reserve2id[rtype], 'visit_date'])\n",
    "        data['%s_store_id' % reserve2id[rtype]].fillna(0, inplace= True)\n",
    "    DataSet[mod] = data\n",
    "    DataSet[mod]['reserved_visitors_sum'] = DataSet[mod]['air_reserved_visitors'] + DataSet[mod]['hpg_reserved_visitors']\n",
    "    DataSet[mod]['reserved_visitors_mean'] = (DataSet[mod]['air_reserved_visitors'] + DataSet[mod]['hpg_reserved_visitors'])/2\n",
    "    DataSet[mod]['reserved_dayoff_mean'] = (DataSet[mod]['air_reserved_dayoff_mean'] + DataSet[mod]['hpg_reserved_dayoff_mean'])/2\n",
    "    DataSet[mod]['reserved_dayoff_median'] = (DataSet[mod]['air_reserved_dayoff_median'] + DataSet[mod]['hpg_reserved_dayoff_median'])/2\n",
    "print('============= join reservation data done.\\n')\n",
    "####### time series related\n",
    "s = time.time()\n",
    "\n",
    "# mix train with test\n",
    "DataSet['tra']['is_train'] = 1\n",
    "DataSet['tes']['is_train'] = 0\n",
    "AllData = pd.concat([DataSet['tra'], DataSet['tes']], axis= 0, ignore_index= True)\n",
    "# !!! dividing into two pieces since 2016/7/1 is a corner point, update time 2017/12/22 15:45\n",
    "DataParts = {\n",
    "    '0': AllData[AllData['visit_date'] < datetime.date(2016, 7, 1)],\n",
    "    '1': AllData[AllData['visit_date'] >= datetime.date(2016, 7, 1)]\n",
    "}\n",
    "# manipulate with each of them\n",
    "@numba.jit\n",
    "def ApplySmoothWeek0(ColValues):\n",
    "    ''''''\n",
    "    n = len(ColValues)\n",
    "    result = np.zeros((n, 1), dtype= 'float')\n",
    "    alpha = 0.01\n",
    "    for i in range(n):\n",
    "        if(i < 5): # 5 weeks or 39 days\n",
    "            result[i] = 0.0#math.log1p(ColValues[i])\n",
    "        else:\n",
    "            result[i] = alpha * math.log1p(ColValues[i - 5]) + (1.0 - alpha) * math.log1p(result[i - 5])\n",
    "    return result\n",
    "\n",
    "@numba.jit\n",
    "def ApplySmoothDay0(ColValues):\n",
    "    ''''''\n",
    "    n = len(ColValues)\n",
    "    result = np.zeros((n, 1), dtype= 'float')\n",
    "    alpha = 0.8\n",
    "    for i in range(n):\n",
    "        if(i < 39): # 5 weeks or 39 days\n",
    "            result[i] = 0.0#math.log1p(ColValues[i])\n",
    "        else:\n",
    "            result[i] = alpha * math.log1p(ColValues[i - 39]) + (1.0 - alpha) * math.log1p(result[i - 39])\n",
    "    return result\n",
    "\n",
    "@numba.jit\n",
    "def ApplySmoothWeek1(ColValues):\n",
    "    ''''''\n",
    "    n = len(ColValues)\n",
    "    result = np.zeros((n, 1), dtype= 'float')\n",
    "    alpha = 0.01\n",
    "    for i in range(n):\n",
    "        if(i <= 5): # 5 weeks or 39 days\n",
    "            result[i] = 0.0#math.log1p(ColValues[i])\n",
    "        else:\n",
    "            result[i] = alpha * (ColValues[i - 5] - ColValues[i - 6]) + (1.0 - alpha) * result[i - 5]\n",
    "    return result\n",
    "\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred) ** 0.5\n",
    "\n",
    "for pidx in DataParts.keys():\n",
    "    ## rolling sum by days\n",
    "    groupped = DataParts[pidx].groupby(['air_store_id'])\n",
    "    visitor_ticks = [39, 46, 53, 60, 67]#, 74, 81, 88, 95, 102, 109, 116, 123]  # for days\n",
    "    print('total groups %s ' % len(groupped.groups))\n",
    "    dfs = []\n",
    "    for g in groupped.groups: \n",
    "        gdf = groupped.get_group(g).sort_values(by= ['visit_date'])\n",
    "        for t in visitor_ticks:\n",
    "            gdf['visitor_tick_sum_%s' % t] = np.log1p(gdf['visitors']).rolling(window= t).sum()\n",
    "            gdf['visitor_tick_sum_%s' % t].fillna(0, inplace= True)\n",
    "        dfs.append(gdf)\n",
    "    # concate\n",
    "    tmpdf = pd.concat(dfs, axis= 0, ignore_index= True)\n",
    "    join_cols = ['air_store_id', 'visit_date']\n",
    "    for i in range(len(visitor_ticks)):\n",
    "        if(i == 0):\n",
    "            continue\n",
    "        # rolling mean for one week\n",
    "        k_mean = 'visitor_rolling_%s_%s_mean' % (visitor_ticks[i], visitor_ticks[i - 1])\n",
    "        tmpdf[k_mean] = (tmpdf['visitor_tick_sum_%s' % visitor_ticks[i]] - tmpdf['visitor_tick_sum_%s' % visitor_ticks[i - 1]]) / (visitor_ticks[i] - visitor_ticks[i - 1])\n",
    "        tmpdf.loc[tmpdf[k_mean] < 0, k_mean] = 0  ## negative values exists, need to be set zero, updated 2016/12/22 20:30\n",
    "        tmpdf[k_mean].fillna(0, inplace= True)\n",
    "        join_cols.append(k_mean)\n",
    "    # merge\n",
    "    tmpdf.drop(['visitor_tick_sum_%s' % col for col in visitor_ticks], axis= 1, inplace= True)\n",
    "    DataParts[pidx] = DataParts[pidx].merge(tmpdf[join_cols], how= 'left', on= ['air_store_id', 'visit_date'])\n",
    "    print('part %s rolling done.' % pidx)\n",
    "# concat after all is done\n",
    "AllData = pd.concat([DataParts['0'], DataParts['1']], axis= 0, ignore_index= True)\n",
    "# restore\n",
    "DataSet['tra'] = AllData[AllData['is_train'] == 1]\n",
    "DataSet['tes'] = AllData[AllData['is_train'] == 0]\n",
    "DataSet['tra'].drop(['is_train'], axis= 1, inplace= True)\n",
    "DataSet['tes'].drop(['is_train'], axis= 1, inplace= True)\n",
    "del AllData\n",
    "print('add time series features done.')\n",
    "#### add date_int\n",
    "for mod in ['tra', 'tes']:\n",
    "    DataSet[mod]['date_int'] = DataSet[mod]['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "    DataSet[mod]['date_int'] = DataSet[mod]['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "print('add date int features done.')\n",
    "### add var_max_lat/var_max_long\n",
    "for mod in ['tra', 'tes']:\n",
    "    DataSet[mod]['lon_plus_lat_x'] = DataSet[mod]['longitude_x'] + DataSet[mod]['latitude_x'] \n",
    "    DataSet[mod]['var_max_long_x'] = DataSet[mod]['longitude_x'].max() - DataSet[mod]['longitude_x']\n",
    "    DataSet[mod]['var_max_lat_x'] = DataSet[mod]['latitude_x'].max() - DataSet[mod]['latitude_x']\n",
    "e = time.time()\n",
    "print('time elapsed %ss' % ((e - s) * 60))\n",
    "print(' ============= add time series related features done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T02:50:15.557387Z",
     "start_time": "2017-12-26T02:50:15.041955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features  ['air_genre_name', 'air_area_name', 'air_city', 'hpg_genre_name', 'hpg_area_name', 'hpg_city']\n"
     ]
    }
   ],
   "source": [
    "### fill nulls\n",
    "from sklearn import *\n",
    "cate_feats = ['genre_name', 'area_name', 'city']\n",
    "cate_cols = ['%s_%s' % (m, cf) for m in ['air', 'hpg'] for cf in cate_feats]\n",
    "for mod in ['tra', 'tes']:\n",
    "    for col in DataSet[mod].columns:\n",
    "        if(col in cate_cols):\n",
    "            DataSet[mod][col].fillna('unknown', inplace= True)\n",
    "        else:\n",
    "            DataSet[mod][col].fillna(0, inplace= True)\n",
    "print('Categorical features ', cate_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T02:50:16.505347Z",
     "start_time": "2017-12-26T02:50:15.559360Z"
    }
   },
   "outputs": [],
   "source": [
    "#### Label encoding for categorial features\n",
    "TrainData = DataSet['tra']\n",
    "TestData = DataSet['tes']\n",
    "for col in cate_cols:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    TrainData[col] = lbl.fit_transform(TrainData[col])\n",
    "    TestData[col] = lbl.transform(TestData[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-26T05:04:18.027548Z",
     "start_time": "2017-12-26T05:04:11.993621Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1.0), (0, 0.0), (0, 0.01), (0, 0.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (0, 0.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (0, 0.01), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (0, 0.01), (1, 1.0), (0, 0.10000000000000001), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 0.98999999999999999), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 0.98999999999999999), (1, 1.0), (1, 1.0), (1, 1.0), (1, 0.98999999999999999), (1, 1.0), (1, 1.0), (1, 0.98999999999999999), (1, 1.0), (1, 1.0), (0, 0.0), (1, 1.0), (1, 1.0), (1, 0.98999999999999999), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (0, 0.0), (1, 1.0), (1, 1.0), (0, 0.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 0.97999999999999998), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 1.0), (1, 0.98999999999999999), (1, 1.0), (1, 1.0), (1, 1.0), (1, 0.98999999999999999), (1, 1.0), (1, 0.98999999999999999), (0, 0.0), (0, 0.02)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation as CV\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "import numba\n",
    "\n",
    "#@numba.jit\n",
    "def ComputeAUC(truth, predict):\n",
    "    ''''''\n",
    "    n = len(truth)\n",
    "    #\n",
    "    pos_num = np.sum(truth)\n",
    "    neg_num = len(truth) - pos_num\n",
    "    #\n",
    "    pairs = zip(truth, predict)\n",
    "    sorted_pairs = sorted(pairs, key= lambda x: x[1])\n",
    "    sorted_truth = [s[0] for s in sorted_pairs]\n",
    "    #\n",
    "    auc = 0.0\n",
    "    x = np.zeros((n), dtype= 'float')\n",
    "    y = np.zeros((n), dtype= 'float')\n",
    "    x[0] = 1.0\n",
    "    y[0] = 1.0\n",
    "    for i in range(1, n):\n",
    "        a = (n - i - np.sum(sorted_truth[i:n]))/neg_num\n",
    "        b = np.sum(sorted_truth[i:n])/pos_num\n",
    "        x[i] = a\n",
    "        y[i] = b\n",
    "        #print(auc)\n",
    "        auc += ((y[i] + y[i - 1]) * (x[i - 1] - x[i]))/2.0\n",
    "    auc += (y[n - 1] * x[n - 1])/2.0\n",
    "    \n",
    "    return auc\n",
    "\n",
    "TrainData.reset_index(drop= True, inplace= True)\n",
    "TestData.reset_index(drop= True, inplace= True)\n",
    "\n",
    "TrainData['TARGET'] = 1\n",
    "TestData['TARGET'] = 0\n",
    "\n",
    "cols = TrainData.columns\n",
    "data = pd.concat(( TrainData, TestData[cols] ))\n",
    "\n",
    "# shuffle\n",
    "data = data.iloc[ np.random.permutation(len( data )) ]\n",
    "data.reset_index( drop = True, inplace = True )\n",
    "\n",
    "drop_cols = ['id', 'air_store_id', 'visit_date', 'visitors', 'hpg_store_id', 'fold', 'TARGET', '']\n",
    "feat_cols = [c for c in cols if c not in drop_cols]\n",
    "\n",
    "x = data[feat_cols]\n",
    "y = data['TARGET']\n",
    "\n",
    "train_examples = 280000\n",
    "x_train = x[:train_examples]\n",
    "x_test = x[train_examples:]\n",
    "y_train = y[:train_examples]\n",
    "y_test = y[train_examples:]\n",
    "\n",
    "clf = RF( n_estimators = 100, verbose = True, n_jobs = -1 )\n",
    "clf.fit( x_train, y_train )\n",
    "p = clf.predict_proba( x_test )[:,1]\n",
    "\n",
    "# clf = LR()\n",
    "# clf.fit( x_train, y_train )\n",
    "# p = clf.predict_proba( x_test )[:,1]\n",
    "\n",
    "# t = list(zip(list(y_test.values), p))\n",
    "# print(t[:100])\n",
    "# sys.exit(1)\n",
    "auc = ComputeAUC(y_test.values, p)\n",
    "print(\"AUC: %.4f\" % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
