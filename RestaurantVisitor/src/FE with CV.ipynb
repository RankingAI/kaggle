{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T14:53:42.558341Z",
     "start_time": "2017-12-25T14:52:48.709175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============\n",
      "tra data: unique stores 829, total 252108, time elased 0.85s.\n",
      "tes data: unique stores 821, total 32019, time elased 0.13s.\n",
      "============= process date related done.\n",
      "\n",
      "add city feature done.\n",
      " ================ add count features done.\n",
      "\n",
      "add holiday type done.\n",
      "========== reset holiday done.\n",
      "\n",
      "================ join holiday, store data done.\n",
      " process reservation data done.\n",
      "\n",
      "============= join reservation data done.\n",
      "\n",
      "total groups 316 \n",
      "part 0 rolling done.\n",
      "total groups 829 \n",
      "part 1 rolling done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:341: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:342: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add time series features done.\n",
      "add date int features done.\n",
      "time elapsed 1517.8231859207153s\n",
      " ============= add time series related features done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numba\n",
    "import os,sys\n",
    "import gc\n",
    "import math\n",
    "\n",
    "def LoadData(InputDir):\n",
    "    \"\"\"\"\"\"\n",
    "    ## load raw data\n",
    "    data = {\n",
    "        'tra': pd.read_csv('%s/air_visit_data.csv' % InputDir, parse_dates= ['visit_date']),\n",
    "        'as': pd.read_csv('%s/air_store_info.csv' % InputDir),\n",
    "        'hs': pd.read_csv('%s/hpg_store_info.csv' % InputDir),\n",
    "        'ar': pd.read_csv('%s/air_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'hr': pd.read_csv('%s/hpg_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'id': pd.read_csv('%s/store_id_relation.csv' % InputDir),\n",
    "        'tes': pd.read_csv('%s/sample_submission.csv' % InputDir),\n",
    "        'hol': pd.read_csv('%s/date_info.csv' % InputDir, parse_dates=['calendar_date']).rename(columns={'calendar_date': 'visit_date'})\n",
    "    }\n",
    "    return data\n",
    "\n",
    "@numba.jit\n",
    "def ApplyDayoff(VisitCols, ReserveCols):\n",
    "    \"\"\"\"\"\"\n",
    "    n = len(VisitCols)\n",
    "    result = np.zeros((n, 1), dtype= 'int8')\n",
    "    for i in range(n):\n",
    "        d = (VisitCols[i]- ReserveCols[i]).days\n",
    "        if(d > 0):\n",
    "            result[i] = d\n",
    "    return result\n",
    "\n",
    "reserve2id = {'ar': 'air', 'hr': 'hpg'}\n",
    "reserve2store = {'ar': 'as', 'hr': 'hs'}# load data set\n",
    "InputDir = '../../data/raw'\n",
    "DataSet = LoadData(InputDir)\n",
    "#### \n",
    "# date related features\n",
    "print('\\n============')\n",
    "for mod in ['tra', 'tes']:\n",
    "    start0 = time.time()\n",
    "    if (mod == 'tes'):\n",
    "        DataSet[mod]['visit_date'] = DataSet[mod]['id'].map(lambda x: str(x).split('_')[2])\n",
    "        DataSet[mod]['air_store_id'] = DataSet[mod]['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "        DataSet[mod]['visit_date'] = pd.to_datetime(DataSet[mod]['visit_date'])\n",
    "    DataSet[mod]['dow'] = DataSet[mod]['visit_date'].dt.dayofweek\n",
    "    DataSet[mod]['year'] = DataSet[mod]['visit_date'].dt.year\n",
    "    DataSet[mod]['month'] = DataSet[mod]['visit_date'].dt.month\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_date'].dt.date\n",
    "    end0 = time.time()\n",
    "    print('%s data: unique stores %s, total %s, time elased %.2fs.' %\n",
    "            (mod, len(DataSet[mod]['air_store_id'].unique()), len(DataSet[mod]['air_store_id']), (end0 - start0)))\n",
    "print('============= process date related done.\\n')\n",
    "######## store data\n",
    "# add city feature\n",
    "for mod in ['ar', 'hr']:\n",
    "    DataSet[reserve2store[mod]]['%s_city' % reserve2id[mod]] = DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]].str[:5]\n",
    "print('add city feature done.')\n",
    "# # divide area/genre into ten pieces\n",
    "# for mod in ['as']:\n",
    "#     DataSet[mod]['air_genre_name'] = DataSet[mod]['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "#     DataSet[mod]['air_area_name'] = DataSet[mod]['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "#     for i in range(3):\n",
    "#         DataSet[mod]['air_genre_name'+str(i)] = DataSet[mod]['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else np.nan)\n",
    "#         DataSet[mod]['air_area_name'+str(i)] = DataSet[mod]['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else np.nan)\n",
    "# print('dividing area/genre into ten pieces done.')\n",
    "# area (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_area_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g\n",
    "        ac['%s_area_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        #ac['%s_area_store_ratio' % reserve2id[mod]] = ac['%s_area_store_count' % reserve2id[mod]]/len(DataSet[reserve2store[mod]])\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_area_name' % reserve2id[mod]])\n",
    "# genre (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_genre_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g\n",
    "        ac['%s_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_genre_name' % reserve2id[mod]])\n",
    "#  area_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_area_name' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_area_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "# city (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_city' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g\n",
    "        ac['%s_city_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        #ac['%s_area_store_ratio' % reserve2id[mod]] = ac['%s_area_store_count' % reserve2id[mod]]/len(DataSet[reserve2store[mod]])\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_city' % reserve2id[mod]])\n",
    "#  city_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_city' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_city_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "print(' ================ add count features done.\\n')\n",
    "######### holiday data\n",
    "data = DataSet['hol']\n",
    "### add holiday days\n",
    "data['visit_date'] = data['visit_date'].dt.date\n",
    "data = data.sort_values(by= 'visit_date')\n",
    "def TagHoliday(df):\n",
    "    ''''''\n",
    "    n = len(df)\n",
    "    result = ['' for x in range(n)]\n",
    "    for i in range(n):\n",
    "        if(i == 0):\n",
    "            result[i] = 'hid_%s' % 0\n",
    "        elif((df[i] - df[i-1]).days == 1):\n",
    "            result[i] = result[i - 1]\n",
    "        else:\n",
    "            result[i] = 'hid_%s' % (int(result[i - 1].split('_')[1]) + 1)\n",
    "    return result\n",
    "holidays = data[data['holiday_flg'] == 1][['visit_date']]\n",
    "holidays['hol_l0'] = TagHoliday(holidays['visit_date'].values)\n",
    "groupped = holidays.groupby(['hol_l0'])\n",
    "recs = []\n",
    "for g in groupped.groups:\n",
    "    hol_days = {}\n",
    "    hol_days['hol_l0'] = g\n",
    "    hol_days['hol_days'] = len(groupped.get_group(g))\n",
    "    recs.append(hol_days)\n",
    "tmpdf = pd.DataFrame(data= recs, index= range(len(recs)))\n",
    "holidays = holidays.merge(tmpdf, how= 'left', on= 'hol_l0')\n",
    "data = data.merge(holidays, how= 'left', on= 'visit_date')\n",
    "data.drop(['hol_l0'], axis= 1, inplace= True)\n",
    "data['hol_days'].fillna(0, inplace= True)\n",
    "print('add holiday type done.')\n",
    "### reset holiday\n",
    "wkend_holidays = data.apply((lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "data['is_weekends'] = (data['day_of_week'] == 'Sunday') | (data['day_of_week'] == 'Saturday')\n",
    "data.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "DataSet['hol'] = data\n",
    "print('========== reset holiday done.\\n')\n",
    "######## join \n",
    "# join holiday data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    data = data.merge(DataSet['hol'], how='left', on=['visit_date'])\n",
    "    data.drop(['day_of_week', 'year'], axis=1, inplace=True)\n",
    "    DataSet[mod] = data\n",
    "# join store data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[reserve2store[rtype]], how= 'left', on= ['%s_store_id' % reserve2id[rtype]])\n",
    "    DataSet[mod] = data\n",
    "print('================ join holiday, store data done.')\n",
    "######### reservation data\n",
    "for mod in ['hr', 'ar']:\n",
    "    start1 = time.time()\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_datetime'].dt.date\n",
    "    DataSet[mod]['reserve_date'] = DataSet[mod]['reserve_datetime'].dt.date\n",
    "    DataSet[mod].drop(['reserve_datetime', 'visit_datetime'], axis= 1, inplace= True)\n",
    "    tmpdf = pd.DataFrame(data=ApplyDayoff(DataSet[mod]['visit_date'].values, DataSet[mod]['reserve_date'].values),index=DataSet[mod].index, columns=['reserve_date_diff'])\n",
    "    tmpdf = pd.concat([DataSet[mod], tmpdf], axis=1)\n",
    "    tmpdf = tmpdf.groupby(['%s_store_id' % reserve2id[mod], 'visit_date'], as_index=False).agg({'reserve_visitors': sum, 'reserve_date_diff': ['mean', 'median']})\n",
    "    tmpdf.columns = ['%s_store_id' % reserve2id[mod], \n",
    "                   'visit_date', \n",
    "                   '%s_reserved_visitors' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_mean' % reserve2id[mod], \n",
    "                   '%s_reserved_dayoff_median' % reserve2id[mod]\n",
    "                  ]\n",
    "    end1 = time.time()\n",
    "    DataSet[mod] = tmpdf\n",
    "    DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]] = np.log1p(DataSet[mod]['%s_reserved_visitors' % reserve2id[mod]])\n",
    "print(' process reservation data done.\\n')\n",
    "# join reservation data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if((rtype == 'hr') & (('%s_store_id' % reserve2id[rtype]) not in data.columns)):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[rtype], how= 'left', on= ['%s_store_id' % reserve2id[rtype], 'visit_date'])\n",
    "        data['%s_store_id' % reserve2id[rtype]].fillna(0, inplace= True)\n",
    "    DataSet[mod] = data\n",
    "    DataSet[mod]['reserved_visitors_sum'] = DataSet[mod]['air_reserved_visitors'] + DataSet[mod]['hpg_reserved_visitors']\n",
    "    DataSet[mod]['reserved_visitors_mean'] = (DataSet[mod]['air_reserved_visitors'] + DataSet[mod]['hpg_reserved_visitors'])/2\n",
    "    DataSet[mod]['reserved_dayoff_mean'] = (DataSet[mod]['air_reserved_dayoff_mean'] + DataSet[mod]['hpg_reserved_dayoff_mean'])/2\n",
    "    DataSet[mod]['reserved_dayoff_median'] = (DataSet[mod]['air_reserved_dayoff_median'] + DataSet[mod]['hpg_reserved_dayoff_median'])/2\n",
    "print('============= join reservation data done.\\n')\n",
    "####### time series related\n",
    "s = time.time()\n",
    "\n",
    "# mix train with test\n",
    "DataSet['tra']['is_train'] = 1\n",
    "DataSet['tes']['is_train'] = 0\n",
    "AllData = pd.concat([DataSet['tra'], DataSet['tes']], axis= 0, ignore_index= True)\n",
    "# !!! dividing into two pieces since 2016/7/1 is a corner point, update time 2017/12/22 15:45\n",
    "DataParts = {\n",
    "    '0': AllData[AllData['visit_date'] < datetime.date(2016, 7, 1)],\n",
    "    '1': AllData[AllData['visit_date'] >= datetime.date(2016, 7, 1)]\n",
    "}\n",
    "# manipulate with each of them\n",
    "@numba.jit\n",
    "def ApplySmoothWeek0(ColValues):\n",
    "    ''''''\n",
    "    n = len(ColValues)\n",
    "    result = np.zeros((n, 1), dtype= 'float')\n",
    "    alpha = 0.01\n",
    "    for i in range(n):\n",
    "        if(i < 5): # 5 weeks or 39 days\n",
    "            result[i] = 0.0#math.log1p(ColValues[i])\n",
    "        else:\n",
    "            result[i] = alpha * math.log1p(ColValues[i - 5]) + (1.0 - alpha) * math.log1p(result[i - 5])\n",
    "    return result\n",
    "\n",
    "@numba.jit\n",
    "def ApplySmoothDay0(ColValues):\n",
    "    ''''''\n",
    "    n = len(ColValues)\n",
    "    result = np.zeros((n, 1), dtype= 'float')\n",
    "    alpha = 0.8\n",
    "    for i in range(n):\n",
    "        if(i < 39): # 5 weeks or 39 days\n",
    "            result[i] = 0.0#math.log1p(ColValues[i])\n",
    "        else:\n",
    "            result[i] = alpha * math.log1p(ColValues[i - 39]) + (1.0 - alpha) * math.log1p(result[i - 39])\n",
    "    return result\n",
    "\n",
    "@numba.jit\n",
    "def ApplySmoothWeek1(ColValues):\n",
    "    ''''''\n",
    "    n = len(ColValues)\n",
    "    result = np.zeros((n, 1), dtype= 'float')\n",
    "    alpha = 0.01\n",
    "    for i in range(n):\n",
    "        if(i <= 5): # 5 weeks or 39 days\n",
    "            result[i] = 0.0#math.log1p(ColValues[i])\n",
    "        else:\n",
    "            result[i] = alpha * (ColValues[i - 5] - ColValues[i - 6]) + (1.0 - alpha) * result[i - 5]\n",
    "    return result\n",
    "\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred) ** 0.5\n",
    "\n",
    "for pidx in DataParts.keys():\n",
    "    ## exponential smoothing by weeks\n",
    "#     groupped = DataParts[pidx].groupby(['air_store_id', 'dow'])\n",
    "#     rmsle = 0\n",
    "#     dfs = []\n",
    "#     for g in groupped.groups: \n",
    "#         gdf = groupped.get_group(g).sort_values(by= ['visit_date'])\n",
    "#         gdf['exp_smooth_week_0'] = ApplySmoothWeek0(gdf['visitors'].values)\n",
    "#         #gdf['exp_smooth_week_1'] = ApplySmoothWeek1(gdf['exp_smooth_week_0'].values)\n",
    "#         dfs.append(gdf)\n",
    "#     tmpdf = pd.concat(dfs, axis= 0, ignore_index= True)\n",
    "#     DataParts[pidx] = DataParts[pidx].merge(tmpdf[['air_store_id', 'visit_date', 'exp_smooth_week_0']], how= 'left', on= ['air_store_id', 'visit_date'])\n",
    "#     ## exponential smoothing by days\n",
    "#     groupped = DataParts[pidx].groupby(['air_store_id'])\n",
    "#     rmsle = 0\n",
    "#     dfs = []\n",
    "#     for g in groupped.groups: \n",
    "#         gdf = groupped.get_group(g).sort_values(by= ['visit_date'])\n",
    "#         gdf['exp_smooth_day_0'] = ApplySmoothDay0(gdf['visitors'].values)\n",
    "#         #gdf['exp_smooth_week_1'] = ApplySmoothWeek1(gdf['exp_smooth_week_0'].values)\n",
    "#         dfs.append(gdf)\n",
    "#     tmpdf = pd.concat(dfs, axis= 0, ignore_index= True)\n",
    "#     DataParts[pidx] = DataParts[pidx].merge(tmpdf[['air_store_id', 'visit_date', 'exp_smooth_day_0']], how= 'left', on= ['air_store_id', 'visit_date'])\n",
    "#     print('part %s exponential smoothing done.' % pidx)\n",
    "    ## rolling sum by days\n",
    "    groupped = DataParts[pidx].groupby(['air_store_id'])\n",
    "    visitor_ticks = [39, 46, 53, 60, 67, 74, 81]  # for days\n",
    "    #visitor_ticks = [5, 10, 15, 20, 25]  # for weeks\n",
    "    #reservation_ticks = [7, 14, 21, 28]\n",
    "    print('total groups %s ' % len(groupped.groups))\n",
    "    dfs = []\n",
    "    for g in groupped.groups: \n",
    "        gdf = groupped.get_group(g).sort_values(by= ['visit_date'])\n",
    "        for t in visitor_ticks:\n",
    "            gdf['visitor_tick_sum_%s' % t] = np.log1p(gdf['visitors']).rolling(window= t).sum()\n",
    "            gdf['visitor_tick_sum_%s' % t].fillna(0, inplace= True)\n",
    "#             gdf['visitor_tick_std_%s' % t] = np.log1p(gdf['visitors']).rolling(window= t).std()\n",
    "#             gdf['visitor_tick_std_%s' % t].fillna(0, inplace= True)\n",
    "#         gdf['holiday_rolling_3'] = gdf['holiday_flg'].rolling(window= 3).sum()\n",
    "#         gdf['holiday_rolling_3'].fillna(0, inplace= True)\n",
    "        dfs.append(gdf)\n",
    "    # concate\n",
    "    tmpdf = pd.concat(dfs, axis= 0, ignore_index= True)\n",
    "    join_cols = ['air_store_id', 'visit_date', 'holiday_rolling_3']\n",
    "    for i in range(len(visitor_ticks)):\n",
    "        if(i == 0):\n",
    "            continue\n",
    "        # rolling mean\n",
    "        k_mean = 'visitor_rolling_%s_%s_mean' % (visitor_ticks[i], visitor_ticks[i - 1])\n",
    "        tmpdf[k_mean] = (tmpdf['visitor_tick_sum_%s' % visitor_ticks[i]] - tmpdf['visitor_tick_sum_%s' % visitor_ticks[i - 1]]) / (visitor_ticks[i] - visitor_ticks[i - 1])\n",
    "        tmpdf.loc[tmpdf[k_mean] < 0, k_mean] = 0  ## negative values exists, need to be set zero, updated 2016/12/22 20:30\n",
    "        tmpdf[k_mean].fillna(0, inplace= True)\n",
    "        join_cols.append(k_std)\n",
    "#         # rolling std\n",
    "#         k_std = 'visitor_rolling_%s_%s_std' % (visitor_ticks[i], visitor_ticks[i - 1])\n",
    "#         tmpdf[k_std] = (tmpdf['visitor_tick_std_%s' % visitor_ticks[i]] - tmpdf['visitor_tick_std_%s' % visitor_ticks[i - 1]]) / (visitor_ticks[i] - visitor_ticks[i - 1])\n",
    "#         tmpdf.loc[tmpdf[k_std] < 0, k_std] = 0  ## negative values exists, need to be set zero, updated 2016/12/22 20:30\n",
    "#         tmpdf[k_std].fillna(0, inplace= True)\n",
    "#         join_cols.append(k_mean)\n",
    "    # merge\n",
    "    tmpdf.drop(['visitor_tick_sum_%s' % col for col in visitor_ticks], axis= 1, inplace= True)\n",
    "#     tmpdf.drop(['visitor_tick_std_%s' % col for col in visitor_ticks], axis= 1, inplace= True)\n",
    "    DataParts[pidx] = DataParts[pidx].merge(tmpdf[join_cols], how= 'left', on= ['air_store_id', 'visit_date'])\n",
    "    print('part %s rolling done.' % pidx)\n",
    "# concat after all is done\n",
    "AllData = pd.concat([DataParts['0'], DataParts['1']], axis= 0, ignore_index= True)\n",
    "# restore\n",
    "DataSet['tra'] = AllData[AllData['is_train'] == 1]\n",
    "DataSet['tes'] = AllData[AllData['is_train'] == 0]\n",
    "DataSet['tra'].drop(['is_train'], axis= 1, inplace= True)\n",
    "DataSet['tes'].drop(['is_train'], axis= 1, inplace= True)\n",
    "del AllData\n",
    "print('add time series features done.')\n",
    "#### add date_int\n",
    "for mod in ['tra', 'tes']:\n",
    "    DataSet[mod]['date_int'] = DataSet[mod]['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "    DataSet[mod]['date_int'] = DataSet[mod]['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "print('add date int features done.')\n",
    "#vdf = DataSet['tra'][DataSet['tra']['air_store_id'] == 'air_ba937bf13d40fb24']\n",
    "#print(vdf[join_cols].head(200))\n",
    "### add var_max_lat/var_max_long\n",
    "for mod in ['tra', 'tes']:\n",
    "    DataSet[mod]['lon_plus_lat_x'] = DataSet[mod]['longitude_x'] + DataSet[mod]['latitude_x'] \n",
    "#     DataSet[mod]['lon_plus_lat_y'] = DataSet[mod]['longitude_y'] + DataSet[mod]['latitude_y'] \n",
    "    DataSet[mod]['var_max_long_x'] = DataSet[mod]['longitude_x'].max() - DataSet[mod]['longitude_x']\n",
    "    DataSet[mod]['var_max_lat_x'] = DataSet[mod]['latitude_x'].max() - DataSet[mod]['latitude_x']\n",
    "#     DataSet[mod]['var_max_long_y'] = DataSet[mod]['longitude_y'].max() - DataSet[mod]['longitude_y']\n",
    "#     DataSet[mod]['var_max_lat_y'] = DataSet[mod]['latitude_y'].max() - DataSet[mod]['latitude_y']\n",
    "e = time.time()\n",
    "print('time elapsed %ss' % ((e - s) * 60))\n",
    "print(' ============= add time series related features done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T14:53:42.765051Z",
     "start_time": "2017-12-25T14:53:42.559996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features  ['air_genre_name', 'air_area_name', 'air_city', 'hpg_genre_name', 'hpg_area_name', 'hpg_city']\n"
     ]
    }
   ],
   "source": [
    "### fill nulls\n",
    "from sklearn import *\n",
    "cate_feats = ['genre_name', 'area_name', 'city']\n",
    "cate_cols = ['%s_%s' % (m, cf) for m in ['air', 'hpg'] for cf in cate_feats]\n",
    "for mod in ['tra', 'tes']:\n",
    "    for col in DataSet[mod].columns:\n",
    "        if(col in cate_cols):\n",
    "            DataSet[mod][col].fillna('unknown', inplace= True)\n",
    "        else:\n",
    "            DataSet[mod][col].fillna(0, inplace= True)\n",
    "print('Categorical features ', cate_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T14:53:43.710520Z",
     "start_time": "2017-12-25T14:53:42.766611Z"
    }
   },
   "outputs": [],
   "source": [
    "#### Label encoding for categorial features\n",
    "TrainData = DataSet['tra']\n",
    "TestData = DataSet['tes']\n",
    "for col in cate_cols:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    TrainData[col] = lbl.fit_transform(TrainData[col])\n",
    "    TestData[col] = lbl.transform(TestData[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T14:54:24.156431Z",
     "start_time": "2017-12-25T14:53:43.712290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 done.\n",
      "Fold 1 done.\n",
      "Fold 2 done.\n",
      "Fold 3 done.\n",
      "Fold 4 done.\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred) ** 0.5\n",
    "## !!! add corner tag since 2016/7/1 is a corner point, update time 2017/12/22 15:45\n",
    "TrainData['is_up_corner'] = TrainData['visit_date'] < datetime.date(2016, 7, 1)\n",
    "TestData['is_up_corner'] = TestData['visit_date'] < datetime.date(2016, 7, 1)\n",
    "# split TrainData into train and holdout with random strategy\n",
    "np.random.seed(2017)\n",
    "msk = np.random.rand(len(TrainData)) < 0.1\n",
    "holdout = TrainData[msk]\n",
    "train = TrainData[~msk]\n",
    "test = TestData\n",
    "# Set up folds\n",
    "K = 5\n",
    "kf = model_selection.KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(1)\n",
    "OutputDir = '../../data/l0'\n",
    "if(os.path.exists('%s/kfold' % OutputDir) == False):\n",
    "    os.makedirs('%s/kfold' % OutputDir)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "    FoldTrain, FoldValid = train.iloc[train_index].copy(), train.iloc[test_index].copy()\n",
    "    FoldHoldout = holdout.copy()\n",
    "    FoldTest = test.copy()\n",
    "    FoldTrain['visitors'] = np.log1p(FoldTrain['visitors'])\n",
    "    FoldValid['visitors'] = np.log1p(FoldValid['visitors'])\n",
    "    FoldHoldout['visitors'] = np.log1p(FoldHoldout['visitors'])\n",
    "    FoldTest['visitors'] = np.log1p(FoldTest['visitors'])\n",
    "    #### dependent features which is extreemly subtle to data-leak\n",
    "    # percentiles features\n",
    "    tickles = ['mean', 'median', 'max', 'min', 'count']\n",
    "    for feat in ['air_store_id']:\n",
    "        gkeys = [feat, 'dow', 'is_up_corner']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'visitors': tickles})    \n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### data-leak, prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(0, inplace= True)\n",
    "        FoldHoldout.fillna(0, inplace= True)\n",
    "        FoldTest.fillna(0, inplace= True)\n",
    "    for feat in ['air_city']:\n",
    "        gkeys = [feat, 'air_genre_name', 'is_up_corner']\n",
    "        gprefix = '_'.join(gkeys)\n",
    "        TmpDOW = FoldTrain.groupby(gkeys, as_index= False).agg({'visitors': tickles})    \n",
    "        tmpcols = gkeys.copy()\n",
    "        tmpcols.extend(['%s_%s' % (gprefix, m) for m in tickles])\n",
    "        TmpDOW.columns = tmpcols\n",
    "        #FoldTrain = FoldTrain.merge(TmpDOW, how= 'left', on=gkeys) #### data-leak, prone to be overfitted\n",
    "        FoldValid = FoldValid.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldHoldout = FoldHoldout.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldTest = FoldTest.merge(TmpDOW, how= 'left', on=gkeys)\n",
    "        FoldValid.fillna(0, inplace= True)\n",
    "        FoldHoldout.fillna(0, inplace= True)\n",
    "        FoldTest.fillna(0, inplace= True)\n",
    "    FoldOutputDir = '%s/kfold/%s' % (OutputDir, i)\n",
    "    if(os.path.exists(FoldOutputDir) == False):\n",
    "        os.makedirs(FoldOutputDir)\n",
    "    #FoldTrain.to_csv('%s/train.csv' % FoldOutputDir)\n",
    "    FoldValid.to_csv('%s/valid.csv' % FoldOutputDir, index= False)\n",
    "    FoldHoldout.to_csv('%s/holdout.csv' % FoldOutputDir, index= False)\n",
    "    FoldTest.to_csv('%s/test.csv' % FoldOutputDir, index= False)\n",
    "    \n",
    "    print('Fold %s done.' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
