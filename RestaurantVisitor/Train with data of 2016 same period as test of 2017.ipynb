{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T07:23:04.421455Z",
     "start_time": "2017-12-18T07:22:56.513451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============\n",
      "tra data: unique stores 829, total 252108, time elased 0.76s.\n",
      "tes data: unique stores 821, total 32019, time elased 0.14s.\n",
      "\n",
      "=============\n",
      "process date done, time consumed 0.14.\n",
      "\n",
      "size of tra is 252108\n",
      "\n",
      "size of tes is 32019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numba\n",
    "import os,sys\n",
    "\n",
    "def LoadData(InputDir):\n",
    "    \"\"\"\"\"\"\n",
    "    ## load raw data\n",
    "    data = {\n",
    "        'tra': pd.read_csv('%s/air_visit_data.csv' % InputDir, parse_dates= ['visit_date']),\n",
    "        'as': pd.read_csv('%s/air_store_info.csv' % InputDir),\n",
    "        'hs': pd.read_csv('%s/hpg_store_info.csv' % InputDir),\n",
    "        'ar': pd.read_csv('%s/air_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'hr': pd.read_csv('%s/hpg_reserve.csv' % InputDir, parse_dates= ['visit_datetime', 'reserve_datetime']),\n",
    "        'id': pd.read_csv('%s/store_id_relation.csv' % InputDir),\n",
    "        'tes': pd.read_csv('%s/sample_submission.csv' % InputDir),\n",
    "        'hol': pd.read_csv('%s/date_info.csv' % InputDir, parse_dates=['calendar_date']).rename(columns={'calendar_date': 'visit_date'})\n",
    "    }\n",
    "    return data\n",
    "\n",
    "@numba.jit\n",
    "def ApplyDayoff(VisitCols, ReserveCols):\n",
    "    \"\"\"\"\"\"\n",
    "    n = len(VisitCols)\n",
    "    result = np.zeros((n, 1), dtype= 'int8')\n",
    "    for i in range(n):\n",
    "        result[i] = (VisitCols[i]- ReserveCols[i]).days\n",
    "    return result\n",
    "\n",
    "reserve2id = {'ar': 'air', 'hr': 'hpg'}\n",
    "reserve2store = {'ar': 'as', 'hr': 'hs'}# load data set\n",
    "InputDir = '../../data/raw'\n",
    "DataSet = LoadData(InputDir)\n",
    "#### \n",
    "# date related features\n",
    "print('\\n============')\n",
    "for mod in ['tra', 'tes']:\n",
    "    start0 = time.time()\n",
    "    if (mod == 'tes'):\n",
    "        DataSet[mod]['visit_date'] = DataSet[mod]['id'].map(lambda x: str(x).split('_')[2])\n",
    "        DataSet[mod]['air_store_id'] = DataSet[mod]['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "        DataSet[mod]['visit_date'] = pd.to_datetime(DataSet[mod]['visit_date'])\n",
    "    DataSet[mod]['dow'] = DataSet[mod]['visit_date'].dt.dayofweek\n",
    "    DataSet[mod]['year'] = DataSet[mod]['visit_date'].dt.year\n",
    "    DataSet[mod]['month'] = DataSet[mod]['visit_date'].dt.month\n",
    "    DataSet[mod]['visit_date'] = DataSet[mod]['visit_date'].dt.date\n",
    "    end0 = time.time()\n",
    "    print('%s data: unique stores %s, total %s, time elased %.2fs.' %\n",
    "            (mod, len(DataSet[mod]['air_store_id'].unique()), len(DataSet[mod]['air_store_id']), (end0 - start0)))\n",
    "print('')\n",
    "# for reservation data\n",
    "# for mod in ['hr', 'ar']:\n",
    "#     start1 = time.time()\n",
    "#     DataSet[mod]['visit_date'] = DataSet[mod]['visit_datetime'].dt.date\n",
    "#     DataSet[mod].drop(['visit_datetime'], axis= 1, inplace= True)\n",
    "#     DataSet[mod]['reserve_date'] = DataSet[mod]['reserve_datetime'].dt.date\n",
    "#     DataSet[mod].drop(['reserve_datetime'], axis= 1, inplace= True)\n",
    "#     end1 = time.time()\n",
    "#     print('time-consuming part %.2f.' % (end1 - start1))\n",
    "\n",
    "DataSet['hol']['visit_date'] = DataSet['hol']['visit_date'].dt.date\n",
    "end0 = time.time()\n",
    "print('=============')\n",
    "print('process date done, time consumed %.2f.\\n' % (end0 - start0))\n",
    "######## store data\n",
    "# add city feature\n",
    "for mod in ['ar', 'hr']:\n",
    "    DataSet[reserve2store[mod]]['%s_city' % reserve2id[mod]] = DataSet[reserve2store[mod]]['%s_area_name' % reserve2id[mod]].str[:5]\n",
    "# area (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_area_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g\n",
    "        ac['%s_area_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        #ac['%s_area_store_ratio' % reserve2id[mod]] = ac['%s_area_store_count' % reserve2id[mod]]/len(DataSet[reserve2store[mod]])\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_area_name' % reserve2id[mod]])\n",
    "# genre (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_genre_name' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g\n",
    "        ac['%s_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_genre_name' % reserve2id[mod]])\n",
    "#  area_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_area_name' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_area_name' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_area_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "# city (store)count\n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(['%s_city' % reserve2id[mod]])\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g\n",
    "        ac['%s_city_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        #ac['%s_area_store_ratio' % reserve2id[mod]] = ac['%s_area_store_count' % reserve2id[mod]]/len(DataSet[reserve2store[mod]])\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= ['%s_city' % reserve2id[mod]])\n",
    "#  city_genre (store) count \n",
    "for mod in ['ar', 'hr']:\n",
    "    rec = []\n",
    "    groupby_keys = ['%s_city' % reserve2id[mod], '%s_genre_name' % reserve2id[mod]]\n",
    "    groupped = DataSet[reserve2store[mod]].groupby(groupby_keys)\n",
    "    for g in groupped.groups:\n",
    "        ac = {}\n",
    "        ac['%s_city' % reserve2id[mod]] = g[0]\n",
    "        ac['%s_genre_name' % reserve2id[mod]] = g[1]\n",
    "        ac['%s_city_genre_store_count' % reserve2id[mod]] = len(groupped.get_group(g)['%s_store_id' % reserve2id[mod]].unique())\n",
    "        rec.append(ac)\n",
    "    tmpdf = pd.DataFrame(data= rec, index= range(len(rec)))\n",
    "    DataSet[reserve2store[mod]] = DataSet[reserve2store[mod]].merge(tmpdf, how= 'left', on= groupby_keys)\n",
    "######### holiday data\n",
    "data = DataSet['hol']\n",
    "data = data.sort_values(by= 'visit_date')\n",
    "def TagHoliday(df):\n",
    "    ''''''\n",
    "    n = len(df)\n",
    "    result = ['' for x in range(n)]\n",
    "    for i in range(n):\n",
    "        if(i == 0):\n",
    "            result[i] = 'hid_%s' % 0\n",
    "        elif((df[i] - df[i-1]).days == 1):\n",
    "            result[i] = result[i - 1]\n",
    "        else:\n",
    "            result[i] = 'hid_%s' % (int(result[i - 1].split('_')[1]) + 1)\n",
    "    return result\n",
    "holidays = data[data['holiday_flg'] == 1][['visit_date']]\n",
    "holidays['hol_l0'] = TagHoliday(holidays['visit_date'].values)\n",
    "groupped = holidays.groupby(['hol_l0'])\n",
    "recs = []\n",
    "for g in groupped.groups:\n",
    "    hol_days = {}\n",
    "    hol_days['hol_l0'] = g\n",
    "    hol_days['hol_days'] = len(groupped.get_group(g))\n",
    "    recs.append(hol_days)\n",
    "tmpdf = pd.DataFrame(data= recs, index= range(len(recs)))\n",
    "holidays = holidays.merge(tmpdf, how= 'left', on= 'hol_l0')\n",
    "data = data.merge(holidays, how= 'left', on= 'visit_date')\n",
    "#data['hol_l0'] = data['hol_l0'].fillna('hid_-1')\n",
    "data.drop(['hol_l0'], axis= 1, inplace= True)\n",
    "data['hol_days'] = data['hol_days'].fillna(0)\n",
    "DataSet['hol'] = data\n",
    "######## join begins\n",
    "# join holiday data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    data = data.merge(DataSet['hol'], how='left', on=['visit_date'])\n",
    "    data.drop(['day_of_week', 'year'], axis=1, inplace=True)\n",
    "    DataSet[mod] = data\n",
    "# join (air)store data\n",
    "for mod in ['tra', 'tes']:\n",
    "    data = DataSet[mod]\n",
    "    print('size of %s is %s\\n' % (mod, len(data)))\n",
    "    for rtype in ['ar', 'hr']: \n",
    "        if(rtype == 'hr'):\n",
    "            data = data.merge(DataSet['id'], how= 'left', on= ['air_store_id'])\n",
    "        data = data.merge(DataSet[reserve2store[rtype]], how= 'left', on= ['%s_store_id' % reserve2id[rtype]])\n",
    "        data.drop(['latitude', 'longitude'], axis= 1, inplace= True)\n",
    "    #data.fillna(-1)\n",
    "    DataSet[mod] = data\n",
    "    #print(DataSet[mod].head())\n",
    "    #print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T07:23:09.244295Z",
     "start_time": "2017-12-18T07:23:08.137744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features  ['air_genre_name', 'air_area_name', 'air_city', 'hpg_genre_name', 'hpg_area_name', 'hpg_city']\n"
     ]
    }
   ],
   "source": [
    "#### encoding for categorial features\n",
    "from sklearn import *\n",
    "\n",
    "cate_feats = ['genre_name', 'area_name', 'city']\n",
    "cate_cols = ['%s_%s' % (m, cf) for m in ['air', 'hpg'] for cf in cate_feats]\n",
    "for mod in ['tra', 'tes']:\n",
    "    for col in DataSet[mod].columns:\n",
    "        if(col in cate_cols):\n",
    "            DataSet[mod][col].fillna('unknown', inplace= True)\n",
    "        else:\n",
    "            DataSet[mod][col].fillna(-1, inplace= True)\n",
    "print('Categorical features ', cate_cols)\n",
    "TrainData = DataSet['tra']\n",
    "TestData = DataSet['tes']\n",
    "for col in cate_cols:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    TrainData[col] = lbl.fit_transform(TrainData[col])\n",
    "    TestData[col] = lbl.transform(TestData[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T07:23:22.683282Z",
     "start_time": "2017-12-18T07:23:17.748592Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features  ['dow', 'month', 'holiday_flg', 'hol_days', 'air_genre_name', 'air_area_name', 'air_city', 'air_area_store_count', 'air_genre_store_count', 'air_area_genre_store_count', 'air_city_store_count', 'air_city_genre_store_count', 'hpg_genre_name', 'hpg_area_name', 'hpg_city', 'hpg_area_store_count', 'hpg_genre_store_count', 'hpg_area_genre_store_count', 'hpg_city_store_count', 'hpg_city_genre_store_count']\n",
      "fold 0: valid score 0.658411, train lenght 4691, valid length 8142\n",
      "fold 1: valid score 0.641554, train lenght 12833, valid length 7663\n",
      "fold 2: valid score 0.644245, train lenght 20496, valid length 8319\n",
      "\n",
      "======================\n",
      "CV score 0.648070, Holdout score 0.660875, Elapsed time: 3.00s\n",
      "======================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "import sys,os\n",
    "\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred) ** 0.5\n",
    "\n",
    "## choose 2016/1/1 - 2016/4/22 as train, then 2016/4/22 - 2016/5/31 as test\n",
    "## split train into 3 folds\n",
    "K = 3\n",
    "test = TrainData[(TrainData['visit_date'] >= datetime.date(2016, 4, 22)) & \n",
    "                    (TrainData['visit_date'] < datetime.date(2016, 5, 31)) ]\n",
    "train = TrainData[TrainData['visit_date'] < datetime.date(2016, 4, 22)]\n",
    "conditions = [\n",
    "    train['visit_date'] < datetime.date(2016, 1, 22),\n",
    "    train['visit_date'] < datetime.date(2016, 2, 22),\n",
    "    train['visit_date'] < datetime.date(2016, 3, 22),\n",
    "    train['visit_date'] < datetime.date(2016, 4, 22)\n",
    "]\n",
    "choices = list(range(K + 1))\n",
    "train['fold'] = np.select(conditions, choices, default= -1)\n",
    "params = {\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"objective\": \"regression_l2\",\n",
    "    \"lambda_l2\": 10,\n",
    "\n",
    "    \"num_iterations\": 100,\n",
    "    \"learning_rate\": 0.2,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"max_depth\": 20, \n",
    "\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.85,\n",
    "    \"bagging_freq\": 12, \n",
    "    \"min_hessian\": 0.001,\n",
    "\n",
    "    \"max_bin\": 127,\n",
    "}\n",
    "## 3-Fold \n",
    "col = [c for c in TrainData.columns if c not in ['id', 'air_store_id', 'visit_date', 'visitors', 'hpg_store_id', 'fold']]\n",
    "print('All features ', col)\n",
    "start_time = datetime.datetime.now()\n",
    "cv_score = .0\n",
    "test_score = .0\n",
    "for i in range(K):\n",
    "    # Create data for this fold\n",
    "    X_train = train[train['fold'] <= i][col]\n",
    "    y_train = np.log1p(train[train['fold'] <= i]['visitors'])\n",
    "    X_valid = train[train['fold'] == i+1][col]\n",
    "    y_valid = np.log1p(train[train['fold'] == i+1]['visitors'])\n",
    "    # train with fold data\n",
    "    d_cv = lightgbm.Dataset(X_train, label= y_train.values, max_bin= params['max_bin'], silent= True, free_raw_data= True)\n",
    "    model = lightgbm.train(params, d_cv)\n",
    "    # for valid\n",
    "    pred = model.predict(X_valid)\n",
    "    rmsle_valid = RMSLE(y_valid, pred)\n",
    "    cv_score += rmsle_valid\n",
    "\n",
    "    print('fold %s: valid score %.6f, train lenght %s, valid length %s' % (i, rmsle_valid, len(X_train), len(X_valid)))\n",
    "\n",
    "# training with whole data of 2016/1/1 - 2016/4/22\n",
    "X_test = test[col]\n",
    "y_test = np.log1p(test['visitors'])\n",
    "d_cv = lightgbm.Dataset(train[col], label= np.log1p(train['visitors']).values, max_bin= params['max_bin'], silent= True, free_raw_data= True)\n",
    "model = lightgbm.train(params, d_cv)\n",
    "# for test\n",
    "pred = model.predict(X_test)\n",
    "rmsle_test = RMSLE(y_test, pred)\n",
    "#print('test score %.6f' % rmsle_test)\n",
    "\n",
    "finish_time = datetime.datetime.now()\n",
    "elapsed = (finish_time - start_time).seconds\n",
    "cv_score /= K # Average valid set predictions\n",
    "test_score = rmsle_test # Average holdout set predictions\n",
    "print('\\n======================')\n",
    "print(\"CV score %.6f, Holdout score %.6f, Elapsed time: %.2fs\" % (cv_score, test_score, elapsed))\n",
    "print('======================\\n')\n",
    "\n",
    "#### train with whole data of 2017/1/1 - 2017/4/22\n",
    "train = TrainData[(TrainData['visit_date'] >= datetime.date(2017, 1, 1)) & \n",
    "                  (TrainData['visit_date'] < datetime.date(2017, 4, 22))]\n",
    "X_test = TestData[col]\n",
    "d_cv = lightgbm.Dataset(train[col], label= np.log1p(train['visitors']).values, max_bin= params['max_bin'], silent= True, free_raw_data= True)\n",
    "model = lightgbm.train(params, d_cv)\n",
    "pred = model.predict(X_test)\n",
    "# Create submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = TestData['id']\n",
    "sub['visitors'] = np.expm1(pred)\n",
    "sub.to_csv('gbm_submit.csv', float_format='%.6f', index=False)\n",
    "os.system('zip lgb.zip gbm_submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "543px",
    "left": "631px",
    "right": "20px",
    "top": "112px",
    "width": "610px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
